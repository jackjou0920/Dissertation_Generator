{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow version\n",
    "print(tf.__version__)\n",
    "\n",
    "logging.basicConfig(level=\"error\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/content/gdrive/My Drive/\"\n",
    "# os.chdir(path)\n",
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./\"\n",
    "topic_vocab = os.path.join(output_dir, \"topic_vocab\")\n",
    "abstract_vocab = os.path.join(output_dir, \"abstract_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_path = os.path.join(output_dir, 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "abstract = []\n",
    "dirPath1 = \"./msc_dataset/lr/\"\n",
    "dirPath2 = \"./msc_dataset/topic/\"\n",
    "dirPath3 = \"./ug_dataset/lr/\"\n",
    "dirPath4 = \"./ug_dataset/topic/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_files = [f for f in os.listdir(dirPath1) if os.path.isfile(os.path.join(dirPath1, f))]\n",
    "lr_files = sorted(lr_files)\n",
    "for fname in lr_files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath1+fname, \"r\", encoding='utf-8') as fp:\n",
    "        abstract.append(fp.read())\n",
    "        \n",
    "tp_files = [f for f in os.listdir(dirPath2) if os.path.isfile(os.path.join(dirPath2, f))]\n",
    "tp_files = sorted(tp_files)\n",
    "for fname in tp_files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    if (fname not in lr_files):\n",
    "        continue\n",
    "    with open(dirPath2+fname, \"r\", encoding='utf-8') as fp:\n",
    "        topic.append(fp.read())\n",
    "        \n",
    "print(len(topic))\n",
    "print(len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_files = [f for f in os.listdir(dirPath3) if os.path.isfile(os.path.join(dirPath3, f))]\n",
    "# lr_files = sorted(lr_files)\n",
    "# for fname in lr_files:\n",
    "#     if (\"txt\" not in fname):\n",
    "#         continue\n",
    "#     with open(dirPath3+fname, \"r\", encoding='utf-8') as fp:\n",
    "#         abstract.append(fp.read())\n",
    "        \n",
    "# tp_files = [f for f in os.listdir(dirPath4) if os.path.isfile(os.path.join(dirPath4, f))]\n",
    "# tp_files = sorted(tp_files)\n",
    "# for fname in tp_files:\n",
    "#     if (\"txt\" not in fname):\n",
    "#         continue\n",
    "#     if (fname not in lr_files):\n",
    "#         continue\n",
    "#     with open(dirPath4+fname, \"r\", encoding='utf-8') as fp:\n",
    "#         topic.append(fp.read())\n",
    "        \n",
    "# print(len(topic))\n",
    "# print(len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178\n"
     ]
    }
   ],
   "source": [
    "# Load input dataset\n",
    "topic = []\n",
    "dirPath = \"./msc_dataset/topic/\"\n",
    "\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        topic.append(fp.read())\n",
    "\n",
    "dirPath = \"./ug_dataset/topic/\"\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        topic.append(fp.read())\n",
    "print(len(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178\n"
     ]
    }
   ],
   "source": [
    "# The corresponding labels\n",
    "abstract = []\n",
    "dirPath = \"./msc_dataset/abstract/\"\n",
    "\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        abstract.append(fp.read())\n",
    "        \n",
    "dirPath = \"./ug_dataset/abstract/\"\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        abstract.append(fp.read())\n",
    "print(len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "data_size = len(topic)\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Cellular Genetic Algorithms for Test Generation', shape=(), dtype=string)\n",
      "tf.Tensor(b'Currently software testing is a crucial and labor-intensive component of software production. Software testing requires test data generation with good code coverage. Automated search techniques used for software test generation will be an improvement to efficiency and cost to software testing, and currently Genetic Algorithms are one the best options as a search heuristic for automated test generation.      The project will investigate in Cellular Genetic Algorithms, another subclass of Evolutionary Algorithms just like Genetic Algorithms, and their potential to replace Genetic Algorithms for test generation with even better code coverage. Research and implementation were done in regards to how these techniques may be applied to test generation, and specifically how using Cellular GAs may improve upon using standard GAs for test generation. The tests were done with the software tool Evosuite which runs a GA on Java classes to generate test-suites and outputs the GA\\xc3\\xa2\\xc2\\x80\\xc2\\x99s performance on code coverage, and a Cellular GA will be implemented into EvoSuite in this project and be compared to the standard GA in its performance for generating test data.      Results achieved showed general favor for cellular GAs due to the advantage of creating more diverse populations than standard GAs and also prevent premature convergence of solutions that more often converged at a local optimum instead of branching out searching for optimal solution for the standard GA.', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'An Online Portal for the Selection and Allocation of Dissertation Projects', shape=(), dtype=string)\n",
      "tf.Tensor(b'The allocation of students and staff to projects in the Department of Computer Science is a complicated procedure, not helped by its lack of centralisation. It relies heavily on emails between staff and students, and uses a number of online systems for different aspects of the process. This carries an inherent lack of transparency.\\n\\nThis project focuses on producing an online portal to consolidate the management of project allocation, allowing each user to access a personalised overview of their stages in the process. By replacing the number of systems currently in use with one single portal, the project aims to simplify the process for all those involved.\\n\\nThrough a literature review, and through several consultations with members of the Department, a set of requirements was identified and a new system produced. The result was a product of sufficient quality and flexibility to replace the existing systems, and the foundation for a portal to manage the lifecycle of projects in the Department.', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Pomorum: A Website Educating Primary Children through Experiential Methods', shape=(), dtype=string)\n",
      "tf.Tensor(b\"Invertebrates make up 95% of the species on this planet, and yet their importance is constantly overlooked. From shellac to birth control hormones, invertebrates are used in many unexpected places.\\n\\nTo encourage a life-long interest in and respect of invertebrates, this project aims to create a website which will teach primary aged children about the different sorts of invertebrates which we encounter in everyday life and the ways in which they change the world around them.\\n\\nBy using David Kolb's Theory of Experiential Learning to teach children more effectively, Pomorum.co.uk aims to be educational and fun, whilst also teaching them to see invertebrates in a new light. Including elements from the UK national curriculum brings Pomorum more relevance in schools, and hopefully turn it into a website which can be used in classrooms around the country.\", shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Transfer data list to tensor\n",
    "examples = tf.data.Dataset.from_tensor_slices((topic, abstract))\n",
    "\n",
    "# Split training data and testing data\n",
    "train_examples = examples.take(train_size)\n",
    "test_examples = examples.skip(train_size)\n",
    "\n",
    "for top, abt in test_examples.take(3):\n",
    "    print(top)\n",
    "    print(abt)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vocabulary： ./topic_vocab\n",
      "vocabulary size：4597\n",
      "load vocabulary： ./abstract_vocab\n",
      "vocabulary size：8190\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_topic = tfds.features.text.SubwordTextEncoder.load_from_file(topic_vocab)\n",
    "    print(f\"load vocabulary： {topic_vocab}\")\n",
    "except:\n",
    "    print(\"no vocabulary, start creating\")\n",
    "    # Create a custom subwords summary tokenizer from the training dataset.\n",
    "    tokenizer_topic = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "                              (top.numpy() for top, abt in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_topic.save_to_file(topic_vocab)  \n",
    "print(f\"vocabulary size：{tokenizer_topic.vocab_size}\")\n",
    "\n",
    "try:\n",
    "    tokenizer_abstract = tfds.features.text.SubwordTextEncoder.load_from_file(abstract_vocab)\n",
    "    print(f\"load vocabulary： {abstract_vocab}\")\n",
    "except:\n",
    "    print(\"no vocabulary, start creating\")\n",
    "    # Create a custom subwords news tokenizer from the training dataset.\n",
    "    tokenizer_abstract = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "                              (abt.numpy() for top, abt in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_abstract.save_to_file(abstract_vocab)\n",
    "print(f\"vocabulary size：{tokenizer_abstract.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a start and end token to the input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(to_t, ab_t):\n",
    "    # tokenizer_summ.vocab_size for the <start> token\n",
    "    # tokenizer_summ.vocab_size + 1 for the <end> token\n",
    "    to_indices = [tokenizer_topic.vocab_size] + tokenizer_topic.encode(\n",
    "                                    to_t.numpy()) + [tokenizer_topic.vocab_size + 1]\n",
    "    ab_indices = [tokenizer_abstract.vocab_size] + tokenizer_abstract.encode(\n",
    "                                    ab_t.numpy()) + [tokenizer_abstract.vocab_size + 1]\n",
    "    return to_indices, ab_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(to_t, ab_t):\n",
    "    # force 'su_t' and 'ne_t' to Eager Tensors by py_function\n",
    "    return tf.py_function(encode, [to_t, ab_t], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over MAX_LENGTH tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 700\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_max_length(to, ab, max_length=MAX_LENGTH):\n",
    "#     return tf.logical_and(tf.size(to) <= max_length, tf.size(ab) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_examples  # output：(Summary, News)\n",
    "                 .map(tf_encode) # output：(Summary index sequence, News index sequence)\n",
    "#                  .filter(filter_max_length)\n",
    "                 .cache() # speed up\n",
    "                 .shuffle(BUFFER_SIZE) # random dataset\n",
    "                 .padded_batch(BATCH_SIZE, # padding to same length for each batch\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # speed up\n",
    "\n",
    "test_dataset = (test_examples\n",
    "               .map(tf_encode)\n",
    "#                .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # tf.cast change data type\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the future tokens in a sequence\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    \n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "  \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    # scale matmul_qk\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # get seq_k sequence length\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # weighted average\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially assign the output dimension 'd_model' & 'num_heads'\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # how many heads divided by d_model\n",
    "        self.d_model = d_model # base dimension before split_heads\n",
    "    \n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "        self.depth = d_model // self.num_heads  # new dimension for each head\n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(d_model) \n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(d_model)  # linear transformation after concatenating heads\n",
    "  \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "    \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "        # divide 'd_model' into 'num_heads' depth\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # concatenate 'num_heads' depth to original dimension 'd_model'\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) \n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  # two linear transformations for input, add ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # one for sub-layer, one for layer norm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        # one for sub-layer, one for layer norm\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # sub-layer 1: MHA\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        # sub-layer 2: FFN\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked multi-head attention (with look ahead mask and padding mask)\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        # Multi-head attention (with padding mask). \n",
    "        # V (value) and K (key) receive the encoder output as inputs. \n",
    "        # Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        # all sub-layers output: (batch_size, target_seq_len, d_model)\n",
    "        # enc_output is Encoder output sequence: (batch_size, input_seq_len, d_model)\n",
    "        # attn_weights_block_1: (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights_block_2: (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "        # sub-layer 1: Decoder layer\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "        # sub-layer 2: Decoder layer focuses the Encoder final output\n",
    "        # (batch_size, target_seq_len, d_model) (V, K, Q)\n",
    "        # attention weights: the importance given to the decoder's input based on the encoder's output\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, inp_padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        # sub-layer 3: FFN\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    # - num_layers: how many EncoderLayers\n",
    "    # - input_vocab_size: transfer index to vector\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # (input_dim, output_dim)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape == (batch_size, input_seq_len)\n",
    "        # all layer output: (batch_size, input_seq_len, d_model)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # combine embedding and positional encoder, and regularization\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,  rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, d_model)\n",
    "    \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "    \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, rate)\n",
    "        # FFN output logits number, represent the probability passing softmax\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "        # Decoder output pass the last linear layer\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 4599\n",
      "target_vocab_size: 8192\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_topic.vocab_size + 2\n",
    "target_vocab_size = tokenizer_abstract.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # comput cross entropy of all position, but not sum up\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # only compute the loss of non <pad> position\n",
    "  \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # the default warmup_steps = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported after Tensorflow 2.0.0-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_models = [128, 256, 512]\n",
    "# warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "# schedules = []\n",
    "# labels = []\n",
    "# colors = [\"blue\", \"red\", \"black\"]\n",
    "# for d in d_models:\n",
    "#     schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "#     labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "# for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "#     plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "#            label=label, color=colors[i // 3])\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer has 4 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 4599\n",
      "target_vocab_size: 8192\n",
      "dropout_rate: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"Transformer has {num_layers} Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the up-to-date checkpoint, the model has already trained 154 epochs.\n"
     ]
    }
   ],
   "source": [
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_size}data\"\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_path, run_id)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'load the up-to-date checkpoint, the model has already trained {last_epoch} epochs.')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"No checkpoint, start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # use Adam optimizer for updating parameters of Transformer\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    # the loss and training acc recorded on TensorBoard\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer has already trained 154 epochs.\n",
      "Saving checkpoint for epoch 155 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-155\n",
      "Epoch 155 Loss 0.7348 Accuracy 0.2322\n",
      "Time taken for 1 epoch: 900.9795050621033 secs\n",
      "\n",
      "Saving checkpoint for epoch 156 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-156\n",
      "Epoch 156 Loss 0.7187 Accuracy 0.2328\n",
      "Time taken for 1 epoch: 424.456946849823 secs\n",
      "\n",
      "Saving checkpoint for epoch 157 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-157\n",
      "Epoch 157 Loss 0.7116 Accuracy 0.2327\n",
      "Time taken for 1 epoch: 414.94530510902405 secs\n",
      "\n",
      "Saving checkpoint for epoch 158 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-158\n",
      "Epoch 158 Loss 0.7043 Accuracy 0.2322\n",
      "Time taken for 1 epoch: 424.58951711654663 secs\n",
      "\n",
      "Saving checkpoint for epoch 159 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-159\n",
      "Epoch 159 Loss 0.6945 Accuracy 0.2334\n",
      "Time taken for 1 epoch: 418.0830888748169 secs\n",
      "\n",
      "Saving checkpoint for epoch 160 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-160\n",
      "Epoch 160 Loss 0.6862 Accuracy 0.2347\n",
      "Time taken for 1 epoch: 406.6566939353943 secs\n",
      "\n",
      "Saving checkpoint for epoch 161 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-161\n",
      "Epoch 161 Loss 0.6812 Accuracy 0.2348\n",
      "Time taken for 1 epoch: 419.01676297187805 secs\n",
      "\n",
      "Saving checkpoint for epoch 162 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-162\n",
      "Epoch 162 Loss 0.6742 Accuracy 0.2358\n",
      "Time taken for 1 epoch: 397.4573152065277 secs\n",
      "\n",
      "Saving checkpoint for epoch 163 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-163\n",
      "Epoch 163 Loss 0.6719 Accuracy 0.2359\n",
      "Time taken for 1 epoch: 388.4317219257355 secs\n",
      "\n",
      "Saving checkpoint for epoch 164 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-164\n",
      "Epoch 164 Loss 0.6644 Accuracy 0.2369\n",
      "Time taken for 1 epoch: 383.65828490257263 secs\n",
      "\n",
      "Saving checkpoint for epoch 165 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-165\n",
      "Epoch 165 Loss 0.6578 Accuracy 0.2379\n",
      "Time taken for 1 epoch: 383.31560611724854 secs\n",
      "\n",
      "Saving checkpoint for epoch 166 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-166\n",
      "Epoch 166 Loss 0.6571 Accuracy 0.2379\n",
      "Time taken for 1 epoch: 383.45574402809143 secs\n",
      "\n",
      "Saving checkpoint for epoch 167 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-167\n",
      "Epoch 167 Loss 0.6534 Accuracy 0.2384\n",
      "Time taken for 1 epoch: 387.89495491981506 secs\n",
      "\n",
      "Saving checkpoint for epoch 168 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-168\n",
      "Epoch 168 Loss 0.6544 Accuracy 0.2379\n",
      "Time taken for 1 epoch: 387.39930987358093 secs\n",
      "\n",
      "Saving checkpoint for epoch 169 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-169\n",
      "Epoch 169 Loss 0.6481 Accuracy 0.2390\n",
      "Time taken for 1 epoch: 384.1277868747711 secs\n",
      "\n",
      "Saving checkpoint for epoch 170 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-170\n",
      "Epoch 170 Loss 0.6369 Accuracy 0.2412\n",
      "Time taken for 1 epoch: 385.0831518173218 secs\n",
      "\n",
      "Saving checkpoint for epoch 171 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-171\n",
      "Epoch 171 Loss 0.6310 Accuracy 0.2417\n",
      "Time taken for 1 epoch: 384.1283552646637 secs\n",
      "\n",
      "Saving checkpoint for epoch 172 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-172\n",
      "Epoch 172 Loss 0.6262 Accuracy 0.2427\n",
      "Time taken for 1 epoch: 384.4701189994812 secs\n",
      "\n",
      "Saving checkpoint for epoch 173 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-173\n",
      "Epoch 173 Loss 0.6236 Accuracy 0.2431\n",
      "Time taken for 1 epoch: 380.2640452384949 secs\n",
      "\n",
      "Saving checkpoint for epoch 174 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-174\n",
      "Epoch 174 Loss 0.6186 Accuracy 0.2442\n",
      "Time taken for 1 epoch: 384.5204520225525 secs\n",
      "\n",
      "Saving checkpoint for epoch 175 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-175\n",
      "Epoch 175 Loss 0.6174 Accuracy 0.2443\n",
      "Time taken for 1 epoch: 383.51062297821045 secs\n",
      "\n",
      "Saving checkpoint for epoch 176 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-176\n",
      "Epoch 176 Loss 0.6095 Accuracy 0.2456\n",
      "Time taken for 1 epoch: 383.76356077194214 secs\n",
      "\n",
      "Saving checkpoint for epoch 177 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-177\n",
      "Epoch 177 Loss 0.6047 Accuracy 0.2461\n",
      "Time taken for 1 epoch: 391.15783286094666 secs\n",
      "\n",
      "Saving checkpoint for epoch 178 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-178\n",
      "Epoch 178 Loss 0.6019 Accuracy 0.2469\n",
      "Time taken for 1 epoch: 387.6139340400696 secs\n",
      "\n",
      "Saving checkpoint for epoch 179 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-179\n",
      "Epoch 179 Loss 0.5996 Accuracy 0.2473\n",
      "Time taken for 1 epoch: 418.2583758831024 secs\n",
      "\n",
      "Saving checkpoint for epoch 180 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-180\n",
      "Epoch 180 Loss 0.5945 Accuracy 0.2484\n",
      "Time taken for 1 epoch: 417.7032001018524 secs\n",
      "\n",
      "Saving checkpoint for epoch 181 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-181\n",
      "Epoch 181 Loss 0.5925 Accuracy 0.2481\n",
      "Time taken for 1 epoch: 434.2560660839081 secs\n",
      "\n",
      "Saving checkpoint for epoch 182 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-182\n",
      "Epoch 182 Loss 0.5872 Accuracy 0.2493\n",
      "Time taken for 1 epoch: 412.92227506637573 secs\n",
      "\n",
      "Saving checkpoint for epoch 183 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-183\n",
      "Epoch 183 Loss 0.5860 Accuracy 0.2492\n",
      "Time taken for 1 epoch: 416.93142199516296 secs\n",
      "\n",
      "Saving checkpoint for epoch 184 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-184\n",
      "Epoch 184 Loss 0.5827 Accuracy 0.2499\n",
      "Time taken for 1 epoch: 426.63024497032166 secs\n",
      "\n",
      "Saving checkpoint for epoch 185 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-185\n",
      "Epoch 185 Loss 0.5814 Accuracy 0.2503\n",
      "Time taken for 1 epoch: 395.1246557235718 secs\n",
      "\n",
      "Saving checkpoint for epoch 186 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-186\n",
      "Epoch 186 Loss 0.5786 Accuracy 0.2507\n",
      "Time taken for 1 epoch: 402.6625671386719 secs\n",
      "\n",
      "Saving checkpoint for epoch 187 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-187\n",
      "Epoch 187 Loss 0.5752 Accuracy 0.2513\n",
      "Time taken for 1 epoch: 408.43361496925354 secs\n",
      "\n",
      "Saving checkpoint for epoch 188 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-188\n",
      "Epoch 188 Loss 0.5734 Accuracy 0.2515\n",
      "Time taken for 1 epoch: 404.51946806907654 secs\n",
      "\n",
      "Saving checkpoint for epoch 189 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-189\n",
      "Epoch 189 Loss 0.5711 Accuracy 0.2521\n",
      "Time taken for 1 epoch: 400.1133940219879 secs\n",
      "\n",
      "Saving checkpoint for epoch 190 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-190\n",
      "Epoch 190 Loss 0.5632 Accuracy 0.2537\n",
      "Time taken for 1 epoch: 410.9476339817047 secs\n",
      "\n",
      "Saving checkpoint for epoch 191 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-191\n",
      "Epoch 191 Loss 0.5650 Accuracy 0.2531\n",
      "Time taken for 1 epoch: 410.5471751689911 secs\n",
      "\n",
      "Saving checkpoint for epoch 192 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-192\n",
      "Epoch 192 Loss 0.5630 Accuracy 0.2533\n",
      "Time taken for 1 epoch: 425.4954090118408 secs\n",
      "\n",
      "Saving checkpoint for epoch 193 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-193\n",
      "Epoch 193 Loss 0.5568 Accuracy 0.2545\n",
      "Time taken for 1 epoch: 422.5572621822357 secs\n",
      "\n",
      "Saving checkpoint for epoch 194 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-194\n",
      "Epoch 194 Loss 0.5527 Accuracy 0.2557\n",
      "Time taken for 1 epoch: 393.77969312667847 secs\n",
      "\n",
      "Saving checkpoint for epoch 195 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-195\n",
      "Epoch 195 Loss 0.5465 Accuracy 0.2568\n",
      "Time taken for 1 epoch: 398.2823758125305 secs\n",
      "\n",
      "Saving checkpoint for epoch 196 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-196\n",
      "Epoch 196 Loss 0.5432 Accuracy 0.2575\n",
      "Time taken for 1 epoch: 387.4686040878296 secs\n",
      "\n",
      "Saving checkpoint for epoch 197 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-197\n",
      "Epoch 197 Loss 0.5463 Accuracy 0.2562\n",
      "Time taken for 1 epoch: 384.48025488853455 secs\n",
      "\n",
      "Saving checkpoint for epoch 198 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-198\n",
      "Epoch 198 Loss 0.5437 Accuracy 0.2572\n",
      "Time taken for 1 epoch: 381.89764404296875 secs\n",
      "\n",
      "Saving checkpoint for epoch 199 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-199\n",
      "Epoch 199 Loss 0.5420 Accuracy 0.2572\n",
      "Time taken for 1 epoch: 385.16611790657043 secs\n",
      "\n",
      "Saving checkpoint for epoch 200 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-200\n",
      "Epoch 200 Loss 0.5415 Accuracy 0.2577\n",
      "Time taken for 1 epoch: 383.1196789741516 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 201 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-201\n",
      "Epoch 201 Loss 0.5361 Accuracy 0.2586\n",
      "Time taken for 1 epoch: 396.0412509441376 secs\n",
      "\n",
      "Saving checkpoint for epoch 202 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-202\n",
      "Epoch 202 Loss 0.5279 Accuracy 0.2600\n",
      "Time taken for 1 epoch: 385.66819977760315 secs\n",
      "\n",
      "Saving checkpoint for epoch 203 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-203\n",
      "Epoch 203 Loss 0.5241 Accuracy 0.2610\n",
      "Time taken for 1 epoch: 387.0331540107727 secs\n",
      "\n",
      "Saving checkpoint for epoch 204 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-204\n",
      "Epoch 204 Loss 0.5220 Accuracy 0.2615\n",
      "Time taken for 1 epoch: 390.15768814086914 secs\n",
      "\n",
      "Saving checkpoint for epoch 205 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-205\n",
      "Epoch 205 Loss 0.5202 Accuracy 0.2625\n",
      "Time taken for 1 epoch: 385.46433901786804 secs\n",
      "\n",
      "Saving checkpoint for epoch 206 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-206\n",
      "Epoch 206 Loss 0.5216 Accuracy 0.2617\n",
      "Time taken for 1 epoch: 381.9221589565277 secs\n",
      "\n",
      "Saving checkpoint for epoch 207 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-207\n",
      "Epoch 207 Loss 0.5201 Accuracy 0.2622\n",
      "Time taken for 1 epoch: 383.36614298820496 secs\n",
      "\n",
      "Saving checkpoint for epoch 208 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-208\n",
      "Epoch 208 Loss 0.5181 Accuracy 0.2622\n",
      "Time taken for 1 epoch: 380.82054710388184 secs\n",
      "\n",
      "Saving checkpoint for epoch 209 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-209\n",
      "Epoch 209 Loss 0.5185 Accuracy 0.2620\n",
      "Time taken for 1 epoch: 393.34839391708374 secs\n",
      "\n",
      "Saving checkpoint for epoch 210 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-210\n",
      "Epoch 210 Loss 0.5114 Accuracy 0.2633\n",
      "Time taken for 1 epoch: 383.63955187797546 secs\n",
      "\n",
      "Saving checkpoint for epoch 211 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-211\n",
      "Epoch 211 Loss 0.5079 Accuracy 0.2644\n",
      "Time taken for 1 epoch: 382.6100709438324 secs\n",
      "\n",
      "Saving checkpoint for epoch 212 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-212\n",
      "Epoch 212 Loss 0.5049 Accuracy 0.2650\n",
      "Time taken for 1 epoch: 381.9513440132141 secs\n",
      "\n",
      "Saving checkpoint for epoch 213 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-213\n",
      "Epoch 213 Loss 0.5018 Accuracy 0.2654\n",
      "Time taken for 1 epoch: 383.3159008026123 secs\n",
      "\n",
      "Saving checkpoint for epoch 214 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-214\n",
      "Epoch 214 Loss 0.5014 Accuracy 0.2657\n",
      "Time taken for 1 epoch: 390.7428979873657 secs\n",
      "\n",
      "Saving checkpoint for epoch 215 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-215\n",
      "Epoch 215 Loss 0.4991 Accuracy 0.2660\n",
      "Time taken for 1 epoch: 386.55281615257263 secs\n",
      "\n",
      "Saving checkpoint for epoch 216 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-216\n",
      "Epoch 216 Loss 0.4950 Accuracy 0.2669\n",
      "Time taken for 1 epoch: 1488.649542093277 secs\n",
      "\n",
      "Saving checkpoint for epoch 217 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-217\n",
      "Epoch 217 Loss 0.4935 Accuracy 0.2670\n",
      "Time taken for 1 epoch: 1084.5876059532166 secs\n",
      "\n",
      "Saving checkpoint for epoch 218 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-218\n",
      "Epoch 218 Loss 0.4925 Accuracy 0.2675\n",
      "Time taken for 1 epoch: 453.73666310310364 secs\n",
      "\n",
      "Saving checkpoint for epoch 219 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-219\n",
      "Epoch 219 Loss 0.4914 Accuracy 0.2675\n",
      "Time taken for 1 epoch: 427.1385762691498 secs\n",
      "\n",
      "Saving checkpoint for epoch 220 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-220\n",
      "Epoch 220 Loss 0.4858 Accuracy 0.2687\n",
      "Time taken for 1 epoch: 419.0247039794922 secs\n",
      "\n",
      "Saving checkpoint for epoch 221 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-221\n",
      "Epoch 221 Loss 0.4860 Accuracy 0.2688\n",
      "Time taken for 1 epoch: 415.0801110267639 secs\n",
      "\n",
      "Saving checkpoint for epoch 222 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-222\n",
      "Epoch 222 Loss 0.4842 Accuracy 0.2692\n",
      "Time taken for 1 epoch: 420.04592299461365 secs\n",
      "\n",
      "Saving checkpoint for epoch 223 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-223\n",
      "Epoch 223 Loss 0.4804 Accuracy 0.2699\n",
      "Time taken for 1 epoch: 414.9364879131317 secs\n",
      "\n",
      "Saving checkpoint for epoch 224 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-224\n",
      "Epoch 224 Loss 0.4802 Accuracy 0.2700\n",
      "Time taken for 1 epoch: 414.11997532844543 secs\n",
      "\n",
      "Saving checkpoint for epoch 225 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-225\n",
      "Epoch 225 Loss 0.4785 Accuracy 0.2702\n",
      "Time taken for 1 epoch: 414.8327281475067 secs\n",
      "\n",
      "Saving checkpoint for epoch 226 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-226\n",
      "Epoch 226 Loss 0.4789 Accuracy 0.2702\n",
      "Time taken for 1 epoch: 410.8150272369385 secs\n",
      "\n",
      "Saving checkpoint for epoch 227 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-227\n",
      "Epoch 227 Loss 0.4759 Accuracy 0.2706\n",
      "Time taken for 1 epoch: 414.0926351547241 secs\n",
      "\n",
      "Saving checkpoint for epoch 228 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-228\n",
      "Epoch 228 Loss 0.4734 Accuracy 0.2710\n",
      "Time taken for 1 epoch: 417.5847680568695 secs\n",
      "\n",
      "Saving checkpoint for epoch 229 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-229\n",
      "Epoch 229 Loss 0.4725 Accuracy 0.2714\n",
      "Time taken for 1 epoch: 410.4947807788849 secs\n",
      "\n",
      "Saving checkpoint for epoch 230 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-230\n",
      "Epoch 230 Loss 0.4701 Accuracy 0.2716\n",
      "Time taken for 1 epoch: 415.458459854126 secs\n",
      "\n",
      "Saving checkpoint for epoch 231 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-231\n",
      "Epoch 231 Loss 0.4738 Accuracy 0.2709\n",
      "Time taken for 1 epoch: 418.6569368839264 secs\n",
      "\n",
      "Saving checkpoint for epoch 232 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-232\n",
      "Epoch 232 Loss 0.4716 Accuracy 0.2716\n",
      "Time taken for 1 epoch: 416.76456570625305 secs\n",
      "\n",
      "Saving checkpoint for epoch 233 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-233\n",
      "Epoch 233 Loss 0.4685 Accuracy 0.2721\n",
      "Time taken for 1 epoch: 419.3883709907532 secs\n",
      "\n",
      "Saving checkpoint for epoch 234 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-234\n",
      "Epoch 234 Loss 0.4649 Accuracy 0.2726\n",
      "Time taken for 1 epoch: 419.3016619682312 secs\n",
      "\n",
      "Saving checkpoint for epoch 235 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-235\n",
      "Epoch 235 Loss 0.4631 Accuracy 0.2734\n",
      "Time taken for 1 epoch: 418.52137327194214 secs\n",
      "\n",
      "Saving checkpoint for epoch 236 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-236\n",
      "Epoch 236 Loss 0.4613 Accuracy 0.2736\n",
      "Time taken for 1 epoch: 417.0518898963928 secs\n",
      "\n",
      "Saving checkpoint for epoch 237 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-237\n",
      "Epoch 237 Loss 0.4593 Accuracy 0.2746\n",
      "Time taken for 1 epoch: 417.75671315193176 secs\n",
      "\n",
      "Saving checkpoint for epoch 238 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-238\n",
      "Epoch 238 Loss 0.4591 Accuracy 0.2747\n",
      "Time taken for 1 epoch: 410.2318639755249 secs\n",
      "\n",
      "Saving checkpoint for epoch 239 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-239\n",
      "Epoch 239 Loss 0.4543 Accuracy 0.2757\n",
      "Time taken for 1 epoch: 413.91263794898987 secs\n",
      "\n",
      "Saving checkpoint for epoch 240 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-240\n",
      "Epoch 240 Loss 0.4523 Accuracy 0.2761\n",
      "Time taken for 1 epoch: 392.10808086395264 secs\n",
      "\n",
      "Saving checkpoint for epoch 241 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-241\n",
      "Epoch 241 Loss 0.4504 Accuracy 0.2763\n",
      "Time taken for 1 epoch: 384.1828398704529 secs\n",
      "\n",
      "Saving checkpoint for epoch 242 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-242\n",
      "Epoch 242 Loss 0.4459 Accuracy 0.2771\n",
      "Time taken for 1 epoch: 382.5631070137024 secs\n",
      "\n",
      "Saving checkpoint for epoch 243 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-243\n",
      "Epoch 243 Loss 0.4485 Accuracy 0.2766\n",
      "Time taken for 1 epoch: 385.52708983421326 secs\n",
      "\n",
      "Saving checkpoint for epoch 244 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-244\n",
      "Epoch 244 Loss 0.4472 Accuracy 0.2769\n",
      "Time taken for 1 epoch: 387.2052698135376 secs\n",
      "\n",
      "Saving checkpoint for epoch 245 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-245\n",
      "Epoch 245 Loss 0.4491 Accuracy 0.2766\n",
      "Time taken for 1 epoch: 378.98604011535645 secs\n",
      "\n",
      "Saving checkpoint for epoch 246 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-246\n",
      "Epoch 246 Loss 0.4431 Accuracy 0.2780\n",
      "Time taken for 1 epoch: 380.34481406211853 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 247 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-247\n",
      "Epoch 247 Loss 0.4425 Accuracy 0.2780\n",
      "Time taken for 1 epoch: 413.45447397232056 secs\n",
      "\n",
      "Saving checkpoint for epoch 248 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-248\n",
      "Epoch 248 Loss 0.4399 Accuracy 0.2787\n",
      "Time taken for 1 epoch: 418.06664204597473 secs\n",
      "\n",
      "Saving checkpoint for epoch 249 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-249\n",
      "Epoch 249 Loss 0.4383 Accuracy 0.2789\n",
      "Time taken for 1 epoch: 410.414186000824 secs\n",
      "\n",
      "Saving checkpoint for epoch 250 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-250\n",
      "Epoch 250 Loss 0.4367 Accuracy 0.2792\n",
      "Time taken for 1 epoch: 398.1964797973633 secs\n",
      "\n",
      "Saving checkpoint for epoch 251 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-251\n",
      "Epoch 251 Loss 0.4357 Accuracy 0.2797\n",
      "Time taken for 1 epoch: 421.5228989124298 secs\n",
      "\n",
      "Saving checkpoint for epoch 252 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-252\n",
      "Epoch 252 Loss 0.4344 Accuracy 0.2798\n",
      "Time taken for 1 epoch: 389.0600447654724 secs\n",
      "\n",
      "Saving checkpoint for epoch 253 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-253\n",
      "Epoch 253 Loss 0.4330 Accuracy 0.2801\n",
      "Time taken for 1 epoch: 379.48278880119324 secs\n",
      "\n",
      "Saving checkpoint for epoch 254 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-254\n",
      "Epoch 254 Loss 0.4314 Accuracy 0.2806\n",
      "Time taken for 1 epoch: 379.0758137702942 secs\n",
      "\n",
      "Saving checkpoint for epoch 255 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-255\n",
      "Epoch 255 Loss 0.4296 Accuracy 0.2811\n",
      "Time taken for 1 epoch: 384.0110490322113 secs\n",
      "\n",
      "Saving checkpoint for epoch 256 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-256\n",
      "Epoch 256 Loss 0.4306 Accuracy 0.2808\n",
      "Time taken for 1 epoch: 400.3321158885956 secs\n",
      "\n",
      "Saving checkpoint for epoch 257 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-257\n",
      "Epoch 257 Loss 0.4307 Accuracy 0.2806\n",
      "Time taken for 1 epoch: 415.0405340194702 secs\n",
      "\n",
      "Saving checkpoint for epoch 258 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-258\n",
      "Epoch 258 Loss 0.4265 Accuracy 0.2815\n",
      "Time taken for 1 epoch: 415.44885993003845 secs\n",
      "\n",
      "Saving checkpoint for epoch 259 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-259\n",
      "Epoch 259 Loss 0.4278 Accuracy 0.2807\n",
      "Time taken for 1 epoch: 402.3555529117584 secs\n",
      "\n",
      "Saving checkpoint for epoch 260 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-260\n",
      "Epoch 260 Loss 0.4277 Accuracy 0.2815\n",
      "Time taken for 1 epoch: 384.5443058013916 secs\n",
      "\n",
      "Saving checkpoint for epoch 261 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-261\n",
      "Epoch 261 Loss 0.4254 Accuracy 0.2813\n",
      "Time taken for 1 epoch: 379.38259196281433 secs\n",
      "\n",
      "Saving checkpoint for epoch 262 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-262\n",
      "Epoch 262 Loss 0.4257 Accuracy 0.2813\n",
      "Time taken for 1 epoch: 377.09155797958374 secs\n",
      "\n",
      "Saving checkpoint for epoch 263 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-263\n",
      "Epoch 263 Loss 0.4237 Accuracy 0.2823\n",
      "Time taken for 1 epoch: 379.03522396087646 secs\n",
      "\n",
      "Saving checkpoint for epoch 264 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-264\n",
      "Epoch 264 Loss 0.4226 Accuracy 0.2825\n",
      "Time taken for 1 epoch: 375.973757982254 secs\n",
      "\n",
      "Saving checkpoint for epoch 265 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-265\n",
      "Epoch 265 Loss 0.4200 Accuracy 0.2828\n",
      "Time taken for 1 epoch: 379.0555489063263 secs\n",
      "\n",
      "Saving checkpoint for epoch 266 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-266\n",
      "Epoch 266 Loss 0.4203 Accuracy 0.2831\n",
      "Time taken for 1 epoch: 379.2794499397278 secs\n",
      "\n",
      "Saving checkpoint for epoch 267 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-267\n",
      "Epoch 267 Loss 0.4202 Accuracy 0.2829\n",
      "Time taken for 1 epoch: 383.4859666824341 secs\n",
      "\n",
      "Saving checkpoint for epoch 268 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-268\n",
      "Epoch 268 Loss 0.4194 Accuracy 0.2833\n",
      "Time taken for 1 epoch: 376.127064704895 secs\n",
      "\n",
      "Saving checkpoint for epoch 269 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-269\n",
      "Epoch 269 Loss 0.4175 Accuracy 0.2837\n",
      "Time taken for 1 epoch: 376.28773307800293 secs\n",
      "\n",
      "Saving checkpoint for epoch 270 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-270\n",
      "Epoch 270 Loss 0.4161 Accuracy 0.2837\n",
      "Time taken for 1 epoch: 379.10353112220764 secs\n",
      "\n",
      "Saving checkpoint for epoch 271 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-271\n",
      "Epoch 271 Loss 0.4127 Accuracy 0.2845\n",
      "Time taken for 1 epoch: 376.37386870384216 secs\n",
      "\n",
      "Saving checkpoint for epoch 272 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-272\n",
      "Epoch 272 Loss 0.4096 Accuracy 0.2856\n",
      "Time taken for 1 epoch: 377.9381368160248 secs\n",
      "\n",
      "Saving checkpoint for epoch 273 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-273\n",
      "Epoch 273 Loss 0.4089 Accuracy 0.2857\n",
      "Time taken for 1 epoch: 378.70672392845154 secs\n",
      "\n",
      "Saving checkpoint for epoch 274 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-274\n",
      "Epoch 274 Loss 0.4059 Accuracy 0.2859\n",
      "Time taken for 1 epoch: 376.8514747619629 secs\n",
      "\n",
      "Saving checkpoint for epoch 275 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-275\n",
      "Epoch 275 Loss 0.4053 Accuracy 0.2865\n",
      "Time taken for 1 epoch: 378.80531311035156 secs\n",
      "\n",
      "Saving checkpoint for epoch 276 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-276\n",
      "Epoch 276 Loss 0.4014 Accuracy 0.2870\n",
      "Time taken for 1 epoch: 381.7592570781708 secs\n",
      "\n",
      "Saving checkpoint for epoch 277 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-277\n",
      "Epoch 277 Loss 0.4050 Accuracy 0.2862\n",
      "Time taken for 1 epoch: 381.8724343776703 secs\n",
      "\n",
      "Saving checkpoint for epoch 278 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-278\n",
      "Epoch 278 Loss 0.4019 Accuracy 0.2875\n",
      "Time taken for 1 epoch: 380.556880235672 secs\n",
      "\n",
      "Saving checkpoint for epoch 279 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-279\n",
      "Epoch 279 Loss 0.4029 Accuracy 0.2869\n",
      "Time taken for 1 epoch: 379.08660197257996 secs\n",
      "\n",
      "Saving checkpoint for epoch 280 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-280\n",
      "Epoch 280 Loss 0.3969 Accuracy 0.2884\n",
      "Time taken for 1 epoch: 377.47568798065186 secs\n",
      "\n",
      "Saving checkpoint for epoch 281 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-281\n",
      "Epoch 281 Loss 0.3980 Accuracy 0.2882\n",
      "Time taken for 1 epoch: 376.2967109680176 secs\n",
      "\n",
      "Saving checkpoint for epoch 282 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-282\n",
      "Epoch 282 Loss 0.3966 Accuracy 0.2884\n",
      "Time taken for 1 epoch: 377.7947487831116 secs\n",
      "\n",
      "Saving checkpoint for epoch 283 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-283\n",
      "Epoch 283 Loss 0.3943 Accuracy 0.2889\n",
      "Time taken for 1 epoch: 379.92401695251465 secs\n",
      "\n",
      "Saving checkpoint for epoch 284 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-284\n",
      "Epoch 284 Loss 0.3924 Accuracy 0.2893\n",
      "Time taken for 1 epoch: 379.38282084465027 secs\n",
      "\n",
      "Saving checkpoint for epoch 285 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-285\n",
      "Epoch 285 Loss 0.3918 Accuracy 0.2898\n",
      "Time taken for 1 epoch: 376.4654562473297 secs\n",
      "\n",
      "Saving checkpoint for epoch 286 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-286\n",
      "Epoch 286 Loss 0.3914 Accuracy 0.2899\n",
      "Time taken for 1 epoch: 385.69863295555115 secs\n",
      "\n",
      "Saving checkpoint for epoch 287 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-287\n",
      "Epoch 287 Loss 0.3909 Accuracy 0.2899\n",
      "Time taken for 1 epoch: 379.6095051765442 secs\n",
      "\n",
      "Saving checkpoint for epoch 288 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-288\n",
      "Epoch 288 Loss 0.3910 Accuracy 0.2903\n",
      "Time taken for 1 epoch: 377.496052980423 secs\n",
      "\n",
      "Saving checkpoint for epoch 289 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-289\n",
      "Epoch 289 Loss 0.3920 Accuracy 0.2895\n",
      "Time taken for 1 epoch: 375.5375962257385 secs\n",
      "\n",
      "Saving checkpoint for epoch 290 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-290\n",
      "Epoch 290 Loss 0.3875 Accuracy 0.2909\n",
      "Time taken for 1 epoch: 377.2408001422882 secs\n",
      "\n",
      "Saving checkpoint for epoch 291 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-291\n",
      "Epoch 291 Loss 0.3906 Accuracy 0.2904\n",
      "Time taken for 1 epoch: 378.9116837978363 secs\n",
      "\n",
      "Saving checkpoint for epoch 292 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-292\n",
      "Epoch 292 Loss 0.3891 Accuracy 0.2905\n",
      "Time taken for 1 epoch: 378.7844009399414 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint for epoch 293 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-293\n",
      "Epoch 293 Loss 0.3874 Accuracy 0.2907\n",
      "Time taken for 1 epoch: 390.3381929397583 secs\n",
      "\n",
      "Saving checkpoint for epoch 294 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-294\n",
      "Epoch 294 Loss 0.3831 Accuracy 0.2920\n",
      "Time taken for 1 epoch: 377.8456871509552 secs\n",
      "\n",
      "Saving checkpoint for epoch 295 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-295\n",
      "Epoch 295 Loss 0.3829 Accuracy 0.2917\n",
      "Time taken for 1 epoch: 384.2829339504242 secs\n",
      "\n",
      "Saving checkpoint for epoch 296 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-296\n",
      "Epoch 296 Loss 0.3810 Accuracy 0.2925\n",
      "Time taken for 1 epoch: 381.10992789268494 secs\n",
      "\n",
      "Saving checkpoint for epoch 297 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-297\n",
      "Epoch 297 Loss 0.3791 Accuracy 0.2926\n",
      "Time taken for 1 epoch: 380.5643730163574 secs\n",
      "\n",
      "Saving checkpoint for epoch 298 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-298\n",
      "Epoch 298 Loss 0.3757 Accuracy 0.2936\n",
      "Time taken for 1 epoch: 376.67883586883545 secs\n",
      "\n",
      "Saving checkpoint for epoch 299 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-299\n",
      "Epoch 299 Loss 0.3763 Accuracy 0.2931\n",
      "Time taken for 1 epoch: 375.81447982788086 secs\n",
      "\n",
      "Saving checkpoint for epoch 300 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-300\n",
      "Epoch 300 Loss 0.3761 Accuracy 0.2931\n",
      "Time taken for 1 epoch: 376.3938219547272 secs\n",
      "\n",
      "Saving checkpoint for epoch 301 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-301\n",
      "Epoch 301 Loss 0.3752 Accuracy 0.2935\n",
      "Time taken for 1 epoch: 377.3135209083557 secs\n",
      "\n",
      "Saving checkpoint for epoch 302 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-302\n",
      "Epoch 302 Loss 0.3760 Accuracy 0.2932\n",
      "Time taken for 1 epoch: 375.9362778663635 secs\n",
      "\n",
      "Saving checkpoint for epoch 303 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-303\n",
      "Epoch 303 Loss 0.3750 Accuracy 0.2937\n",
      "Time taken for 1 epoch: 378.9188940525055 secs\n",
      "\n",
      "Saving checkpoint for epoch 304 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-304\n",
      "Epoch 304 Loss 0.3728 Accuracy 0.2942\n",
      "Time taken for 1 epoch: 376.74058294296265 secs\n",
      "\n",
      "Saving checkpoint for epoch 305 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-305\n",
      "Epoch 305 Loss 0.3739 Accuracy 0.2939\n",
      "Time taken for 1 epoch: 383.86385893821716 secs\n",
      "\n",
      "Saving checkpoint for epoch 306 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-306\n",
      "Epoch 306 Loss 0.3714 Accuracy 0.2946\n",
      "Time taken for 1 epoch: 381.5187692642212 secs\n",
      "\n",
      "Saving checkpoint for epoch 307 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-307\n",
      "Epoch 307 Loss 0.3695 Accuracy 0.2951\n",
      "Time taken for 1 epoch: 379.99747681617737 secs\n",
      "\n",
      "Saving checkpoint for epoch 308 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-308\n",
      "Epoch 308 Loss 0.3690 Accuracy 0.2952\n",
      "Time taken for 1 epoch: 375.8915672302246 secs\n",
      "\n",
      "Saving checkpoint for epoch 309 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-309\n",
      "Epoch 309 Loss 0.3678 Accuracy 0.2955\n",
      "Time taken for 1 epoch: 376.79659700393677 secs\n",
      "\n",
      "Saving checkpoint for epoch 310 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-310\n",
      "Epoch 310 Loss 0.3676 Accuracy 0.2955\n",
      "Time taken for 1 epoch: 381.82688784599304 secs\n",
      "\n",
      "Saving checkpoint for epoch 311 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-311\n",
      "Epoch 311 Loss 0.3678 Accuracy 0.2954\n",
      "Time taken for 1 epoch: 379.95991492271423 secs\n",
      "\n",
      "Saving checkpoint for epoch 312 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-312\n",
      "Epoch 312 Loss 0.3651 Accuracy 0.2962\n",
      "Time taken for 1 epoch: 380.57372283935547 secs\n",
      "\n",
      "Saving checkpoint for epoch 313 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-313\n",
      "Epoch 313 Loss 0.3651 Accuracy 0.2962\n",
      "Time taken for 1 epoch: 378.99642992019653 secs\n",
      "\n",
      "Saving checkpoint for epoch 314 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-314\n",
      "Epoch 314 Loss 0.3650 Accuracy 0.2964\n",
      "Time taken for 1 epoch: 378.7127251625061 secs\n",
      "\n",
      "Saving checkpoint for epoch 315 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-315\n",
      "Epoch 315 Loss 0.3623 Accuracy 0.2968\n",
      "Time taken for 1 epoch: 380.6991560459137 secs\n",
      "\n",
      "Saving checkpoint for epoch 316 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-316\n",
      "Epoch 316 Loss 0.3616 Accuracy 0.2965\n",
      "Time taken for 1 epoch: 383.79067397117615 secs\n",
      "\n",
      "Saving checkpoint for epoch 317 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-317\n",
      "Epoch 317 Loss 0.3585 Accuracy 0.2979\n",
      "Time taken for 1 epoch: 379.27555203437805 secs\n",
      "\n",
      "Saving checkpoint for epoch 318 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-318\n",
      "Epoch 318 Loss 0.3582 Accuracy 0.2976\n",
      "Time taken for 1 epoch: 377.9180929660797 secs\n",
      "\n",
      "Saving checkpoint for epoch 319 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-319\n",
      "Epoch 319 Loss 0.3568 Accuracy 0.2979\n",
      "Time taken for 1 epoch: 378.8955109119415 secs\n",
      "\n",
      "Saving checkpoint for epoch 320 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-320\n",
      "Epoch 320 Loss 0.3534 Accuracy 0.2988\n",
      "Time taken for 1 epoch: 386.30036306381226 secs\n",
      "\n",
      "Saving checkpoint for epoch 321 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-321\n",
      "Epoch 321 Loss 0.3549 Accuracy 0.2985\n",
      "Time taken for 1 epoch: 376.9264190196991 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-6711fc30508b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 589\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EPOCHS = 500\n",
    "print(f\"Transformer has already trained {last_epoch} epochs.\")\n",
    "#print(f\"rest epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# writing information on TensorBoard\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "epoch = last_epoch\n",
    "while(1): \n",
    "    start = time.time()\n",
    "  \n",
    "    # reset TensorBoard metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        #print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          #        epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    # save checkpoint for each epoch\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    # loss and accuracy showed on TensorBoard, supported on Tensorflow 2.0.0-beta\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "    \n",
    "    epoch += 1\n",
    "    if (train_loss.result() < 0.1):\n",
    "        break\n",
    "\n",
    "# 2187 data\n",
    "# 150 epoch -> 29 hours\n",
    "# Epoch 151 Loss 0.7426 Accuracy 0.2098"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(inp_sentence):\n",
    "    start_token = [tokenizer_topic.vocab_size]\n",
    "    end_token = [tokenizer_topic.vocab_size + 1]\n",
    "  \n",
    "    inp_sentence = start_token + tokenizer_topic.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # <start> token\n",
    "    decoder_input = [tokenizer_abstract.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  # increase batch dimension\n",
    "  \n",
    "    # auto-regressive，generate one word at each time, and add into Transformer\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # create ner masks when generating a new word\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        # return the result if the predicted_id is equal to the <end> token\n",
    "        if tf.equal(predicted_id, tokenizer_abstract.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation(sentence):\n",
    "    predicted_seq, _ = prediction(sentence)\n",
    "\n",
    "    # filter <start> & <end> tokens and turn back english tokens\n",
    "    target_vocab_size = tokenizer_abstract.vocab_size\n",
    "    predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "    predicted_sentence = tokenizer_abstract.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "    print(\"sentence:\", sentence)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"predicted_seq:\", predicted_seq)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"predicted_sentence:\", predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: machine learning on product review data for recommender systems\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor(\n",
      "[8190 3184    7 2388    2 7121   68 1102    9  415 1494 1909  758    1\n",
      " 5250  402    8    1  535  617  864 8047   22  828   41 6893   54    4\n",
      "  164   14 3178  123    3  759    1  828   41 7983 2542 8050    6   26\n",
      "    9    5  384    2   37  156   41 8043 1042 7966  718  156    7   25\n",
      "    9  154    3  345    1  407  959  488    6    1  184    2 2329  102\n",
      "  105 1851    4 1133 5109    6   93    9    5  721    2 1941    3  208\n",
      "    1  384    2 1370 5334  699    1   78 1361 2956 3731  195   32   39\n",
      " 1370 2367    7   25    9 1786    3  208  142    1 1873 2188  102   78\n",
      " 1256   37    8   11  358    6    1  959 4895    9  714   22  677   37\n",
      "    3  527    1  154    3 1861    1  828    7    1  828    7    1 2329\n",
      "  102    1 2329  102  450  828    7   81 1371    9 1592    3  921    1\n",
      " 1102    2    1 1873 2188  102    1  685  137    6 8043   41 8043   41\n",
      " 8043   41 7983 3113 2893 8043   54    9   42   19    1  685   60    2\n",
      "   81 2329  102    1  685   37  356    2    1 1552 5511   54    4 2681\n",
      " 3677    2    1   37    4    1  450  346   20    1  156    6    1  386\n",
      "   17  255    4    1   72  242    3    1  242  123   32  255   37 1812\n",
      "    2    1  146  329   31    1 1102  102    1 1102    2 1102    2    1\n",
      " 3382 1807 3518 3730   35], shape=(243,), dtype=int32)\n",
      "--------------------\n",
      "predicted_sentence: recently , analyzing of ontologe similarity is often diamonting the remarkable behaviour in the tasks especially seq - network ( smt ) and so that reinforcement method to represent the network ( 1 + t . it is a type of data model ( mri space model , which is important to solve the relevant graph models . the problem of links between other entities and semantic values . there is a lot of corpora to find the type of renowners the two uniquerier has been rency , which is connected to find out the above relationships between two dimensional data in this problem . the graph dictionary is pre - structured data to simulate the important to combine the network , the network , the links between the links between neural network , its operation is simply to train the similarity of the above relationships between the experimental data . m ( m ( m ( 1 ).m ) is based on the experimental model of its links between the experimental data structure of the fundamental frequenc ) and cross validation of the data and the neural network with the model . the experiments are given and the results related to the related method has given data storage of the models generated from the similarity between the similarity of similarity of the unavailarl .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"machine learning on product review data for recommender systems\"\n",
    "print_generation(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: implementing dynamic dewey labelling scheme on xml database systems using java programming language\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor(\n",
      "[8190    1  267    2   11   21    9    3  203    5  730   13 7629 7966\n",
      "  475 4172   40   31   81 3705 1233    6   11  173    9    3  122    5\n",
      "  141 4172 7966    8  174  555    6   67    2    1 3002   55   29   13\n",
      "    1  750 1068   17   85   24   33  141  237    4 2023 2601 2274 8045\n",
      "    6    1  750 1068   83    3  122    5   55 2934    7  231  231  231\n",
      "  231  231  158    2   11   55  538    8  717    7    1   55 2453    7\n",
      "    1   55   83 1831   19    1  339  156    7    4   85   42   19    5\n",
      "  141  124   36   85   33  141 3719    6    1   55   36   85  141 3719\n",
      "    6    1 1414   25   36   34    3 7303 2023 2601 3208  807  378  222\n",
      "  421   31  174  629   31    1 5676 2426    7  174  421 4771 2471   35], shape=(140,), dtype=int32)\n",
      "--------------------\n",
      "predicted_sentence: the purpose of this project is to implement a speaker for compact word printing from its subsum . this dissertation is to develop a java print in xml database . one of the jast software system for the initial stages are developed by using java program and japanuto . the initial stages were to develop a software specifications , first first first first first part of this software engineering in addition , the software parser , the software were placed on the background model , and developed based on a java language was developed using java servlets . the software was developed java servlets . the python which was used to indexed japanoraet al : document from xml files from the cdsvm , xml document labelled .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"implementing dynamic dewey labelling scheme on xml database systems using java programming language\"\n",
    "print_generation(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: generating text with neural network in natural language processing\n",
      "--------------------\n",
      "predicted_seq: tf.Tensor(\n",
      "[8190  314  124  624    7  166  126    9  415  340   13 1926    6   87\n",
      "  219    3  345  314  124  624    7  321    2  166  126  435    6   87\n",
      "   38   39    5  212    8   11 2731    6   87  316  310    3  345  708\n",
      "  166  126  159    8  708  166  126  159 1043  192  159 1632  311    3\n",
      "  345 1424   22  244   22  166  126  118   17  415   34    3 2727  114\n",
      "  314  124    3  345   49   78  159   20   49   78  159    4  345   48\n",
      "  159    4  345   48  484    6    1  317    2   11   21    9    3 2727\n",
      "    1  212    2  314  124    3  707   77    3  345 1424   22  319    2\n",
      " 7987 7989 7983   22  319    4 6250 8049    6 7983   22  319    2 7983\n",
      "   22  319    4 8036   22 7983   22 7983   22 7983   22 7983   22 7983\n",
      "   22 7983   22 7983   54    4 7983    6 7983    6 7983   22 7983   22\n",
      " 7983   54 7983    6 7983    6 7983    6 7983    7 7983   54    9   34\n",
      "    3 7983    6 7983   22 7983   22 7983   22 7983   54    4 8036  465\n",
      " 1258   34    8 7983    6 7985    6 7983    6 7983    6 7983   22 7983\n",
      "   22 2538 6368    7 7983    6 7991 7909 2538 6368    7 8036 7983   22\n",
      " 8036 7983   22 8036  465 1258 5421    7 8036 7983    6 3658 7966   60\n",
      " 7983    6 7986 1402 8036  465    4 8036  465    4 8036  465    4 8036\n",
      "  465    4 8036  465    4 8036  465    4 8036  465    4 8036  465    4\n",
      " 7908  506 1672    2 5105    2 5105    2   49   72  178 1402    3 7909\n",
      "    6 7986    6 7986    6 3658    6 7986 1402 1258   27  185    1   72\n",
      "  178 1402    4 7982    6 3658 5084   93    9    5  129    2    1   72\n",
      " 3008   14   47  178  506  319  894   15   47  178    1  308   37  165\n",
      "    2 7983    6 7986 2891 7991  506  319  894   15   47  178    1   72\n",
      "  178    1 7991 7984 7991 7984 7991  506  319 4647  157 7907    6 7986\n",
      " 7989 7991    6 7986 1402    4 7982    6 7986 5074 7983 2715 7982    6\n",
      " 7986 2891 7991   35], shape=(354,), dtype=int32)\n",
      "--------------------\n",
      "predicted_sentence: natural language processing , machine learning is often useful for humans . we need to solve natural language processing , because of machine learning techniques . we have been a task in this thesis . we present approaches to solve four machine learning algorithms in four machine learning algorithms reduce our algorithms correctly applied to solve low - to - machine learning methods are often used to translate human natural language to solve these two algorithms with these two algorithms and solve different algorithms and solve different algorithms . the goal of this project is to translate the task of natural language to learn how to solve low - accuracy of 571 - accuracy and indicators . 1 - accuracy of 1 - accuracy and f - 1 - 1 - 1 - 1 - 1 - 1 - 1 ) and 1 . 1 . 1 - 1 - 1 ) 1 . 1 . 1 . 1 , 1 ) is used to 1 . 1 - 1 - 1 - 1 ) and fg dataset used in 1 . 3 . 1 . 1 . 1 - 1 - 9 entities , 1 . 9659 entities , f1 - f1 - fg dataset kernel , f1 . 14 model 1 . 45 fg and fg and fg and fg and fg and fg and fg and fg and 68 % precision of vertices of vertices of these results than 5 to 65 . 4 . 4 . 14 . 45 dataset can improve the results than 5 and 0 . 14 %. there is a number of the results indicated that more than  % accuracy cannot be more than the training data set of 1 . 48 9 % accuracy cannot be more than the results than the 92929 % accuracy reaches 84 . 479 . 45 and 0 . 4491 %, 0 . 48 9 .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"generating text with neural network in natural language processing\"\n",
    "print_generation(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.750000\n",
      "Individual 2-gram: 0.333333\n",
      "Individual 3-gram: 0.000000\n",
      "Individual 4-gram: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'the', 'test']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGH Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0.49411764217577864, 'p': 0.5833333333333334, 'r': 0.42857142857142855}\n",
      "{'f': 0.23423422957552154, 'p': 0.3170731707317073, 'r': 0.18571428571428572}\n",
      "{'f': 0.42751590030718895, 'p': 0.5277777777777778, 'r': 0.3877551020408163}\n"
     ]
    }
   ],
   "source": [
    "hypothesis = \"the #### transcript is a written version of each day 's cnn student news program use this transcript to he    lp students with reading comprehension and vocabulary use the weekly newsquiz to test your knowledge of storie s you     saw on cnn student news\"\n",
    "\n",
    "reference = \"this page includes the show transcript use the transcript to help students with reading comprehension and     vocabulary at the bottom of the page , comment for a chance to be mentioned on cnn student news . you must be a teac    her or a student age # # or older to request a mention on the cnn student news roll call . the weekly newsquiz tests     students ' knowledge of even ts in the news\"\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hypothesis, reference)\n",
    "print(scores[0]['rouge-1'])\n",
    "print(scores[0]['rouge-2'])\n",
    "print(scores[0]['rouge-l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
