currently software testing is a crucial and labor - intensive component of software production . software testing requires test data generation with good code coverage . automated search techniques used for software test generation will be an improvement to efficiency and cost to software testing , and currently genetic algorithms are one the best options as a search heuristic for automated test generation . the project will investigate in cellular genetic algorithms , another subclass of evolutionary algorithms just like genetic algorithms , and their potential to replace genetic algorithms for test generation with even better code coverage . research and implementation were done in regards to how these techniques may be applied to test generation , and specifically how using cellular gas may improve upon using standard gas for test generation . the tests were done with the software tool evosuite which runs a ga on java classes to generate test - suites and outputs the gaâ  s performance on code coverage , and a cellular ga will be implemented into evosuite in this project and be compared to the standard ga in its performance for generating test data . results achieved showed general favor for cellular gas due to the advantage of creating more diverse populations than standard gas and also prevent premature convergence of solutions that more often converged at a local optimum instead of branching out searching for optimal solution for the standard ga .
the allocation of students and staff to projects in the department of computer science is a complicated procedure , not helped by its lack of centralisation . it relies heavily on emails between staff and students , and uses a number of online systems for different aspects of the process . this carries an inherent lack of transparency . this project focuses on producing an online portal to consolidate the management of project allocation , allowing each user to access a personalised overview of their stages in the process . by replacing the number of systems currently in use with one single portal , the project aims to simplify the process for all those involved . through a literature review , and through several consultations with members of the department , a set of requirements was identified and a new system produced . the result was a product of sufficient quality and flexibility to replace the existing systems , and the foundation for a portal to manage the lifecycle of projects in the department .
invertebrates make up 95 % of the species on this planet , and yet their importance is constantly overlooked . from shellac to birth control hormones , invertebrates are used in many unexpected places . to encourage a life - long interest in and respect of invertebrates , this project aims to create a website which will teach primary aged children about the different sorts of invertebrates which we encounter in everyday life and the ways in which they change the world around them . by using david kolb ' s theory of experiential learning to teach children more effectively , pomorum . co . uk aims to be educational and fun , whilst also teaching them to see invertebrates in a new light . including elements from the uk national curriculum brings pomorum more relevance in schools , and hopefully turn it into a website which can be used in classrooms around the country .
the athlete tracking market has been expanding greatly over the past few years , due to the growing smartphone market and the introduction of popular free mobile applications such as strava . this has grown to include recreational athletes and just elite athletes . however , these applications tend to rely on the use of global positioning systems ( gps ) and as a consequence only work reliably outdoors . they do not aim to track positional movements nor do they attempt to calculate as much data as elite athlete tracking systems . this project aims to look into tracking technologies , specifically looking at the feasibility of using apples new ibeacon technology to create an indoor based athlete tracking application for ice hockey . this would have the potential to be used by athletes at both elite and recreational levels at a relatively low cost .
the primary objective of this dissertation is to provide a bio - inspired solution to the multiple travelling salesman problem ( mtps ), with the secondary objective of allowing a user to input geographical locations . this resulting in the output of an optimal route , visiting each location once , and only once , returning to the depot node at specified intervals . this report outlines several bio - inspired techniques that can be used to solve the tsp and mtsp , resulting in the decision to use an evolutionary technique ; genetic algorithms , based on merits and external factors . following this an in depth discussion of the requirements , design and implementation of a python based implementation is conducted , with the conclusion being multiple selection and optimisation techniques being implemented . tournament selection with various tournament sizes and roulette wheel selection with different levels of elitism imposed were compared , with it being found that roulette wheel selection with high levels of elitism imposed produces superior results for the uniform depot visit frequency mtsp . various parameters were also experimented with , and the impact of a sub - tour optimisation technique being evaluated , with the result of this being a drastic increase in fitness . as a result of this project , a python programme that is capable of solving the mtsp , whilst interacting with google maps has been produced , which provides near optimal results on most test cases .
in the last ten years , the cyber world has seen the proliferation and evolution of social networking sites , from friends reunited to facebook . engagement in such sites is phenomenal , and the ability to harness successful elements from them may be useful in manipulating human generosity by charity ( non - profit ) organisations , particularly at a time of austerity and increased regularity of major natural disasters . this project aims to measure whether social networking techniques and activity significantly affect a person ’ s decision to donate more tokens to charity sites . the addition of social networking features to a set of charity sites reflecting genuine concern for others ( altruistic ) and self - interest ( selfish ) may influence a natural tendency to donate more to those sites . features such as the presence of more human faces , names of supporters and information regarding how they donated ( displayed in the form of a news feed ) may socially influence a user ’ s decision to donate .
cloud platform providers regularly offer a wide range of generic services , which can be combined with one another ( and with user - specific apps ) to create very complex composite systems . the number of clients , services and applications available on typical platforms is growing rapidly , and this has obvious management implications . the number of services between which interactions take place means that rogue or faulty services need to be identified quickly ; at the same time the proliferation of knock - on effects and emergent behaviours mean that human intervention is no longer enough . cloud platforms need to be self - managing - services need to monitored in real time to spot problems , and remedial action needs to be taken as quickly as possible to ensure that service level agreements are not violated . research has recently begun into techniques for monitoring cloud service characteristics . at the same time , methods based on stream reasoning , the semantic web and cloud sensor networks are being developed which could feasibly allow platforms to replace faulty services with best - possible replacements as and when the need arises . goals will be to : review research into monitoring of cloud service characteristics , identify areas where further work is required , suggest possible solutions , develop a proof of concept demo system showcasing identified behaviours , present experimental results showing the extent to which solutions behaved as expected
information is never strictly the result of a single medium , for example when looking at a news article not only is there a body of text , but an accompanying image . to attempt to find links between the articles and the images , this project investigated the ability to use image classification methods to attempt to discover links between images and the articles of a popular news website with the aim of being able to predict from what category the image came from , and onto attempting to see if the images can be tied to important words in the article . beginning with simple binary classifications the project then expands to attempting to predict article categories based on the images and finishes by using important words from the article text as a category to attempt to predict important words in the article from the image alone . the project discovered that binary image classification is fairly easy to implement and achieves good performance , image classification for categories is harder but still achieves performance of over double the accuracy of guessing , and categorising images based on the important words from the article is very difficult .
over 500 million tweets are broadcast daily on twitter . the volume of information is inconceivable to a mere human , however by using a tool such as sentiment analysis , defined as the extraction of opinions or emotions from raw data , we can attempt to derive meaning from it . this project will explore the potential of sentiment analysis on twitter data in order to predict the public response to a public event , notably to forecast the outcome of a reality television series . this provides an apt domain for experimentation in this field , as there are multiple public votes throughout a short window of time .
this project aims to develop the automatic translation of state machines to object models , as a part of the wider remodel project in model - driven engineering . the ultimate goal is to construct a model transformation tool using the java programming language that can translate a high - level abstract state machine into a low level object - oriented model of a system . the first step is the development of an xml dialect for modelling state machines , to be known as remodel asm ( abstract state machines ). this xml dialect may eventually include concepts such as state nodes , transitions , events and guards , choice and merge nodes , and start and end points depending on the desired complexity level . the second step of the project is to develop a framework for the translation of remodel asm into remodel oop , a pre - existing xml dialect , which presents the classes and methods found in an object - oriented language . translators already exist to convert remodel oop into executable programs .
the overarching aim of this project was to develop a playable video game inside of the logisim digital circuit simulator . this video game could be played , analysed and edited by future students ; creating a unique and hopefully fun learning tool . the game chosen was helicopter - a flash game available online , where the key objective is to apply thrust to a helicopter in order to avoid obstacles on a side - scrolling map . it was successfully designed from its very onset which included drawing the game interface , programming the game - play and displaying an overall game score . the components of the game circuit can be analysed within logisim and there are several improvements which could be added to the overall game by future students . whilst initial intuition and research had suggested that a cpu would need to be designed to run the game code - the specification was fully addressed using only logic components to represent entities in the game with the only stored information needing to be the map designs . interestingly a potential limitation of logisim was also discovered , namely variability in clock speeds when being run on different computers affecting the performance of circuits and therefore the difficulty level of the game .
blu - ray disc is the leading high definition video format which allows advanced online content and internet connectivity on bd - live capable players . this project aims to use blu - ray ’ s java capability , to provide a library of functions for facilitating the development of applications which take advantage of this internet connectivity . this dissertation focuses primarily on the design and implementation of a bootstrap application , allowing the download of content from the internet , and its addition to the content available on disc via a virtual file system . this bootstrap application can then be adapted to the content providers needs .
sentiment analysis is a field of great interest to nlp researchers , to businesses and to politicians . over recent years , the explosion of social media and web 2 . 0 platforms have made available vast quantities of data that make sentiment analysis both more feasible and desirable as manual methods become insufficient . it is however greatly domain dependent . tailoring sentiment analysis applications for specific domains by hand is a time consume task . this paper aims to investigate and evaluate a method for the unsupervised induction of sentiment lexicons from large amounts of domain separated data . we also aim to determine to evaluate the performance differences between general and domain specific lexicons , if indeed there are any , at unsupervised lexical sentiment analysis . the dataset used in this project is comprised of every publicly available reddit comment from october 2007 to may 2015 .
nowadays , people use web applications every day to aid their personal life and work . one specific area where web applications are particularly relevant is the entertainment and sports industry . modern web technologies provide facilities for real - time sport statistics tracking to the public and thus enhance online entertainment and betting businesses . the objective of this project is to create a lightweight html5 in - play football monitoring application running on a skybet analyst ' s ipad . the application ' s aim is to gather , utilise and store statistics about football teams and players . this data could be potentially used by the company to improve their current sports betting systems . thus , this project explores viable cutting - edge technologies and aims to build a fast , reliable and user - friendly application to perform in - play football monitoring .
now that , as a society , our information needs are nearly wholly satisfied by the internet and the content therein , we as users possess a set of tools which complement each other so well as to greatly improve our experience and understanding of information . this project aims to unify the process of reading news articles the contain geospacial information with the generation of appropriate maps to provide a visualisation of where events are happening or have happened and by this process , provide context for news articles where the user may not possess an understanding of the geography of an area .
as hardware has become increasingly powerful , the doors have been opened for a wide range of more computationally intensive simulation procedures . in particular , agent - based modelling has seen a recent surge of interest in the fields of biology and economics . for this project we propose using an agent - based model to create an implementation of the classic physics problem of cosmological n - body simulation , in order to investigate the process of galaxy formation . our simulation should be displayed as an evolving 3d graphic , to allow for easy interpretation of the data . the gpu , with its massively - parallel architecture is well suited to the computation of n - body systems , and allows us to create high - fidelity simulations using mid - level consumer hardware . we therefore also investigate methods for performing our model ' s calculations on the gpu . the primary aims of the project are to determine whether agent - based models are suitable for the investigation of physical phenomena , and to produce a realistic model of galaxy formation in doing so .
an experiment was run in order to determine the effect of a humanoid robot called " nao " on the learning of children with autism compared with a traditional human teacher method and an e - learning method utilisng brian cox videos . methods : the quality of the teaching was measured using a multiple choice quiz with questions based on the taught content , the enjoyment and satisfaction which was gauged using a binary yes / no style questionnaire after the quiz had been undertaken . the videos were ( 1 ) black holes , ( 2 ) searching for liquid water on mars , and ( 3 ) star death and the creation of elements . results : the results are as follows : t - tests between the different conditions meant that we could not reject the null hypothesis for the quality of nao ' s teaching . however , the null hypothesis for the human teacher could clearly be rejected , and as such , this teaching method was deemed the highest quality . comparing just these two , the null hypotheses for nao ' s and the video ' s quality of teaching could not be completely accepted . a chi squared test for the questionnaire results meant that we could not reject any of the null hypotheses for the enjoyment and satisfaction of any of the teaching methods . conclusions : we conclude that the quality of nao ' s teaching was lower than that of the human teacher ' s ( p = 0 . 000123 ) there was no significant difference between the quality of nao ' s teaching and the video ' s ( p = 0 . 5388 ). we could not find enough evidence to suggest which condition yielded the most enjoyable and satisfactory learning experience ( probability level ( alpha ) value = 0 . 4830 ).
translating the bpel to dfa is not a easy thing to do . the most crucial components that involved in this project are : bpel , java code , antlr and some other self - defined grammar rules , etc . briefly the bpel ( business process execution language ) is the way to express how the simple web services can be constructed to a complex one with the order of certain combinations . so if there is something missing or the program with wrong order then the whole system would not work properly . the data flow algebra is a process of methods integration , which means the integration of formal and diagrammatic models of the methods used for developing the software systems . on the other hand , it also defines the components in the process , which is about how each component interacts with others and what forms or orders they should be . to build a bridge from bpel to dfa there are several tools that should be used . first of all , java language is the selected programming language used in this project . secondly , t he basic idea of constructing the project is using antlrwork to create some grammar files , then let antlrwork to generate both lexer and parser code from these grammar rules which are created before . then make a test file and put it into the system and let the antlrwork read it . on the other hand we can also use eclipse or java netbeans to make the test files . however the most suitable software that i recommend is still antlrwork which is more convenient and has better mobility on both compiling and translating the code . last but not the least , the bpel should be translated according to these grammar rules into dfa .
telepresence is the ability to be present in an environment from a remote location . regardless of whether or not true presence is achievable , the practical benefits of widespread adoption of telepresence are many , such as the revolutionising of economies by eliminating geographical barriers leading to increased mobility of labour . environmental benefits are also evident , if one can be present in a remote environment from their current location why then suffer the cost of greenhouse emitting forms of travel ? through the use of the kinect motion detector , nao robot and oculus rift the project will aim to create a telepresence system which maximises the feelings of presence , experience with the constraints of the hardware . the main features implemented to achieve this aim in the resolving system involve , the imitation of a teleoperator ' s movements by the nao and the real - time display of the user ' s emotions through its leds in colour representation .
loneliness is something which is experienced by most people at some point in their lives . it can have negative effects on the sufferers ' health as well as general wellbeing . it is known that animal - assisted therapy and owning pets helps to reduce the effects of loneliness . recently social robots which , are made to look like animals have become more readily available . given that these robots look and act like animals , this paper tests to see if they can be used in the same was as animal - assisted therapy . in this paper the paro is the robot being used to test the hypothesis . the experiment consisted of a literature survey to collate current information regarding the background of the project and the experiments used and their results . the experiment had two sections , an online survey and a human study . the results from this study did not reject the null hypothesis , this could be down to some constraints in the experiment . this are is still very promising so the final section on this paper is the further work in the field and how to improve on the experiment .
in automata theory , finite - state machines are a mathematical model of computation , which are comprised of one or more states and transitions between such states . they can be used in the modelling and analysis of systems , whether it is hardware or software . a push - down automata is one such example of a finite state machine which makes use of a stack . a context - free grammar is used to describe how to construct all possible strings in a given formal language . a context - free grammar contains production rules which can be recursively substituted until a string is produced . in automata theory , it can be proven that every push - down automata has an equivalent context - free grammar which defines it and the process of doing so appears in various automata and formal language literatures . the purpose of this software is to aid in the learning and visualisation of this process ; the conversion of a push - down automata into an equivalent context - free grammar . this process entails the production of a drawing tool , allowing the user to create a push - down automata . then , the pushdown automata must be altered to adhere to certain preprocessing requirements . from this altered push - down automata , production rules can be produced to form a lengthy context - free grammar . finally , this lengthy context - free grammar must be simplified to leave a concise context - free grammar which defines the language of the original push - down automata the user created .
xml stands for extensible markup language that encoding documents in a format of both human readable and machine readable by following a set of rules that defined in the xml 1 . 0 specification produced by the w3c . the main goal of xml is designed to structure , store , and transport information . in this project , we are going to find out which is the most java friendly xml api . this will include possible candidates like java dom , java sax , jdom , jaxb , jaxen and sheffield ' s local contender , jast . the project will use 19 carefully designed events to complete the experiment in terms of memory consumption , speed of processing , package features , xpath capability , validation support , and user experience . the result of project indicates that jaxp is the best processor in overall performance . jaxt is good at memory utility and provide a more comprehensive function . furthermore , jaxp and jast share the winner of user experience event .
this project is about building a working computer inside a video game called minecraft . it aims to do so by setting a specific set of requirements and then attempting to meet them . the literature covered in the technical survey section aims to prepare the user by explaining the underlying theory behind building circuitry inside minecraft . it covers topics which are translated into the video gameâ  s world in the following chapter . the computer itself is entirely based in minecraft and implements its own instruction set and componentry design . the report will takes the reader through all stages of designing and implementation and will attempt to give a reason behind all of the decision making . we will then test the entire system and provide the results , analysing them carefully and explaining their use . finally , we will discuss the potential future expandability options and the finished productâ  s current strengths and weaknesses .
reinforcement learning is an area of machine learning which is focused on how software agents take actions within an environment to maximize a reward function , the aim of this is for the agent to determine the ideal behaviour within its environment , to maximize its performance . in 2015 google deep mind developed a reinforcement learning agent capable of learning and playing atari 2600 classic games using high dimensional sensory inputs . this agent surpassed skilled human ability within many of the games tested . this project aims to develop a game of battleships then apply several deep reinforcement learning tachniques in order to create an agent that can play the game and learn through play to improve with an aim of outperforming current methods for agents to play the game .
endocytosis is the process of a biological entity entering a cell . to a virus this is vital as it requires the proteins found in the cells cytoplasm of to replicate . this project applies graphics and animation techniques to this process producing an animation . graphical representations of both cells and viruses are animated , detecting and responding to intersections with other objects and boundaries . implementation is courtesy of two well established technologies , java and opengl . server side programming is researched for the implementation of an on - line library of entities , accessible for all over the world wide web . both client and server systems are specified using formal object oriented design methodologies . this report critically details the technology , techniques available before selecting the most appropriate . the system design is outlined , and novel parts of the implementation explained further . finally a critical discussion of work and test results concludes the document .
sensors are closely coupled to effectors in reactive robot controllers . such controllers can be used in artificial evolution , which is the simulation of natural evolution . such evolution is an attractive technique to implement because of its simplicity and power . genetic algorithms and neural network controllers are tried and tested experimental methods for the study of artificial evolution . this project implements reactive robot controllers and their artificial evolution and co - evolution . co - evolution is the simultaneous evolution of more than one population of individuals . each population acts as a changing task or environment for the other and it is commonly investigated using predator / prey scenarios , both in simulation and on real robots . the project report begins with an overview of robot control , artificial evolution , and genetic algorithms . neural nets and their evolution , evolutionary robotics and finally co - evolution are then reviewed . it next moves on to the implementation , design and testing of a realistic robot simulation , which formed the bulk of this project . the simulation of motion production and representation are especially considered . after this , some preliminary experiments in robot evolution and co - evolution are described to prove the realistic functioning of the simulation . these include the alteration of neural network controller architecture , as well as parameters of the robots and genetic algorithms . the experiments reinforced the crucial importance of the environment , which is appropriate in a project dealing with reactive robotics . miscellaneous results showed the evolutionary power of simple neural networks with recurrent connections and sigmoid transfer functions . many interesting questions were raised but unfortunately time did not allow their investigation .
speech recogniser output is an un - segmented , unpunctuated stream of text . this report investigates the use simple word rate statistics to locate document boundaries within these texts . existing methods of topic boundary detection are presented before developing a system based upon self - similarity analysis . this model analyses the text streams graphically for adjacent areas of similarity , attempting to group these areas into documents . an algorithm to locate the values manually is implemented in order to attempt the automatic location of document boundaries . measures of similarity to use with the streams are developed using simple word rate statistics . they are evaluated using the results from the automatic algorithm , from comparing known document boundaries and boundaries detected by the algorithm over the similarity data produced using the measure . the shortfalls of the model are analysed suggesting why this method may proves less effective than some of the alternative models when using speech recogniser output .
the idea of home automation has been portrayed in science fiction films ever since someone first came up with the idea of being able to talk to a computer . now in 2003 many of these home automation ideas are becoming reality . it is possible to control the lights , curtains , kettle and even the fridge from the computer . with the expansion of the internet and increased availability of high - speed connections , more people want the ability to control their homes while they are away . this project looks at how current home automation technologies can be improved to offer more control over a greater number of devices . it also looks at how these new devices could then be controlled from the computer , by means of a java application .
a caricature is a comic representation of a person by exaggeration of characteristic traits . this report details the development of a caricaturing system that attempts to automate the process of caricaturing an individual as far as possible . the system is unique in that it produces the caricature in 3d . the system uses orthogonal photographs to gather the 3d information about the individual compares this information to an average and then deforms a generic head model to represent the differences between the two . the report details the software engineering process of development of the system from requirements capture to testing and evaluation of the system .
research has been conducted into previous and current navigation techniques and systems in the area and into the hardware that will be used . during this project a reactive robotic system that uses infrared beacons as signposts to direct it the robot towards a goal was designed and implemented on hunter robot . the robot also has an object avoidance system so that it does not collide with objects in its path .
an utterance of a dialogue agent in interactive dialogue can be produced in a wide number of linguistic forms and styles . an agent ' s choice of linguistic form leads the addressee to make particular inferences about the agent ' s personality and attitudes towards other people . the aim of this project is to explore the notion that linguistic style is a key aspect of character . it is designed to experiment with the believability of dialogue agents , by discussing and extending related work in the field , and by testing users ' reactions to a virtual dialogue simulation . included is a detailed review of the relevant work conducted in this field , linked through similar ideas and notions relating to personality and emotion . similar work is analysed , and the key concepts are used to decide upon a suitable implementation .
landscape rendering is an area of computer graphics that is becoming increasingly more popular as the advances in graphics hardware are making such world possible to model . the aim of the project is to investigate the techniques related to render real - time applications and to develop a real - time walk through of a realistic landscape . there are three parts to the project . plant generation part covers the generation of plants using l - systems as this allows access to the structure of the plant that can later be used in optimisation . the landscape generation covers the generation of the landscape using a simple ecotope model . n the rendering is performed using a quad - tree to reduce the number of plants render and billboard objects in the distance along with a custom method called t - shaped billboards . the project was successful in implementing these features and the walk through of realistic landscapes is possible .
large web projects often suffer from communication problems with project leaders having difficulty in keeping track of which developers are updating what files and where . without any management system in place there is no way of tracking which developers have made revisions to files which causes confusion and conflict as one developer overwrites the work of another . this project will concentrate on researching the various techniques available for solving the version management problem which include differencing and merging of web pages . the term ' edit script ' will be introduced , and by storing subsequent versions of web pages as a set of instructions transforming one page into another , it will be possible to vastly reduce the amount of storage space required for a ' project repository '.
since alan turing ' s paper ' computing machinery and intelligence ' ( 1950 ), there have been many attempts to create a program that could pass the turing test , none of which have succeeded . while many people still see this as their goal , others have now moved away from actually trying to prove intelligence and instead are concentrating on creating programs that merely give the illusion of intelligence . their goal is to incorporate such programs into our everyday lives , starting with the internet . the aim of this project is to see to what extent such programs would be considered useful . to do this users are asked to test each of three programs , all of which book a table at a restaurant , and then rate the usefulness of each . the results achieved show that not only are conversational programs considered useful but also that the way in which the program converses affects the perceived usefulness .
the project aimed to design and build a tool that would enable a designer to create complete specifications for object oriented components using motive . the user would interact with the tool using a graphical user interface programmed in java . the final system enabled the user to classify methods as " create ", " modify " or " access " within the object type view . the object algebra was then able to use these methods to create axioms . the object machine view allowed the user to insert states , transitions and regions into the drawing area and change both their size and position . an interface was created that could possibly be expanded in future to allow designers to create complete specifications for object oriented systems .
l - systems provide a way of creating visually appealing representations of plants . they are a form of string rewriting system combined with appropriate methods for converting a string into a 3d ( or 2d ) image of a plant . the project implements a method of using l - systems to model a community of plants fighting for space within an environment . environmental factors such as sunlight and water are taken into account in the system produced .
in the field of computer graphics , the process of converting vector images to raster images known as rasterisation is a trivial process which can be implemented with relative ease . however , in order to achieve the reverse process of rasterisation known as vectorisation is extremely difficult . this project will centralise around the process of vectorisation . this project was initially proposed by a member of the history department from the university of sheffield . the aim is to implement the vectorisation process and apply it to digitised historical maps to extract boundary and landmark information . we shall discuss various methods of edge detection and shape recognition . an implementation of the canny edge detector has been created combined with a chain coding algorithm which allows an efficient conversion of a raster image to a vector image . the canny edge detector contains a modification of the standard gradient detection algorithm which helps prevent ' double vision '. double vision occurs when a line is recognised as two edges . all this is wrapped up in a system which allows manipulation of the image after edge detection providing an all - in - one drawing package .
the study of touch in virtual environments is a relatively new field of research . it is important to the advancement of human - computer interaction that we have a complete understanding of how we perceive the things in the world around us . this paper is a study of perception based on touch and vision . through experimentation looking at perceived size of virtual objects and our perception of texture , this study explores the importance of vision and touch .
e - learning is a relatively new technology , providing businesses and academic institutions with a means to supplement training and provide a resource that bypasses the constraints of time and geography , only bound by the need for a device with access to the internet . one problem with this technology is the lack of positive user interaction . this project aims to address this issue and to investigate the application of e - learning technologies to provide a supplementary resource for the com2020 abstract data types module .
in many three - dimensional computer games there is an element of demolition , for example a character may be required to break a wall . whenever such an object in a game is destroyed , the animation is not realistic since the impact point is static and there is no sense of force associated with the impact . the aim of this project is to produce a technique that can realistically animate an object being demolished in real - time . this includes creating a random crack pattern based on the characteristics of the original impact and applying it to an arbitrary object . in this work , techniques for creating a crack pattern are discussed and then one is implemented in a program that can break up objects . the program is evaluated and proves that it is possible for the real - time demolition of objects and that the techniques developed here could be used in computer games in the future .
the government funded project called the cistercian in yorkshire project wishes to create a realistic soundtrack to accompany a virtual walkthrough of roche abbey . this project explores acoustic rendering techniques for enclosed spaces focusing on ray - tracing . the aim of this project is to create a system , which implements the ray - tracing algorithm for arbitrary 3d geometric models . the ray - tracing algorithm has been applied in three dimensions as a 3d ray - tracing program . a model of the abbey has been collected in vrml . this model has been judged to be too complex and thus a simplified model of the abbey has been created in java3d . using models created in java3d , a file containing information on the impulse response is created . head related transfer functions have been explored and used to add a 3d spatial effect to the impulse responses created by the ray - tracing program . the design , implementation and evaluation of the system are followed , including the assessment of the final system . this is completed using a variety of strategies and techniques . difficulty areas of the project are discussed along with improvement and further work prospects .
this project proposes to build a system which implements summarization techniques . often we don ' t want to read a text in its entirety , and would prefer to read a shortened version that highlights the salient points . the goal of text summarization technology is to provide such summaries automatically . in one short sentence we could define the objective of the proposed system as : " watch the news and tell me what happened while i was away ." commercial systems which carry out summarization tend to rely on relatively simple heuristics , such as selecting and concatenating the first and last sentences in each paragraph ; the summariser in word 97 works in this way . research systems tend to focus on more sophisticated techniques . in particular , a long term goal has been the development of techniques which first analyse the text in order to derive some representation of its content , and then select from this content in order to generate a shorter form . this project combines two techniques maximal marginal relevance ( mmr ) [ 1 ] and text tiling [ 4 ] in order to build an automatic news summarization system . we evaluate the quality of the results obtained from these techniques using precision and recall measures [ 6 ]. the results obtained from the evaluation of the system shows that about 75 % of the automatic generated summary were consistent with those given by human judges as being relevant to the original news story document . we conclude that the chosen approaches for the project are good and believe that interested person ( s ) in this research area will undoubtedly find the techniques presented useful .
the popularity of wireless communications is at an all - time high . an increasing number of users are relying on mobile access to the world wide web and especially data structures stored on remote servers . the cost of wireless communication means that not all users can access their data on - demand . this encourages us to explore ways of looking at remote data while minimising the cost . in this project we aim to discuss various prefetch strategies . this involves synchronising data just before the user checks his or her device , so that the user is always looking at the most up - to - date replica of the data structure . we explore various synchronisation strategies that aim to minimise connection costs whilst performing prefetch . we then develop an application for a mobile device that monitors a variety of device activities ; this data can then be used to optimise synchronisation times . we also look at the current wireless services available , and discuss the feasibility of prefetch strategies in the current world .
the animation of virtual character is an active field of research . the face is a special part of this character due to its complexity and its communication abilities .
document retrieval and summarization is a developing area of re - search , but most of this research is concentrating on typed text doc - uments . there is a vast amount of spoken documents available to us , and a large amount of this would be very useful to people if only there was some way of nding exactly what they were after in the archives easily . most people tend to ignore these archives however be - cause there is no way of searching them easily , and even when it has been searched they would then have to sit through the actual shows before deciding on how useful the document is and what information it contains . the goal of this project is to produce a system that solves part of this problem for a collection of spoken news broadcasts . we have avail - able a selection of transcripts of north american news broadcasts . the aim of this system is to produce summaries of these transcripts using a variety of dierent summarization methods . the nal system contains a number of ways to summarize a given news article , and a system for storage and retrieval of these articles . the way that it was built means that it is possible to use the system from anywhere on the internet , and access the data remotely .
the world of biomedical science is a continually expanding research area which requires laborious and careful cataloguing , a process which is ongoing . one particular area of importance is that of protein discovery . there exist a vast number of proteins already identified with new additions on a daily basis , to maintain a comprehensive database of all these proteins is a thankless and demanding task that requires a lot of user input and regular updating . currently there are several online databases which attempt to provide accurate representations of this data ; however there is still a fair amount of inconsistency when contrasting these information sources . the aim of this project is to utilize techniques already in existence and implement added functionality to build a system capable of producing a comprehensive protein annotation system which requires little user input and is capable of automatically updating it contents as more and more proteins are discovered .
the field of text summarisation has been a key area of research for many years . now with the popularity and availability of information sources on the internet the need for effective automatic summarisation is clear . this report outlines the many of the techniques that have been attempted in other research and attempts to extend this . this report focuses on the work carried out by carbonell et al 1998 and their formulation of the technique known as maximal marginal relevance ( mmr ). this technique was originally developed to aid in information retrieval to broaden the scope of search results , however it is also widely used in multi - document summarisation to reduce redundancy . this report looks at applying this technique to the summarisation of single documents , with that intention that the novelty versus relevance ranking will produce worthwhile summaries . the evaluation of this project uses a corpus , the hong kong newspaper corpus , which contains 200 documents with professional generated extracts . this allowed for an evaluation of the overlap between the automatic summaries and the professional extracts . overall the results are promising with 50 % agreement with the manual extracts at 30 % compression . which is significantly better than the base case ; calculated by random sentence selection .
morphing is the process by which one object is made to ' become ' another through a series of calculated intermediates . these intermediates are an effective means to synthesise a genetic hybrid . the purpose of this project is to investigate the suitability of various morphing techniques to the creation of human - primate face hybrids , and the ease with which specifying the morph may be enhanced whilst ensuring a high quality result . this culminates in the production of a tool to be used by a client in psychological research . as the report progresses the tool is expanded to incorporate a semi - automatic approach to feature specification . the resultant tool allows rapid marking of features with the aid of a simple template , but is subject to performance issues relating to the chosen field morphing technique .
the growing use of mobile communication applications has created the need to extend the existing traditional text messaging with richer content . to meet the above necessity the following project has been undertaken . this report will take a look at the current mobile world and discuss about the evolution from sms ( short messaging service ) to mms ( multimedia messaging service ). it also confers about the mms architecture and how the 2 . 5g and 3g network technologies like gprs ( general packet radio service ) and umts ( universal mobile telecommunications system ) seamlessly will change the future of mms . in this dissertation , the open multimedia messaging ( mms ) standards , published by 3gpp and wap - forum organisations are well understood in order to create a valid mms message . also an mms composer interface is built , which allows users to compose a multimedia message i . e . combination of text , sound and image with or without need for extensive understanding of mms .
the project aims to introduce xml databases to ' fields ' that require it most but haven ' t got the skills and knowledge to explore this exiting fast growing world of xml . as most fields use traditional databases out of compulsion in spite of the associated difficulty and inflexibility . the report presents a system that generates an xml document ( s ) to model a database for semistructured data . the system is intended for field specialists like archaeologist who has no knowledge of computer science to generate xml documents from data provided from excavation fieldwork . these xml files can be store in native xml databases to allow searches and queries .
the goals of this project include the laying down of a software standard that can be easily adapted by others to create cross platform messengers . secondly it was intended that the software developed should have multimedia capabilities and finally we intended to explore the use of messengers in the university environment to improve teaching and learning practices . in this thesis i have developed a skeleton protocol that is built upon tcp and uses xml to implement the cross - platform messaging packets . to design the client and server i have used java 1 . 4 even at the risk of being a few nanoseconds slower because of two main reasons . firstly to be fully cross - platform the software had to be " write once , run anywhere " secondly the architecture that i have developed is aimed for providing a stable framework to future developers who want to rapidly develop a messaging application . since a fully functional messenger uses threading to implement its core components i wanted to use a language that would implement threading in the simplest and most hassle - free manner . java on this count is the acknowledged market leader . in the first few chapters i have gone about explaining the structure of my server and client . what sets my messenger apart from the rest of the systems available on the web is its use of xml . then i explain the structure of the packets that i have used and the standard i intend to create for extensibility with future messengers and adaptability with existing browsers . finally i devote a section to my contribution to the field .
this project is concerned with the different types of particle filters . there are two main types upon which many variations are based . they are the condensation algorithm and the kalman filter . there are many applications for this field of computer vision , surveillance , measurements for medical procedures , and implications in the leisure industry ( i . e . automated cameras ). unfortunately there are still many problems , occlusion , light , image quality , robustness , real - time speed to name a few . this project will be looking at the strengths and weakness of each one . the aim of this project is eventually to implement a particle filter which will be able to track an image through a series of frames or a video sequence .
intelligibility testing is currently widely used assessment for dysarthria speakers . a statistical algorithm for testing speech consistency has been developed which analyses results returned by human assessors . this dissertation will discuss various possible approaches to the design of computer - based speech intelligibility and consistency testing tool , the overall aim is to build a piece of software for the automation of speech intelligibility and consistency testing . this software should also be capable of recording acoustic data as its input and playing such data back to a group of judges ( listeners ) with normal hearing . additionally , it should automatically generate and output statistical reports to assist clinical diagnoses . the proposed application is not intended to replace the therapists , but to enhance their efficiency . experienced therapist would determine the configuration and actual testing parameters . this dissertation will also describe and analyze certain experiments conducted using those tools .
ventricular fibrillation ( vf ) is a deadly cardiac arrhythmia . it has been widely accepted that these kinds of deadly cardiac arrhythmias are due to re - entrant waves of electrical activity . this paper investigates into the mechanisms that initiate re - entry . the project develops a 2d simulation of a propagating action potential ( ap ) in a simulated virtual cardiac tissue and investigates ( a ) how large ( radius ) does a stimulus have to be to initiate a propagation . ( b ) how long does the stimulus have to be applied to initiate a propagating ap for a given radius ; as this area has not been vastly explored this paper reviews articles read up to date , namely the underlying physiology and ionic models . the requirements and implementation to create the simulation have been discussed . results and limitations of the project have been analyzed and a summary of the process involved , is outlined .
voting is a time consuming and expensive process , involving the initial casting of votes , the counting and the administration behind the scenes . e - voting can provide the solution ; by reducing the cost of administration and making the casting of a vote less time consuming by providing the capability to make your vote over the internet . however , in order for the system to be accepted voters must view the system as being secure , a concern which has recently been highlighted in a pilot scheme for e - voting . the project aims to implement an electronic voting system for the sheffield university student ' s union within which registered students who are eligible to vote can place their vote in a secure web based environment . the system aims to reduce the cost of running elections and also utilize security methods and techniques to ensure that votes are cast in secret . a summary is provided of the history of voting mechanisms and the impact that they can have on the result of an election is provided , before moving onto an analysis of the required electronic system . a discussion of the security and encryption techniques is then presented , including accounts of methods used in initial testing . finally , conclusions are presented accompanied with a plan for the implementation of the system .
speech technology is ever increasing in complexity and accuracy . the full potential of speech technology for use in games has been relatively unexplored , being reserved for novelty titles . an experimental adventure role play game has been developed to explore the feasibility of a speech interface in greater depth , and ascertain of what use it is to gamers , both those able to use a mouse and keyboard and those with disabilities . it is established that a speech interface would certainly enable disabled gamers to play mainstream titles , but for the majority of gamers a speech interface must be implemented with care and as a complementary device to the mouse and keyboard . there are areas in which a speech interface could add a great deal of immersion , but there are also tasks for which a speech interface detracts from the perspective of the game and consequently alters game play dramatically . further research is therefore suggested into the multimodality of a speech interface and game features which might benefit from it .
the aim of this project is to create a tool that allows me to experiment with the fight - off between individual and groups goals within an arena . i hope to be able to discuss how best to simulate the movement and work of individuals when sectioned into groups . in particular i experiment with different methods of individuals selecting group goals , and different restrictions that might be imposed on individuals with regards to movement between groups . i propose four experiments and apply them to a basic system simulating group behaviour . the results show how these experiments affect the system in both positive and negative ways . i then describe ways these could be combined to both improve and specialise the system , and finally initiate various discussions that have risen from this project .
this project is rooted in the computer games industry and the concept of increasing game longevity by way of dynamic generation of content . the problem of creating believable , topographical plans of various building types is addressed by way of creating an algorithm and supplementary data sources . the algorithm takes into account dilemmas such as : the allocation , size and type of room based on the generic type of building and available space ; the distribution of other features such as stairwells , doors , windows , lifts and corridors . the algorithm does not take into account the diversity and uniqueness of modern architecture or any extraneous features related to buildings such as plumbing or outside features such as balconies or wiring . receipts
testing is used to find errors and faults within a software system . one of the problems is knowing if the system has been fully tested using the test plan provided . therefore a complete test set must be produced to make sure the test plan is complete . this to has its own problem of the test set produced been huge and including redundant information . but through some basic assumptions and clever multiplications these problems can be solved and still resulting in a fully testing the system . this projects aim is to produce a system that generates a test set for harel statecharts with the test method refinements , removing of prefixes and cutting test sequences . this uses chow ' s w method as the basis for the test method and then extends it to allow it to be used with statecharts .
artificial intelligence in games has in the past been sidelined by the importance of graphics and optical realism . today however many people who play games no longer feel that graphics are the most important area , and that the intelligence of the none player characters ( npc ' s ) is just as vital , if not more important . in recent years the ai of games has improved substantially , started in part to the good level of ai in valves ' half - life ' game . although computer characters can shoot very accurately and dodge well , there are still areas that human players are much better at , such as knowledge of the environment , efficient use of teamwork , the ability to " hunt ", and survival instincts . this dissertation was aimed at investigating some of the modern ai techniques and approaches , to see whether these could be used in games to improve npc behaviour realism . the main approach was to use reactive behaviours and multi - agent technology to enable the npc ' s to communicate and coordinate their actions . as part of this dissertation an experimental program was written to demonstrate some of the multi agent techniques and other ai methods that could be used in games to improve their realism . an evaluation of this program and the methods it employs , as well as its successes , failures and limitations are included in this dissertation .
this research is intended to identify what kind of behavioural patterns given to the computer controlled ' townsfolk ' in role playing games are likely to give the player the impression that the agents in question behave in a human - like manner , and so giving the sense of a living game world . this is done through the creation of series of videogame worlds each containing these autonomous agents behaving in different manners , and analysis of the response from a sample audience to the events on screen . this research first identifies any progress made on this research by others thus far , before moving on to the design and implementation of all of the editing software required to create these game worlds , their inhabitants , the behavioural routines . once the game worlds have been created , the results and feedback from the test audience will be analysed and used to create a second set of game worlds , improved on the last set through the knowledge gained from the feedback . the new game worlds will then be analysed in the same way ; outside of the time constraints of this project this process would continue , constantly refining the ai behaviours . this ongoing analysis will ultimately explore the possibility of defining what kind of behaviour is considered lifelike , why , and how this knowledge could be used to increase our ability to model behaviour in a video game world .
abstract speech corpora are large collections of speech audio data , which are generally used for research and normally only certain sections of this data , are used . it would be advantageous to users if all the data from different corpora were stored together in conjunction with a method to abstract specific sections quickly and easily . this project ' s aim is to design a web based query tool for accessing data from a variety of speech corpora . it incorporates regular expressions to extract information from a number of different transcription levels . it also supports corpora , which have multi - simultaneous speaker channels ( e . g . shatr ), which may be typically queried for overlapping word or phoneme pairs . a simple java applet has been created that can be used to construct simple or complex queries and display the results . the applet has been designed to be easily portable between corpora , and has been successfully tested on shatr .
languages impose strong constraints on the ways in which words can form sentences . the process of analysing a sentence into its parts is known as parsing . this is a computationally expensive process . it has been argued that for most natural language processing tasks , full parsing is not required , and intermediate approaches have been developed that allow sense to be made of the structure of a sentence without fully understanding it . one such approach is that of phrase chunking . in recent years there have been several machine learning approaches to phrase chunking , which have used a range of different features of text as features , such as words and their part of speech tags , of instances which are used for training . this project looks at some of the sets of features that have been used in previous approaches to chunking , and attempts to find the optimal set of features for a memory - based approach . it was found that if words have a process called attenuation applied to them , which generalises low - frequency words , the performance scores will be better than if only part of speech tags , or a combination of part of speech tags and words are used . this performance score is comparable to most existing single - classifier approaches to chunking , but ultimately , an approach that uses a combination of different systems is the only way in which performance could be significantly improved .
this project is based on the work of a native french student at the university of sheffield , christine fiandino , involved in a phd entitled « setting up a cd - rom to learn french pronunciation ». as she had the occasion to observe through a survey she led on a set of french teachers in several universities , « intonation often does not receive the attention it deserves » [ 1 ]. the principal aim of this project is to use computer - aided learning to provide students with more interactive exercises , taking advantage of the computer ' s ability to deal with multimedia , and especially with sound inputs and outputs . the original idea was to use computational and mathematical resources on speech recognition to allow students to record sentences . the goal of the project would then be to analyse these recordings , providing students with feedback , in order for them to improve their intonation . thus , this project contains two different types of exercises : first , text - oriented exercises , presented through a web site , teaching the students important rules in french pronunciation and secondly , sound - oriented exercises , based on a pitch tracker , allowing the students to record their voices and to match their samples with pre - recorded ones .
the project i was assigned on requires me to understand the use of javaspaces in a travel example . i was instructed to use the javaspaces called persistent object manager ( pom ) implemented by another fellow student in the department of computer science , university of sheffield on the travel example . i did not experiment on the original javaspaces but did make some comparison between pom and the original javaspaces . the scope was limit down to experiment on the javaspaces as compared at the beginning of the semester due to lack of time . the experiment on using pom sees the capability of using javaspaces as object - oriented database management systems in the future . the advantages and disadvantages of pom are clearly seen in the design . using uml in the early stage of the implementation helps to picture the whole flight booking in the travel system with more logic and precise implementation . i have tried to implement the system fully but the result is very discouraging . i have experiment the pom with a certain small testing program . however , the poor skill in programming in the system becomes an obstacle in making the whole system running smoothly . anyhow , i have experiment much , understand , and research well on javaspaces besides providing good implementation on the use cases . i will discuss all this in the theory and implementation in of the design on the system in the report .
this report begins by examining and analysing four different multi - document summariser systems . three of these systems produce biographical summaries while the other system produces a more factual summary . the aim of this project is to study and analyse the various techniques used and the approaches taken in these systems , with the goal of producing a system that generates a single coherent , accurate and non - redundant biographical summary of a person requested by a user . the project will try to solve and develop problems found in the researched systems . this report describes in detail the various components of the proposed system and methods studied and developed in order to produce a multi - document biographical summarizer . the results of this system are demonstrated and a thorough evaluation of the system and project is given .
technology is advancing at a great rate , with this progression is the use of distributed computing ; increasing demands are made for software tools in which can compliment these new advances and requirements . such tools and frameworks are required to overcome the complexities of distributed computing , such as partial failure , latency , synchronization all popular problems of distributed computing in which the use of javaspaces are aimed to triumph . the m4 project is concerned with the construction of a demonstration tool in which can provide browsing and querying of a large archive of analyzed meetings . this archive of data is collected from various past meetings in a room equipped with multimodal sensors . the aim of this project is to construct and analyse a distributed processing software tool for the m4 archived multimodal data based around javaspaces . by using javaspaces the project will develop a system to handle queries made upon the m4 archived data , as well as identify any limitations with javaspaces and methods taken to try to overcome such problems .
this paper describes the successful use of a probabilistic technique for attributing authorship to texts . given a disputed text and two possible authors , each author ' s style can be characterised by picking their favourite words from a training set . those words that appear frequently in an author ' s training set , but infrequently in the other ' s , are picked as that author ' s " distinguisher " words . the number of occurrences of these words in the disputed text is counted , and then a bayesian formula is used to calculate who wrote the test text . sets of tests of the method were conducted . experiments using bigrams ( two - word distinguishers ) were done , producing impressive results : 100 % correct attribution of authorship to texts of 200 words and over . the method was also used to attribute authorship to the scenes of the play ' henry viii '. the results deemed the play mostly written by shakespeare , in contradiction to the opinion of scholars . related topics : authorship attribution , text classification , bayes ' theorem , stylometry
the aim of this project is to develop of a computer assisted learning ( cal ) tool which will become part of the matlab auditory demonstrations ( mad ) project . the tool will demonstrate independent component analysis ( ica ) and blind source separation ( bss ) which currently does not have a suitable tool for tutorial teaching . the paper discusses the development a cal tool to fulfil this aim . the tool uses both the audio playback of sound and the plot of the joint probability density function of the mixtures through various stages of the ica process . this presents the ica process to the user both through audio playback and visually . the background information to this project including ica methods and cal tools is covered as well as the development process .
in recent years , neural networks have shown to be capable of providing strong solutions for the diagnosis of medical conditions . however , despite their success they have never been widely accepted by the medical community . this is largely due to the lack of transparency in the method they use to reach a diagnosis for a given medical condition . a critical factor in medical diagnosis is the necessity to be able to explain how a diagnosis was reached . c4 . 5 rule - pane is a rule based machine learning technique that employs a neural network as a pre - process in the generation of a rule set . this technique is believed to provide the strong generalisation capability inherent in neural networks , along with the obvious comprehensibility of a rule set . this project investigates the application of the algorithm to a number of well known datasets and discusses its validity for use in the field of medical diagnosis .
the department of mechanical engineering wishes to be able to assess the progress made by students throughout the semester . to avoid the extra work associated with mid - term exams for both lecturers and students a computer aided learning system has been devised that will allow common misconceptions and errors to be identified . these can then be corrected by the lecturer before it is too late . previous work carried out in this field has been evaluated , the advantages of these systems have been incorporated and the disadvantages avoided . a critical review of appropriate technologies for such a system is made . the requirements are then analysed , and a system designed and implemented producing a completely java based solution . the paper culminates with an evaluation of the developed system and the project as a whole .
recent developments in technology have proliferated the use of multimedia on the internet . we already have adequate search tools for use with text based documents , but we are now beginning to require search tools for use with audio and video data . in this project we will investigate the performance of a new information retrieval approach against an existing one . we will use text based data which has been recognised by an automatic speech recogniser . this data will therefore contain errors , which may affect the performance of our search methods .
the project sets out to create a real - time physics simulator for the game of jenga ™ in 3d . it is observed that the game of jenga ™ is physically that of creating a stable stack of blocks , a situation with a high number of contacts indirectly connected to each other at any one time . analysis of the popular rigid - body method for physics based simulation shows it not to be an ideal solution for such a large number of contacts and an alternative , spring - based method is devised . the spring - based method developed has the advantage of guaranteed resolution of all collisions after a couple of iterations , an essential feature when dealing with a stack of blocks such as in jenga ™. on the negative side , the blocks can now be deformed , which can only be reduced by taking small time steps . this proves to be quite possible for the game of jenga ™ due to its largely low velocity nature . the results of the spring - based method are quite encouraging . though the simulator produced cannot handle the full jenga ™ tower , testing scenarios against a portion of the real - tower produced very convincing results . deformation of blocks is not noticeable , and physical behaviour is very believable . the simulator achieves real - time speeds for about a third of a full jenga ™ tower .
this project ' s aim was to build a graphical user interface for programming the hunter robots from magna ' s robot arena . previously the robots could only be passed basic parameters which influenced their existing program . these parameters generally influence aspects of behaviour such as distance before the hunter will " pounce " on a prey , speed at which they will follow a prey and distance at which the hunter will " pounce ". this was obviously not suitable for programming complex behaviours . the gui discussed in this document allows non - programmers to create complex behaviours using a simple interface system . this document discusses similar projects as well as reviewing the tools and methodologies which were invoked during the gui ' s development . it also describes the development and evaluation process , as well as looking at the finished product itself . this project has produced a gui which allows users to program the hunter robots with complex behaviours through a simple , easy to use interface , although the programs generated are based on finite state machines and so not able to reach the level of complex behaviour of neural net programs a high level of complexity is still achievable without the user having to know c ++ or anything about the compilation and download process . this project ' s aim was to build a graphical user interface for programming the hunter robots from magna ' s robot arena . previously the robots could only be passed basic parameters which influenced their existing program . these parameters generally influence aspects of behaviour such as distance before the hunter will " pounce " on a prey , speed at which they will follow a prey and distance at which the hunter will " pounce ". this was obviously not suitable for programming complex behaviours . the gui discussed in this document allows non - programmers to create complex behaviours using a simple interface system . this document discusses similar projects as well as reviewing the tools and methodologies which were invoked during the gui ' s development . it also describes the development and evaluation process , as well as looking at the finished product itself . this project has produced a gui which allows users to program the hunter robots with complex behaviours through a simple , easy to use interface , although the programs generated are based on finite state machines and so not able to reach the level of complex behaviour of neural net programs a high level of complexity is still achievable without the user having to know c ++ or anything about the compilation and download process .
when the technologies became available , huge amounts of data were stored as printed text in manuscripts , archives and other forms . the information within was , and still is , of immense use to social historians , academics , professionals , a whole range of people . often though , the only way to find the information desired is for a long and arduous manual procedure of searching through the texts . what would make the searching of data more efficient and more effective would be some automated way of extracting the desired information from the texts . one such example of data stored in printed text is venn ' s alumni cantabrigienses , a biographical dictionary of former members of university of cambridge . the information within is of great use to historians of the era and genealogists alike . however , the volume of data held in the alumni is too huge to even consider a manual extraction producing effective results . the aim of this project , therefore , is to create an automated extraction system that will take the texts of the alumni as input and produce as output a heavily structured set of data that can have powerful searching algorithms applied to it .
the use of pattern recognition is widespread , through the use of statistical approaches such as pca and fa computers are able to analyse linear patterns in data . but if we wish to analyse more non - linear data these linear algorithms do not really su ± ce , the generative topographic mapping algorithm is described as a non - linear interpretation of ppca and fa and as such is far better suited to non - linear data . the gtm involves finding the mapping of data from a high dimensional data space to a low dimensional latent space , through the use of the expectation maximization algorithm the gtm is able to reach convergence . this project studies the gtm algorithm and aims to take a slightly di ® erent approach to the original authors ( bishop et al ) of the gtm . the main theory behind the gtm is looked at and the approach of deriving a linear gtm and then a gaussian process latent variable model gtm are discussed within this report . further proof of the linear and gaussian process latent variable model algorithm ' s theory is achieved by the implementation and testing of the algorithms .
this project first introduces time series and then goes on to explore time series analysis . explaining what time series can be used for and how my project fits into time series analysing . it will then go on to explain basic statistical concepts steadily progressing to complex statistical machine learning algorithms . after all is explained the algorithms will be programmed in matlab ( version 6 ), and the time series will be modelled in the software . these programs will be presented for evaluation . a conclusion and evaluation will then be given towards the end of the project which summarises the projects findings so far , along with the plan of action for the second semester .
the language barrier has become a serious limitation of people ' s ability to search and understand documents published on the world wide web . cross - language information retrieval systems attempt to address this problem by allowing users to specify a search query in one language and return results in a second . however sensible query translation is a problem due to names of organizations , places , people etc not appearing in bilingual dictionaries . a lexicon containing proper name translations could be used in conjunction with a bilingual dictionary to translate query terms . the aim of this project is to build a system capable of harvesting pairs of translated documents from the web , analyze them and extract pairs of proper names to compile a lexicon .
in this project i seek to investigate the acceptance of artificial beings and the architecture of such a being . social and ethical reasons for rejection of artificial life are discussed and considered , as is relevant literature . the architecture is demonstrated by an implemented system to play the computer game nethack , with evaluation of the implementation . the implementation demonstrates that the layered - goal architecture performs well in complex and uncertain environments and so would be suitable for use in a home or office system . consideration is also given to the ability of the implementation to appear human and this is evaluated using a modified form of the turing test . the results of this indicate that it is possible for the relatively simple layered - goal architecture to produce human like results .
a description of the implementation of boolean information retrieval engine in an object oriented programming language , ( java ) is given with detail of the problems encountered and solutions . the resulting system which compares different size passages with a view to ascertaining the benefits for a question and answer system is explained and the results retrieved from the system are presented and interpreted .
as the use of digital video increases so does the need to record annotation data that describes the content . once this content is stored it can be used in a wide range of applications , ranging from human gesture research to indexed databases of video . it is therefore necessary to have tools that facilitate the creation of such annotation data , and it is the aim of this project to create such a tool . research into the field of linguistic and video annotation is carried out , and includes reviews of the tools transcriber , the mate workbench and anvil . the atlas project for creating an abstract annotation toolkit for creating , storing and retrieving annotations is investigated , and idea of annotation graphs is also researched and reviewed . the interactive annotation tool is then specified and designed , keeping in mind the work published by jef raskin about the " humane interface ". the details of implementation and testing are given , and finally the main results are discussed . it is concluded that the system created is a useful general - purpose video annotation tool , but there are a number of improvements that could be made and there is a great deal of work that could still be done .
technology is a constant area of development and research . it is also becoming common place in the workplace and at home alike . with the advent of the internet in 1993 , a whole world of resources has been placed at our fingertips . its popularity growing daily , with 533 million users [ computer industry almanac ] to date and a predicted 945 million [ computer industry almanac ] in 2004 , so too have the demands of the users . this places a requirement on the developers to create fully functional , interactive web sites at a rapid pace . this projects aim is to develop a web based collaborative work environment for multi user developing of web sites . the system will incorporate a previous project that enables a virtual postit ® note to be attached to any object on a web page . the postit ® note capability will be handled automatically .
assessing the performance of a large number of students requires a great amount of time and effort . this project involved the design and implementation of a web database application to manage practical mark data for the department of computer science ( dcs ) in the in the university of sheffield . linked to a project gjb - 5 : a web database for student practical marks , the projects delivered optima , a system that automates aspects of the existing procedures , and adds value with the ability for lecturers to view practical mark data analysis . there are many different challanges in developing a web database application and this project had specific aims and objectives concerning functionality , design , and practicality . useful insights into developing this type of application were gained ; proposals are made for future investigation in this field , together wiht further development in optima .
rewriting : a technique for defining complex objects by successively replacing parts of a simple initial object using a set of rewriting rules or productions . this technique lays the foundations upon l - systems are build and operate . l - systems provide us with a way of simulating the growth of plants through rewriting and also allowing illustrating the models in 3 - d . this project describe how a stochastic l - system simulator can be implemented along an existing system that will simulate the behavior of a community of plants competing for space in a 3d environment .
quad - rotor unmanned aerial vehicles ( uavs ) are being developed at the university of sheffield and around the world . they offer great manoeuvrability , but require constant stabilisation to remain in flight . microcontrollers on - board these small uavs must keep them stable by sampling sensors and setting the speeds of the motors . in order to test control software before implementing it on a real flyer , the aim of this project was to develop a generic simulator . as it will model the physics and electronic architecture of quad - rotor uavs , dynamic models and relevant types of electronic devices are reviewed . the simulator has been developed and is applicable to a wide range of uavs . a method of programming microcontrollers has been devised that means code can be transferred directly to the real microcontrollers after testing in the simulator . however , this is as of yet untested . the simulator is designed to be highly versatile and extended by its users .
robot localization is an area of importance in robotics and simultaneous localisation and mapping ( slam ) and is a field which has been well researched over the years , and many techniques have been employed to try to combat it . the aim of this project is to research and produce a working map building system using the hunter robots that we have been supplied with a few minor adjustments as necessary . the hunter robots have infrared range finders and remote control receivers along with a bump sensor ( see appendix 9 . 1 ) and include a pic16f877 microcontroller which can be programmed using c . the three stages of localization , map creation and map displaying are researched and analysed and requirements set out along with the techniques used in implementing the final working system .
the p - calculus is an important part of the theory of computer science . it provides a way to express clearly and in detail the behaviour of mobile systems and to treat them rigorously . however , the language in which it is expressed is complex and at times can be difficult to grasp fully . this can sometimes present problems for those trying to gain an understanding of the underlying principles . as such the development of a computer program that can help to illustrate and explain the processes would be useful . it is intended that this report will consider the p - calculus and one example in particular that illustrates part of it . following this is an in depth examination of all the relevant issues and material that are to be involved in the creation of a program that will simulate this example .
the field of artificial intelligence is a broad one that is incorporated in many useful applications that we use in modern day life , knowingly or not . the complexity of artificial intelligence techniques ranges over a huge scale . state space search techniques are often not easy to understand and it is perceived that graphical animation could help in teaching this subject . the aim of this project is to produce software programs to do exactly that . this project is successful in producing two interactive java applets , the first addresses the map traversal problem , and the second explores the game of nim .
the use of computers in the teaching environment has increased rapidly in recent years . a growing area in computer - aided learning is courseware – interactive software used by the student to aid the learning process . the aims of this project were to write two pieces of courseware that animate two artificial intelligence techniques for a module taught at sheffield university . the software was to be reviewed by analysing feedback from students that use the software and its strengths and weaknesses as courseware are analysed . this report discusses the design processes behind the final software produced , as well as the usefulness of the visualisation techniques applied and shortcomings in user interface design that limit the value of the end products as courseware .
self - organisation in insect colonies is an incredible feat of nature . the distributed intelligence of a colony of insects is completely opposite to the way in which humans organise themselves . for this reason , in computing this form of problem solving had not been thought of until recently . now the field of artificial life is helping , not only in the study of biology , but also in communications networks and in finding solutions to complex problems . how does each insect know what it is supposed to do as part of a colony ? and how do small groups of insects work together as a team ? in this project , we will be looking at these questions and others , so as to better understand how co - ordinated behaviour works . the aim is to produce an artificial life program to model group retrieval of prey by ants .
this study involves techniques applicable to a wide range of uses concerning face biometrics . one of these uses is in the development of security systems . a successful and robust facial feature recognition system could gain some ground towards making remembering a pin number or unauthorised people gaining access to restricted areas a thing of the past . the project aims to develop facial feature recognition as a sub - problem of face recognition in images within the context of chinese face reading . it involves an algorithm that extracts the features required and stores them in memory space , using a medium - level technique that converts the image into image data . this data is then compared to stored standard features and a resultant chinese face reading delivered to the user based on the analysis of a particular feature , namely the eye .
ever since the birth of science fiction the idea of intelligent , life like robots has captured our imaginations . it is in the area of robotic pets that we have come closest to this goal . unlike in industrial applications , robotic pets are not soulless automatons , but designed to mimic life - like attributes such as emotion , personality and personal goals . it is the aim of this project to research and explore the ways in which artificial pets create the impression of life and then attempt to implement this theory by constructing an embodied robotic agent that will function in the role of a persistent household pet and novelty item . these aims were partially achieved by the creation of a robot utilising reactive intelligence techniques capable of navigating in a real world environment and displaying different behaviours based on its emotional state modified by sensor input .
question answering has extensively been researched , with the focus being on information contained within a fixed size corpus , although the web has been considered . question answering involves querying an information retrieval system where the query is expressed in natural language . the aim of this project is to develop a tool that will effectively answer questions that may be submitted to a web search engine . the answer will take the form of the most relevant document and the retrieval system will be the popular web search engine google . in this project , techniques that potentially improve text retrieval have been used for question answering based on the web . such techniques consist of query expansion and local context analysis . the procedure begins with a training process then progresses to apply the above techniques . achievements to date have been in the form of developing the learning process that is , obtaining a large database of faqs and thereby forming the training set and reformulating the query . the reformulated query is then submitted to google via the google api , thereby the results can be analysed for the query expansion and re - ranking of documents .
plagiarism is a problem that has increased in difficulty to prevent with the widespread use of the internet , and its ability to allow students to plagiarise from sources that are no longer easily identifiable by teachers or lecturers . this project aims to test the information retrieval technique known as the vector - space model as a way of detecting derivation from source documents , using data from the meter project , following the assumption that this would be useful in detecting real life plagiarism . the project found that the vector - space model did have plagiarism detection abilities , particularly when the dice coefficient was used to measure the differences between the source document vector and the derived document vector . page 3 of 48
the task of developing an intelligent robot has been at the forefront of artificial intelligence for many years . this report presents a solution of how to develop a robot capable of autonomously navigating in a simple maze environment . the behaviour exhibited by a robot successfully navigating in this environment is intended to be a shown as intelligent . the report outlines the way the distributed adaptive control architecture can be implemented to achieve this task by using a genetic algorithm . it also explains many important factors that should be considered when designing and implementing this type of project , such as the correct choice of sensors , and whether the solution be implemented in simulation or using real robots .
part - of - speech tagging is an important area of research in natural language processing . without a good tagging system , complex analysis of documents by computers cannot be achieved , as they will be unable to grasp any kind of meaning from the document . by understanding what each word in a sentence means , we can understand what the document is about , and more importantly , exactly what we can do with the document . a system that can understand natural language will prove to be a great step forward in human computer interaction . a user will query a system in a way that they understand , and receive a response from the computer in the same manner . this research will be the foundation for much more complex systems that will be used in day - to - day life . this project is a research analysis of the topic of part - of - speech tagging , and i shall be implementing a particular type of tagger .
software testing grows continuously more important in modern software engineering to ensure a high quality of software is maintained . as such methods to improve software testing are continually evolving . one of the most important software testing methods is the category partition method . this project aims to deliver a functioning toolkit to make the category partition method quicker and easier to use . the toolkit will use xml documents to define the test specifications and deliver the test frames , and will use java to provide a user friendly interface and to handle the transformations .
robotics is a hot research field . with the advances in computer hardware careering ever forwards according to moores law , advanced algorithms can now reliably locate a robot in an uncertain environment , even under the sole guidance of inaccurate sensors . while most robotic research is undertaken on the ubiquitous nomad 200 platform , the discovery of a series of java - on - a - board devices and " plug and play " sensors and actuators motivated a project of hardware development that resulted in a custom - built robot platform for the department . this new platform , using the latest technology , will serve as a launch board for a robot research programme based on real - world robots , instead of the common but less useful simulations . this project attempts to gauge the capabilities of the robots by developing a localization and mapping algorithm . unfortunately , due to a series of hardware problems , the algorithm is not tested on the real robot , and a simulator is used to evaluate its accuracy .
genetic programming is a subsection of genetic algorithms , which take inspiration from the theory of evolution to produce computer program solutions to problems . in this paper , a virtual simulation environment is constructed to study the predator - prey phenomenon in a coevolutionary environment , where both predator and prey are controlled by an evolving genetic program and they provide a changing environment for each other to cope with .
there are many different languages that may be spoken , written , read , or even all of these . many of these languages that can be written are stored on computers . in order for the maximum amount of these languages to be available to the maximum amount of people , there is a need for an automated language identifier , to identify which language a piece of text is written in . my project is to design , program , test , and completely develop a piece of software that can handle the need for automated language identification . this means that the software should have no need for any human input or interaction once the programming has been completed , other than to input the text that the user wishes to be identified . the programming could be undertaken in either of the programming languages suggested such as java , c ++, or perl , or in fact any language that has text processing capabilities . the identifier , once it is completed , should be able to distinguish between as many languages as possible ; however , instead of looking to produce an immaculate piece of software , it is possible that the project could take on a more research - based role , and help to provide some insight into the best approach for language identification to take , therefore improving the techniques used in the areas of language identification .
as digital video becomes ubiquitous in both the home and workplace , the development and evaluation of tools to manipulate such video is clearly important . tools to ¯ nd camera cuts in digital video allow higher level tools to build on this functionality , although cuts can be di ± cult to ¯ nd and a lot of object motion can sometimes be mistaken for a cut with some methods . evaluation of the histogram method showed that using eight bins per colour component , and assuming conditional independence between components , gave good results . these results were found when using a gaussian to model the distribution of the boundary and non - boundary data , with worse results being generated when a gaussian mixture model was used instead . when hidden markov models were applied to the data , some con ¯ gurations were found to do as well or slightly better than the gaussian model , although the performance on certain video types was far from perfect .
optium v . ( pl . optima ) the best possible compromise between opposing tendencies [ latin , literally ' best thing '] this paper describes in detail the development and bulid of optima - an online practical marks system for the department of computer science . the key features of the final bulid include : a single store of practical mark information , access to analysis of marks by lecturers and the simplification of tasks for the departmental administrator . furthermore , for the first time students will also be offered a visual representation of their performance in relation to their peers , in the form of bar graphs and scatter plots .
the processes of natural selection and evolution can be applied to a wide variety of problems that exist within many different domains . the branch of computing that deals with the approximation of these systems is known as evolutionary computing . in this project , an attempt will be made to model an environment in which individual artificial life - forms , or agents , are able to interact with one another and evolve over a number of generations . the focus of this project will be getting these agents to cooperate with one another in such a fashion as to achieve a reasonable goal . from a starting population of random agents , this project will aim to cause a number of clearly defined co - dependent sub - types to emerge from the system over a period of time that are capable of meeting the requirements of the goals that are set .
this aims of this project is to produce a summariser that summarises biographical information from web pages . the system then outputs a summary which contains links to the web pages of origin . for testing purposes , the system was modified slightly to accommodate the capabilities of human testers . the results of the automatically generated summary are then compared to human generated summaries . evaluation was then carried out using the recall - precision method . this was done on individual web documents instead of the system as a whole to see how efficiently the system was running . the definitions of recall and precision were altered slightly to suit the purpose .
multi - agent systems possess many unique features that separate them from other forms of software : they are able to adapt to changes in environment , they allow distributed computation , they are scalable and above all robust . this makes them an ideal system to use in rapidly changing or unpredictable environments . due to the autonomy exhibited by many intelligent agents , once they are running , there may be very little possibility for control . therefore , extensive testing is required in advance to ensure that agents will behave correctly . this project tries to overcome some of the problems associated with designing multi - agent systems by providing a simple framework to allow models of agents to be quickly constructed and tested in a simulated environment . a set of tools will be implemented that allow the rapid construction and verification of the models of agents that make up multi - agent systems . special attention will be paid to simulating biological systems as they often exhibit behaviour analogous to that of a well - designed multi - agent system .
the aim of this project is to design and implement a system which extracts new words and names from the existing sources of frequently updated material freely available from the internet . a system will be produced which will extract the text from newspaper articles in order to allow an end - user to gather information about the usage of proper names and new words to compare their usage between sources and through time .
adaptive algorithms are widely used in the field of robotics . these algorithms have been used to improve robotics in many fields like localisation , mapping , face recognition , control systems , etc . this project will concentrate on some adaptive algorithms , especially the mcl in the localisation of robotics . there are some experiments to show the difference in localisation performance caused by implementing the lms algorithm with different levels of sampling .
this paper ’ s goal is testing and evaluating the existing forefront techniques of named entity ( ne ) identification . by looking at specific automatic ne annotation software and examining its performance against manual annotated data , it is expected to present the current automatic annotation process ’ s performance and behavior .
cardiac cells are known as myocytes . this text ’ s aim is to model myocyte contraction within a tool usable for biological research . the model is designed to assemble branching myofibrils spanning a cylindrical or elliptical myocyte . these myofibrils ’ sarcomeres generate tension subject to experiencing delays in the intracellular calcium ion concentration waveform , variations in the myofilament tension and calcium ion concentration relationship , myofilament sensitivity to temperature and the sarcomere lengthtension relationship . occurrence of myofibrils and their branching within a myocyte is modelled to determine affects on myocyte shortening and sarcolemma shape during contraction . stiffness within the model can be stipulated by varying the behaviour of the elastic titin filaments . research on how tension distributes across z lines according to their material properties is possible using tension distributions . the same technique is employed to model the sarcolemma ’ s material properties and research effects of sarcolemma rigidity or flexibility during myocyte contractions and relaxations .
web services facilitate automated machine - to - machine access to the resources of the world wide web , and , finally , fulfill the dream of distributed component - based services , as proposed by earlier technologies such as dcom and corba . however , many existing tools for developing web services , whether in the form of low - level toolkits or integrated solutions in larger products , differ in their usability and the features they provide . this project concerns the design of a web services platform , which will allow the creation , consumption and testing of web services to take place in an easy - to - use and language - agnostic way . in addition , several test case web services have been designed to provide through functional testing of any implementation of the framework outlined here , using live data . this includes an industrial - scale web service component for accessing the meddra database system , the development of which provided the primary motivation for this research .
current video caption analysis techniques are intended for annotating clips to permit content based retrieval . these are very slow and do not take into account the origin of the video . this paper will look at a tv assistant application that analyses live video data in real time . the analysis is based on the presence of onscreen captions . these are decoded and fed to an information retrieval module . meta - data concerning the video is then obtained from the world wide web . details of the gathered documents are presented to the user and he or she can choose to view the complete articles . the unique implementation is described ; the promising test results are presented and evaluated .
visualisation of the molecule in its atomic form was one of science ' s greatest achievements . from then on the determination of molecular structure has become common practice in determining chemical compounds and reactive nature . this project aims to allow the average user , who has little scientific knowledge and internet access to molecular data , to be able to recreate fully customizable 3d models of these molecular structures inside an intuitive , user - friendly interface on the average personal computer or workstation . models can be transformed endlessly , allowing for data to be viewed from any point in 3d space , and will use a number of well - known display modes and colouring schemes . the project will then be further extended to incorporate the use of a virtual reality system , allowing these molecule structures to be viewed within a fully 3d environment and with manipulation via virtual world controllers . the virtual molecule viewer aims to push the boundaries of scientific visualization and , using the enhanced perception offered by virtual reality , introduce new ways for exploring molecular structure via the natural interactions obtainable from an integrated virtual interface . keywords : molecules , visualisation , molecular structure , virtual reality
most computer games require that the image on screen is updated tens of times per second . the use of accuracy and complex modelling are kept to a minimum because required complex calculations lower this speed of this updating . o bject demolition suffers this consequence and is currently modelled very basically in real - time . the project investigates the real - time generation and application of a cracking pattern approximating the theory p rovided by fracture mechanics . the subject of polygon clipping is explored as well as its application within const ructive solid geometry using binary space partitioning trees as a volumetric representation of space . constructive solid geometry is applied to a polygon model to break up its structure . the visual results of the simulation are extremely encouraging though suffer inconsistencies when a pattern fails to encompass the entire object . performance in the system is faster than previous methods have accomplished , thoug h is not consistently fast enough to achieve real - time classification .
many phenomena in the real world depend on too many unknown parameters to precisely predict . however , simplified models of these phenomena can be created , using observed data and some useful statistical tools , such that predictions can be made to some degree of accuracy . the aim of this project is to create a model that represents the results of a football match , based on the results of already played matches . the likelihood of this model will be compared to the likelihood of bookmaker ' s predictions , with the ultimate aim being to make more accurate predictions than the bookmaker . four differing models are presented and discussed , using the techniques of maximum likelihood , bayesian inference , and lastly , variational inference .
this project looks to extend the work of an interactive cd - rom created by the humanities research institute at the university of sheffield . the cd - rom was designed for the use of academics and researchers interested in a corpus of letters written to flora tristan , a french socialist campaigner during her " tour de france " in the years of 1843 - 44 . this project looks to implement additional features and functionality to the existing system , to aid users in navigating the system more easily and achieving goals in a quicker manner . it is hoped that the work carried out in this project will bring the flora tristan system one step closer to becoming a publishable product . these improvements to the current system will primarily be in the form of additional functionality added to the letter documents , the option of choosing between an english and french interface , and an improved initial screen to allow first time and casual users to be able to use the system more easily . functional and user testing of the implemented requirements has shown that they do indeed provide both quicker and easier methods for achieving goals . it has also demonstrated that the majority of these implemented requirements work in the correct manner 100 % of the time , but has highlighted some problems in the linking within letters requirement that may not have been discovered otherwise . in conclusion the implemented requirements have benefited and aided users of the system and brought it one step closer to being publishable , but there still remains some further work in order to reach a stage where the flora tristan system is complete and at a publishable point .
over the past few decades there has been an explosion in the number on online technical documents , news stories and other formal documents . associated with this large increase in documents has come a large increase in the number of acronyms defined . as a result of this there is now a need to create systems capable of automatically identifying acronyms and their meanings in order to help improve user understanding of technical documents . the aim of this project is to design and implement a system capable of identifying acronyms in free text and inferring their meanings . the databases of found acronyms should be available for querying online .
this paper investigates several approaches to automatic document classification by author where the number of classes is greater than two . documents are classified using a maximum likelihood estimator and assuming a multinomial model with different parameters for the different populations of documents . this report compares the performance of an n - ary classifier with two different tournament methods using binary classification and analyses their strengths and weaknesses . it is shown that both tournament methods provide a viable alternative to the standard approach .
for many years software engineers have been designing tools that aid and automate the work of others . case tools provide automation for the software engineer himself by allowing the automation of the software design process . the discovery case tool allows users to produce electronic versions of the models that form the discovery method . the electronic versions of these models will allow users of the discovery method new freedoms to design and edit their models quickly and to produce highly presentable and consistent project designs . this system consists of an anticipated four combined projects . subsequently this report concentrates on the automation of task flow models and state machines .
with professional ( human ) abstractors , the abstracts may not always be consistent or objective , and the abstractor may not always produce the same abstract for the same document , if he tried to do two abstracts at different time periods . this is why automatic summarization can be an incredibly useful tool . the aim of the project was to produce an automatic summarizer that can condense documents into a sensible , coherent and useful abstract . the summarizer will produce a single - document , indicative summary of a fixed length , and it will consist entirely of sentences extracted from the original document . after implementation and testing , it was shown that the program produced is better than a random selection of sentences , however it still has flaws , although it is felt that a lot of this is due to the ambiguity of human abstracts that were set as a benchmark . extra features and possible expansion of this work is outlined at the end .
the calendar is a human invention , developed by the necessity to order time . ever since civilisation began on this planet the human race has kept track of the passing of time . be this by means of using astronomical data , other natural cycles , or by simply counting days , it has always been done . as many cultures and religions have evolved throughout time , so have calendars . this report investigates the history and origins of the calendar systems used today and those of important systems that have gone out of use . computer representation of dates has been fraught with software and hardware errors . many computer systems cannot cope with dates before ad 1900 and so the ability to enter historical dates into calendars is extremely limited . the aim of the project is to create a software tool that will convert dates between these differing calendar structures and allow the use of historic dates .
the " online cataloguing " project that this project is a sub project of is based on becoming a digital library . the digital library is a powerful , method in which to store all of this information within the " online cataloging " project . not only must all this data be stored and viewed as an online library but also be able to have to facility to search this vast amount of data . as such , there are many digital libraries that host lots of information and also have the facility to be able to search . the facilities all have their own unique way of searching methods some of which are very effective . the project aim was to provide an efficient search engine that dynamically searched the xml and then transform and display this information for all the users fascinated with the collection . various technologies were investigated to see what was the most efficient was to achieve the project aim . a web - based search engine was created using the most efficient technologies that allowed for the text searching .
the outcome of the project is a 3d data integration , prototyping , and display environment (" arc - vista "), to help reconstruct and portray the architecture and context of an archaeologists findings . the environment makes special use of virtual reality hardware , but can also be used on any conventional personal computer that has a java virtual machine ( jvm ).
time - series is a collection of observations made sequentially in time . the scope in using time - series occurred in variety of fields . this scope may cover from economics to engineering . as time - series is so essential , it is worth to analyze the methods for time - series prediction . previous approaches to model time series such as the box - jenkins , holt - winters and finite impulse response network were described to see how time series were modelled . the field on neural networks has grown as more people are aware of the capability of neural networks in dealing with complex pattern recognition problems . in particular , the theory of neural networks and several fundamental methods in neural networks such as single layer networks , multilayer networks and radial basis function network were described in this dissertation . apart from that , several issues that are relevant to the implementation of the neural network methods such as initial conditions , learning rate , momentum , local minima , over - fitting and cross - validation were also described to have a better understanding of the methods . finally , the results of implementing the neural network methods in modeling time series are evaluated and discussed .
some people argue that the construction industry cannot evolve much further than the point at which it finds itself today . however , whatever these people think , the reality is that the construction industry , like any other industry , as a result of new technological advances , has scope to change . the physical techniques of construction will not change dramatically , but process development and project planning can improve the management of the industry , making it more cost - effective . the aim of this dissertation is to provide a web - based application that will assist in making improvements to project planning . the application aims to develop a system of dynamic project planning , which allows schedules to be changed as tasks evolve . if a task takes longer than originally specified , the estimated duration of the task will increase on the application and the project schedule will change automatically . the system should also work in the opposite way . if the original time specified has been over - estimated the system should change the project schedule automatically . the application has been developed using a relatively new technology , enhydra xmlc . this report documents the process of the development of the application from the initial requirements and analysis stage through to project completion .
existing models of the heart use finite element models and prolate spheroidal coordinate systems to generate data consisting of a set of parallel planes along with fibre geometry . visualization of this data is important for characterising these models and help understand the link between structure and function of the hearts . i describe in this report what techniques can be used to represent the data graphically in three dimensions and what tools can be used to solve the problems faced in creating the required visualization tool . i then describe how i created such a visualisation tool .
this project started off with the open - aim of ' progressing ' a web - based tutorial on the subject of the brayton cycle - a series of processes that occur during the operating of a ' brayton engine '. through discussions with members of the mechanical engineering department , a series of aims and sub - aims were established that would improve upon the original model . one of these aims , and a recurring theme of this project , was to use techniques from the area of computer aided learning to develop a tool that can efficiently educate a student . the system was successfully implemented using a range of e - commerce technologies including java server pages , javabeans , servlets and a tomcat server .
this paper examines the widespread difficulties that both children and adults experience with elementary concepts in electricity . we identify the long and pervasive history of these issues and argue how they may be addressed . the two key aims of this work are to identify which concepts in electricity students find most problematic and to provide an it - based tool to facilitate the communication of these concepts . the basic ideas for this tool were put forward by dr . jon scaife at the department of teaching at the university of sheffield . the solution was required to be an interactive animation that embodied the high - level bahaviour of electric current in dc circuits of cells and resistors . arbitrary circuits of the required format may be input and processed . input circuits are drawn in the conventional digrammatic fashion and currents in each section correctly calculated . a simple yet intuitive user interface was successfully implemented and structured to facilitate maintainence and reuse . many techniques to overlay the high - level behaviour of electric current on a system of particles were prototyped but proved inadequate or simply untenable . a suitable technique was not found in the time available and so unfortunately an adequate simulation could not be developed . perhaps the most important achievement however has been the exploration of the aforementioned techniques and the guidance this work will provide any future engineers wishing to produce a tool of this kind .
multidimensional data is widely used for analytical purposes of a whole host of situations . to aid in the analysing of this data it is often useful to be able to visualise this data . however humans can only visualise a maximum of 3 dimensions . this project looks into developing a system that can display multidimensional data in true 3 - d via the fakespace rave . this report covers the investigation in to many multidimensional - 2 - d representation techniques and their possible extension to 3 - d . the report then follows the implementation of a selection of the techniques . the methods implemented in the finished product were principal component analysis , radviz and projection pursuit the final system gave good results at displaying multidimensional data and provides a good basis for further work with exploratory data analysis
departmental research groups rely on a number of resources to conduct their research . currently there is no system or mechanism for documenting all these available resources and making these resources available to the groups for their projects . these resources are typically of books / journals / theses that individual people own , cd - based and intranet based resources such as conference proceedings , both electronic and hard copy version and corpora . in addition possibly equipments and tools that are used . a system that catalogues all this resources could potentially save the time and inconvenience of ordering from the library if not held by the university library . naturally this system will allow this information to be searchable . the system will be a web - based application . the system allows maintenance by an administrator but also allow individual researchers to update the resources they own .
incorporating the value and power of a search engine into the everyday use of a text editor is a fairly unexplored , yet potentially useful field . manipulating access to both of these resources in one piece of software so that users can perform searches on words in their document can not only provide convenience , but by introducing information extraction techniques it could make use of the whole document in order to coin a more appropriate search query which in turn should retrieve better search results . the aim of this project is to produce a powerful search tool that implements this idea in order to help users understand the words that they are dealing with by letting them find related web documents . text editing packages already provide features ( for example thesauruses ) that help users understand all types of word except proper nouns ( i . e . the names of people , places , organisations etc .), so this project will be innovatively aimed at dealing with proper nouns .
automatic authorship attribution has a number of real world uses , from resolution of disputes over the origin of historical literature , detecting plagiarism , interception of hidden messages in correspondence as well as in the field of forensic linguistics . this project aims to explore techniques for the automatic attribution of text authorship by machine . it investigates existing text classification methods with a view to building a simple and reliable system to categorize documents into a number of known classes by author . for the construction and training of a text classifier , machine learning concepts will also be employed . the material to which this will be applied will be popular works of fiction taken from the catalogue of project gutenberg .
with the advent of the internet in 1993 , a whole new way of shopping , researching and communication became possible . its popularity growing daily , with 533 million users [ computer industry almanac ] to date and a predicted 945 million [ computer industry almanac ] in 2004 , so too have the demands of the users . people with disabilities form one of the largest online communities , as the internet and web have enabled them to take advantage of computing and communication to perform tasks of which they were not previously able . things that were once difficult for people with disabilities , such as buying goods at a store , are made easy by e - commerce technologies . however , research suggests that , 95 to 99 percent of all web sites are inaccessible to the visually , hearing or mobility impaired . this project aims to provide an accessibility web browser that will improve this figure .
a vast number of creatures in the animal kingdom exhibit some kind of fur . this paper presents a complete ' shells and fins ' method for modelling this fur in real - time for use in interactive systems . a wide range of properties are researched , evaluated and adapted for use in the project . original methods are presented for hair type interpolation and fur dynamics . here , the concept of hair domination is introduced as a method to decrease hair quantities in a fur patch , and the wind vector animation approach is adapted for use on fur . a review is then made against other methods of fur rendering to develop concept for further work .
there is growing interest in higher education for electronic learning resources and tools which could make use of the large technological infrastructure that has built up around students and institutions over the past few years . this paper investigates the technology that would be required to produce a tool for the automatic visual and aural capture of lectures . a possible implementation is then presented , based on a system of key - frames to encapsulate the lecture ' s content and meaning , and is tested against synthetic , mock -, and real lecture data . the resulting system shows promising results , but is intolerant to some of the more unanticipated complexities of real video data and the evaluation concludes that while most of the initial design is effective , some is ineffective in dealing with real data .
the study of the stability of numerical systems is key in developing new algorithms for computer aided geometric design ( cagd ) packages . there is well established theory surrounding the condition numbers of distinct and multiple roots . however , little work has been done with respect to the condition numbers of a nearly singular simple root of a polynomial . this case is important as the effect of a close neighbouring root must be taken into account in the same way that the effect that two close magnets on each other must be considered in physics , even though they may be considered in isolation when they are far apart . this project addresses this issue and investigates new theory for this case , starting first with the univariate case and then extending it to encompass the bivariate case .
as computers get faster and faster , the amount of ' idle cpu time ' for the average computer user continues to increase . while a typical person will use their computer for word processing and other office tasks , listening to music , watching videos , and browsing the internet , there will usually be spare ' processing time ' available . there are many scientific and commercial projects around the world that require large amounts of processing time , even on fast modern supercomputers , to calculate results . if a service were available whereby these processing ' jobs ' could be farmed out to thousands of users , this may provide a cheaper and faster means of running these projects , and some monetary return for computer users subscribed to the service . this project develops such a service , with the help of javaspaces , a java jini service providing a means for distributed processing over networks . space - based processing is investigated and implemented to form part of a web service which can distribute code and data as objects for remote users to process and send back results , achieving an effective , scalable system for distributed processing .
computer facial animation first originated in the 70s with the advent of the first polygon mesh model of a human face . from these humble beginnings real - time facial animation as a medium of human - computer interaction is becoming more common and widespread . the process of generating facial animation that is synchronised with an audible voice from text input is known as visual text - to - speech ( vtts ) synthesis . this project expands this process to include other aspects of human emotion that cannot be described using basic text to create expressive vtts . the application for the project is a children ' s story reader . emotional vtts is achieved through the use of story text mark - up language ( stml ) files , specifying the text to be read along with emotive gestures that are used to expand the meaning of the text , this should improve the realism of the synthesis . the face is modelled using a polygonal mesh and the animation is controlled using linear interpolation . the results of this approach are quite encouraging . the story reader produces synchronised , expressive , facial animation in real - time . the expressive aspects of facial animation are expressed as a combination of anger , disgust , fear , happiness , sadness , and surprise . although , no other emotive elements were included , the success of simple facial emotion has been shown to improve the quality of vtts .
a method for managing large numbers of sound files needs to be found for use as a tool in music composition software . this tool would extract musical statistics ( metadata ) from a sound file , which describe some characteristics of the file . the database of sound files could then be sorted by their similarity to a target sound . this project reviews some of the prominent techniques in the field of audio retrieval and classification , and implements and tests them on a small musical database . the best techniques form the basis of a new tool for the classification of sound files .
for mobile robots to be functional a suitable navigation method is needed . when placed in an unfamiliar location , people may use signposts to help them find where they want to go . this project hopes to show that robots may be able to do something similar . research has been conducted into previous and current navigation techniques and systems in the area and into the hardware that will be used . during this project a reactive robotic system that uses infrared beacons as signposts to direct it the robot towards a goal was designed and implemented on hunter robot . the robot also has an object avoidance system so that it does not collide with objects in its path .
this project was conceived during the second year software hut project where the author was first introduced to the concept of pair programming as part of the extreme programming methodology . during that time it became apparent that there was a distinct lack of a workable collaborative editor suited for development . the aim of this project is to provide a fully functional and easy to use editor for collaborative development . this includes devising a concurrency scheme and a means for communication over a network between the various clients . this project implements a client / server architecture , where updates are distributed via a central control authority . this report presents the results and achievement of the project as well as the processes used to create it . a concurrency scheme and network protocol have been devised and are presented in this report . designs for the server and client applications are presented , as well as a record of implementation . the project has succeeded at implementing a usable , though not entirely complete cross platform collaborative editor .
the control structure known as a finite state machine is often used to implement the process of lexical analysis , performed in order to convert an input string into a series of token units . these token units can then be parsed , without backtracking , according to rules which can be laid out in a deterministic form using syntax diagrams . the second year module com2010 - introduction to functional programming introduces to students both lexical analysis by finite state machine , and deterministic parsing from syntax diagrams . these concepts are introduced using the functional programming language haskell . it is the aim of this project to produce a tool which can aid the students in understanding the haskell code used to implement these concepts , and also to aid the development of lexical analysis and parsing systems undertaken by students .
the teaching of computer ethics to software engineering students through university courses , this project demonstrates , could be further refined in order to demonstrate the importance of ethical values in the computing community and discipline . the importance of making potential future it experts aware of legal , moral and social obligation is paramount to building a professional community who are able to conduct themselves in a responsible and exemplary manner . a narrative approach to case study presentation has the intention to project these important fundamentals to students through an engaging text , encapsulating characters with feeling and personality , and a storyline which students can comfortably read whilst absorbing the ethical dilemmas faced by the people in the story . this will hopefully engage the reader at a deeper level with the text , promoting a more innate quality of understanding , which will in turn help fuel more emotive and passionate ethical debate in group discussion . student appreciation of the weight and principles of the course material presented in professional issues is the key element of this project ' s goals ; if the information and findings arising as a result of this project can offer steps towards this aim then it will have been a worthwhile effort .
multi - agent systems ( mas ) are part of an emerging field that can be applied for solving complex problems across any domain . mass are envisaged by models of biology and economics . these systems represent a new way of analysing , designing , and implementing complex software systems . they have many unique features : they are highly autonomous and are able to quickly adapt to changes and this makes them an obvious choice to be used in uncertain environments . due to the autonomy exhibited by many intelligent agents , once they are running there may be very little scope for control . agent oriented software engineering ( aose ) [ 8 ] is a promising approach for developing such complex mass . despite the great deal of research in this area there are many challenges that need to be tackled , for example , designing a system that is sufficiently autonomous to serve in different domains . the aim of this project is to investigate some object oriented ( oo ) paradigms associated with the design of mass , compare them and identify key features that can be implemented using formal methods . to gain a better understanding of this it would prove useful to carry out a case study on the foraging behaviour of ants . in this project i will try to overcome some of the problems associated with designing multi - agent systems by providing a generic framework to allow models of various agents to be quickly constructed and tested in a simulated environment . a tool will be implemented that will allow the user to construct the environment and models of agents that make up multi - agent systems . this project will mainly deals with simulating biological systems as they often exhibit behaviour analogous to that of a well - designed multi - agent system .
robotics is one of the popular subjects in the department of computer science in the university of sheffield . numerous students enrol this subject every year . it is arduous to arrange practical works and laboratory classes for students ; due to the enormous number of students and limited availability of robots in our department . a robotics simulator is one of the possible solutions to address this problem . the virtual environment of the simulator allows students to design , develop and test their virtual robots and control programs . students therefore can experience the robot development process without having physical robots . this project aims to develop a robotics simulator for students to use as a tool for developing their robots and control programs . java ™ programming language is selected for developing this project . in order to allow students to create and edit simulated worlds and virtual robots , two accessory programs are added into this project . these two programs are used respectively for creating and editing simulated worlds and virtual robots . furthermore , multiple virtual robots and control programs are supported and can be run in the simulator simultaneously . the other goal for this project is to ensure that the control programs developed in this simulator are capable of controlling physical robots without major modification of their source codes . the simulator and the accessory programs of this project are developed successfully despite of some of the requirements have not been fulfilled . the two accessory programs , " world builder " and " robot builder " are running in a reasonably level of standard . the simulator is also working in a satisfactory level . furthermore , the control program developed in this simulator has controlled a physical robot successfully . the fundamental goal of this project therefore is achieved .
the aim of this dissertation is to create a case tool for the discovery method for object - oriented analysis and design . the case tool shall be developed in java and provide the standard functions expected of a tool of this type . the tool shall support the task modelling phase of the discovery method focusing upon task analysis , in particular task structure modelling and narrative modelling . the case tool allows the editing and manipulation of the graphical form of task structure models and narrative models . this can be performed from various perspectives all integrated through a single user interface and data repository . the tool shall allow for editing of all integrated models and views , while maintaining consistency of structure and data integrity . the case tool shall automate the process of converting task structure models to narrative models . this is one of several anticipated projects with the final goal of producing a fully functioning case tool for all phases of the discovery project . an integrated framework shall be adopted to allow the future evolution of the tool . this dissertation is being performed concurrently with that of griffiths , both contributing different parts to the project goal of an integrated case tool .
it is hoped to develop a teaching environment , where robots will work in research lab 4 under the auspices of a technician and the students will use these robots from our teaching laboratory . these robots are java programmable and the actions of the robots will be captured on video , which can be streamed to an individual student , a group of students or indeed to the whole class . there will be a communications system in the robotic area so that an individual student can communicate special instructions to the technician . when students want to test programs on one of the robots , a queue manager will be consulted , which will indicate the estimated waiting time . if the student still wishes to test his / her code then s / he can queue their code . when a student ' s code is loaded onto one of the robots the loader communicates back to student . the current video stream is directed to the student ' s terminal and the student will be able to communicate with their robot and they can use the voice system to communicate any special instructions to the technician . this project will look at developing such a queue manager ( the queuing server ), able to load and execute code automatically on various items of hardware ( such as robots ) and then use live video streaming to allow students to watch the code being executed .
sea shells use activators and inhibitors to control pigment production . the shell consists of calcified material growing at one margin , the growing edge . it is here where pigments are incorporated to produce patterns . the main aim of this project is to produce a piece of software that visualises activator and inhibitor reactions in pattern formation . relating this process back to biology is also undertaken as a plausible route of development . the functionality of current pattern simulation tools is used to guide the design of the system being developed . the crucial aspects for inclusion are parameters available for the user to manipulate , the effects being displayed in the consequent pattern that is produced . experiments of plausible development tools for this system lead to the emergence of matlab to generate a graphical user interface as well as different plots that are implemented in the final simulation tool delivered .
throughout this university and undoubtedly nationwide , there is a need for departmental heads to retrieve and analyse feedback from students , on the departments modules and degree programs . this project attempts to reduce the time process involved with the common paper - based questionnaire scenario , through the implementation of a web - based application that can be used via a browser , by both students and administration staff . research into current online questionnaires systems has been discussed , and implementation achieved through the use of active server pages coupled with a relational database . this adaptive and dynamic advance can be traced back to the requirements . analysis , design , coding and testing were all carried out using the incremental paradigm approach and future areas of interest for the system and subsequent projects are also discussed .
we all have an innate ability , which has existed since the earliest years of our infancy , to recognise faces . computers , on the other hand , do not . this dissertation is set out to analyse the various techniques used for facial recognition of images , their performance and the difficulties that may be encountered . a system is implemented to use the eigenfaces technique for recognition . the details of this algorithm , both in the case of training and identification using a classifier , are described . test on the system will be performed using public benchmark databases and tests will be performed to reveal the tolerance of the algorithm to image variations .
the focus of this dissertation is the automatic generation of timelines as an intuitive interface for interactively browsing the events contained in a corpus of broadcast news transcripts . in this project we discuss and evaluate a simple statistical model of term frequencies in a stream of unstructured text that is made up of transcripts of news broadcasts . the primary focus of the project is to extract significant features from the text and with these , identify events that are contained in the stream of broadcast news transcripts . the goal is to have a list of all events , their relative importance and their first occurrence and duration in the collection in order to generate a timeline interface .
reactive robots are electronically created moving objects which respond to their environment . this project investigates existing robots with the aim of designing and creating a robot that will be able to collect ping - pong balls . existing collection robots are examined as well as robots which perform object avoidance , object location , object collection and navigation involving returning to a set point . plans are then produced to modify an existing robot enabling it to collect ping - pong balls . the implementation of these plans is then discussed and the ability of the robot produced to fulfil the specification made is tested . further evaluation of the robot ' s ability to find ping - pong balls is performed , with the aim of maximising the efficiency of the robot .
snooker is a game very similar to pool or billiards . it is played on a table covered in green cloth . it is a tactical game where players must plan ahead if they are going to score enough points to win . this planning ahead requires the players to spend quite a lot of time thinking , which leads to pauses in the action of the game . the aim of this system is to take an unedited video of a game of snooker , recorded from the television , and reduce the amount of time it takes to watch it without removing any of the information that is important to following the flow of the game . to achieve this the system will have to use a combination of video , image and audio processing techniques . to date a framework has been produced to read videos and initial versions of some of the image processing that maybe required for the system have been produced .
when prospective students or visiting dignitaries come to look around the department , it would be impressive to be able to produce a vehicle for every research group in the department to be able to showcase some applications of their work . an autonomous guide robot would provide such a vehicle whilst , at the same time , being able to show visitors around . the aim of this report is to embody the behaviours of localization and goal seeking by studying other successful guide robots and selected generic robotic programming techniques . these findings are used to implement the required behaviours onto a dcs guide robot . the report results in the implementation of a robotic architecture that while not yet deployable in a real world situation , provides a foundation for further work to improve the robot ' s performance .
in the 2003 software hut ( sh ) module , the three winning teams developed software using a variant of the traditional software engineering methodology . despite the fact that half the groups involved developed software using the agile methodology known as extreme programming ( xp ), none managed to develop the best piece of software . this paper describes the motivation , development and construction of the project ' s investigation into the difficulties in understanding xp . it begins by examining the components that form the building blocks of an xp project through analysing past projects and papers , which in turn motivates several project hypotheses regarding the most problematic areas . the resulting chapters detail the creation of mechanisms whose aims are to validate these hypotheses and ultimately help improve undergraduate students ' understanding of the xp methodology . the project delivers a case study and on - line support tool to address these issues and goes on to analyse these solutions from the perspectives of requirements , testing and user evaluation . the conclusions of which provide the direction of further ways to investigate the area .
the inclusion of imperfections in a synthesised image makes it look more ' lived in ', and can improve the perceived realism dramatically . currently , if an artist wishes to include blemishes , then each one must be modelled and placed within a scene . this can be very tedious and take many hours . this study aimed to automate the imperfection addition process , by allowing each scene object to emit affections which create imperfections on objects that lie close by . the imperfection that occurs will be affected by the force with which it was emitted from the affecting object , and by the susceptibility of the object it is to affect . the project successfully allows interactions between scene objects , and imperfections can be automatically added and localised to a certain area of the scene that surrounds the affecting object . the imperfections created mainly utilise texture application , although the system has implemented simple volume imperfections as well .
with the resent addition of a sound api to the java programming language it is now possible to create digital audio applications in the language . the aim of this project was to produce midi controlled software based modular synthesizer , including standard the modules that one would expect to find in a hardware modular synthesizer , such as oscillators , filters , envelope generators , etc . it was intended that the set of modules would be easily extendable , and that the module implementations would be reusable in different applications . these aims have been achieved . all the module types have been created including a digital approximation to the moog vcf . the module set can be easily extended , and the module implementation is independent of the application .
this dissertation discusses the implementation of a system for sound recognition , it aims to detect a sound event within an unclean sound environment . recognition of the sound environment is the first step toward the classification of the sound event . thus far the field of sound recognition has been a neglected area of research , with the majority of work devoted to the specific area of speech recognition . this project aims to extend research into this field by incorporating some of the techniques used in speech recognition and some techniques used in general sound recognition . by using the gaussian mixture model as the statistical classifier in the recognition stage it will be possible to train the gaussian ' s to model the sound data .
but wait can a computer tell if a program is well written ? this report will look at how what makes a program code to be a good program code and how a computer tool can be use to judge programmers code . a semi - partially successful tool was produce that checks code if it follows a coding standard , which encourage beginner programmers to write easily read code .
the second year module " com2010 : introduction to functional programming " aims to teach students the fundamentals of functional programming by means of the functional language haskell . two systems have been implemented to help them . the hugs environment used in the department does not currently provide suitable debugging tools , and few tools are available for debugging haskell in general . therefore the main aim of this project is to implement a debugger . the debugging system described , hades , extends work on sparud ' s evaluation dependency tree , overcoming its limitations by incrementally constructing the edt , and allowing users to choose which sub - expressions to evaluate . the front - end is written in java , and also has the two - fold advantage of allowing users to learn better the ideas behind functional programming , as well as being a neat and efficient solution to many problems that students are likely to come across . overall the system works well , and has received much positive feedback . the evaluation presents some ideas of how this system could be further improved to overcome the few problems it has . the second system involves the implementation of an online tutorial to be used in conjunction with the lectures given for the module . although time constraints meant that a full website was not implemented , it provides an aid to the lecture notes for the course , and the quizzes which can be attempted online were found to be useful in understanding topics .
the aim of this project is to give an overview of the p_systems as part of formal language theory , and to develop a graphical interface for specifying them . the tool built in this respect should enable the user to fully define a p_system through the provided graphical interface . the user interface should be self - explanatory allowing for the novice users to easily specify and work on a p_system without any previous knowledge on p_system mathematical definition and properties .
the principal form of human interaction with a computer is via a mouse andkeyboard , requiring the user to be stood or seated at the terminal . current alternatives to this require large amounts of ( often expensive ) equipment . throughout this paper , an unobtrusive alternative to current interaction techniques will be explored , allowing the user to control a computer via movement , recognised through a visual medium , namely , a web - cam . furthermore , the solution will provide a framework , or library of techniques , for application programmers to utilise when creating new programs ? freeing the user from the traditional bonds of a mouse . a principle aim of the paper is to ensure that the solution requires minimal extra equipment , and offers no further imposition on application programmers . the solution must be real - time and require low cpu overheads .
remembering directions has been a problem for people for as long as people have had places to go . a classic example is that of a family lost in a car pulling over and asking directions from a local , who proceeds to reel off a list of directions . the family thanks the local and sets off confident of getting to their destination , only to get past a few junctions before saying “ was it left here , or right ?” this project will take a look at systems that are available to find directions and give them to the user , whether they use multimodal generation or not and then will investigate which methods of communication with the user are most effective in conveying the correct set of directions to the user .
developments in artificial intelligence have made it possible for robots to accomplish complicated tasks in the real world . however , to develop an effective robot control system can be a lengthy and expensive process without the help of computer simulation . computer based simulations have increased the efficiency of this development process and have opened the door to simulating environments which were not available before . this project will try and build an accurate and realistic simulation of a robot and it ' s interaction with the world . its movement and collisions with other objects will be aided by a physics software development kit called the open dynamics engine ( ode ). the simulation will be developed until it behaves as a robot would in the real world . once complete this can be used to extrapolate its behaviour in alternative environments .
this is all very well where the user has the physical ability to use the key tools , a keyboard and mouse , to take advantage of these technologies . my project has spawned from the stardust ( speech recognition for people with severe dysarthria ) project , dysarthria is a common speech disorder which , in its severest form , results in unintelligible speech . this disorder is commonly associated with other general neuromotor disabilities . [ 1 ]. so people with dysarthria often have motor - neuron problems which has the result of ineffective use of a keyboard and mouse . as fine control of especially the mouse is crucial , i will endeavour to use speech as an input medium . so the aim of my project is to facilitate the use of web - based services for people suffering from dysarthria . here is delivered a report to accompany the system developed , a voice enabled web browser .
this paper presents a java application programming interface ( api ) to facilitate genetic programming , which will allow programmers to evolve a population of programs to learn a function or perform a task . this is achieved by more frequently selecting and breeding those programs whose output approximates a target function . by providing an extensive literature survey , followed by a detailed account of the design and implementation of the api , the paper proceeds to demonstrate how recent developments in genetic programming research have been integrated into the project software . once developed , the api forms the basis of a programme of experiments comprising symbol regression , pattern classification and waveform timbre classification tasks , each of which is reported in the paper alongside appropriate critical assessment of the apis performance . of particular interest is a study which investigates the classification of instrument timbre from a vector containing either a short section of the time domain signal , or its corresponding frequency representation . this is in contrast to most applications of genetic programming to tasks of this nature , which perform classifications on the basis of pre - extracted features alone , or alternatively , destructively modify a copy of the signal in an indexed memory .
spam is flooding the internet with many copies of the same message , in an attempt to force the message on people who would not choose to receive it . with the various email filter tools available , a user can simply use one of these tools to filter out spam . however , more than often , spam is able to find a way into the user ' s email inbox , even though such a tool is utilized . thus , this project focuses on the research , design and development of an email filter tool using the approach of an information retrieval technique , namely the vector space model . a set of training data is given to the email filter tool , such that it would be able to recognize the semantics of spam . thus , whenever spam is matched in the users account , it would be filtered . next , utilizing the similar technique , the function of categorizing non - spam emails into subcategories is attempted . thus the user is able to categorize exactly where each non - spam should fall in . finally , a email search tool is implemented such that a user is able to search through the vast amount of emails in his / her account for a particular matching email . finally , integration of all these said functions into a single functional email program is attempted .
new technologies are common place in a subject that is constantly evolving . some technologies that appear to be the next big thing soon become yesterday ' s news . two such technologies that look like they were made for each other are java and xml . xml provides the possibilities of completely portable data . this makes the distribution of data a simple and viable option for a high percentage of people . with the possibility of having so much data flowing around a technology is required to handle the data and perform user specific functions . java provides the technology to utilise the data stored . this application combines the two technologies to provide a portable data store that is a cross platform application .
the aim of this project is to produce short compressed sentences from given long sentences . these compressed sentences should contain all the important information from the long sentence and be grammatically correct and be a shorter compressed version of the original sentence . sentence compression is becoming more important due to the new advances in technology such as mobile phones and pda . with the wap now widely available on mobile phones it is becoming increasingly important to compress the information on websites so that it is viewable on mobile phone and pda screens . this compression also means that the size of the information sent is much smaller , this saves both time and money . in order to maximise efficiency large sentences are required to be compressed ( shortened ), into a smaller sentence which contains all the important information from the larger one . however the sentence has to not only contain all the relevant information but also be grammatically coherent and make sense . hence important words cannot just be taken from large sentences and put back together to form a shorter sentence , this would make no sense at all to the reader . it is also very hard if not impossible to define important words in a sentence , never mind extract them . the aim of the project is to produce a series of rules from training sentences , which when applied to unseen long sentences will output a shorter compressed sentence . the training sentences will consist of a long and short version of sentences from which rules will be learned as to be able to transpose the long sentences to the short sentences . the aim of this project is to be given a set of unseen sentences and apply the rules crated from the training data , producing a short compressed sentence , which should contain all the important information from the long sentence and be perfectly coherent but is shorter . this can then be compared to a human created compressed version of the long sentence and graded accordingly .
the world of computing is changing rapidly , with new protocols for performing inter - application communication being developed in quick succession . it is important that modern programming languages keep up with the new development and provide access to the new techniques as they become available . different problems are best suited to different kinds of languages , and thus it is important that there be a wide spectrum of paradigms , which can be employed with in the interoperability framework . this project focuses on the purely - functional programming language haskell , in an attempt to bring to the world of functional programming the many benefits of interoperation , and to bring to the world of interoperation the many benefits of the purely - functional paradigm . the culmination of this work has been the haskell application interoperation framework architecture , a framework on which to build interoperating applications with minimal repetition of basic concepts .
it has been observed that the successful recognition of a talker on an automatic speech recognition system varies , with most people being recognised , but with a few not being recognised . this project aims to find a method for predicting a talker ' s performance when using an automatic speech recognition system , by analysis of various characteristics of the talker . these characteristics are ; consistency , clarity and conformity . predicting the performance of a talker would be much cheaper and less time consuming than the method of running an automatic speech recognition trial on that talker .
in russia 1919 , an instrument that could be played without touch made its debut to the world . settling into popular culture it became known as the theremin , after its creator leon theremin . this project aims to recreate the theremin in a digital form , where control of the instrument will be achieved via the use of a webcam . the paper explores object tracking techniques to discover reliable ways of tracking a hand , that are also computationally efficient , as the system must run in real - time . methods of synthesising sound will also be reviewed . however , instead of replicating the original theremin sound , avenues will be explored to try and broaden the ability of the instrument by incorporating new sounds and new methods of control . a software engineering process of development is undertaken , which is detailed in this paper from background literature to system requirements , design to implementation , and finishing with testing of the system and a discussion of the results .
algorithms used in state space search have varying success rates when applied to certain problems . the majority of algorithms are logic based and have some success , however there are algorithms becoming more widely available that rely on lessons learnt whilst studying living creatures . one such algorithm is the ant colony optimisation algorithm which is based on the behaviour of ants whilst in search of food . this algorithm has a great success rate when applied to certain state space problems but the generated data can be hard to analyse . visualisation of an algorithm can give a much greater insight into how it is working and this project aims to develop a system to visualise such an algorithm search in virtual reality , a medium with great potential when it comes to visual manipulation and analysis .
neural networks can be used in many ways on their own but are also used collectively to compose ensembles . it is possible for certain nets to generate errors on different data and so in an ensemble these nets produce errors that other nets can compensate for , so that the best possible result is obtained . the aim of this project is to investigate how neural nets can be combined together into ensembles to provide a better generalisation performance . at present most approaches ensemble all the available neural networks , however if only a few were used to create an ensemble a better generalisation could be achieved . furthermore the issue of diversity among ensembles is considered in order to establish whether it really is necessary for effective ensemble creation . ensembles are created and experiments performed to determine whether the combination of a selection of neural nets is more beneficial than ensembles composed of all neural networks available . in addition , an attempt is made to assess the diversity between neural nets to see how it affects the creation of ensembles . the disagreement measure and the double - fault measure are two measures of diversity used to see whether recent doubts about the benefits of diversity in ensembles are plausible .
with the amount of research performed during a phd course , the client , who is the managing director of the company responsible for the website " doctoralstudents . com ", proposed the idea of a bibliographic data storage system which would be used to store references from sources that the student wishes to use in their work . this report details the research and processes that were undertaken to implement a working bibliographic storage system using xml and java . included in this are the steps of research that were necessary to determine the way in which the solution should be implemented and the design of the system that would implement the solution . the software produced was then tested to ensure that it is functioning correctly and the project was evaluated to show the way in which the clients needs were met , how the technologies used for the development of the software helped the development , the processes that were used during development and the way in which the application could be enhanced .
over the past few years , automated translation tools have become more and more utilised by the general public . however , the results provided by these tools have come under criticism for their lack of accuracy and the countless errors that they make . this paper describes the different ways of measuring the performance of varying machine translation ( mt ) systems , and evaluates the effectiveness of the current online translation tools using these methods . currently , it seems that online mt systems will never be accurate enough to replace human translators due to their need for real world knowledge . however , the role which they currently play as an aide to the general public whilst surfing the internet is important and effective . the code here is a perl script called autotag . pl . it runs by passing it the filenames of the files that you want to be tagged . i used it with all my tagged files in the same directory . for example :- would tag the file untaggedfile . txt
the sheffield urban contextual databank , or sucod for short , describes a project dealing with the dynamic production of on - line virtual cities . the project has been developed by the university of sheffield ' s school of architecture . sucod currently stores data for buildings in sheffield city centre in and around the year 1900 . the aims of the project are to extend the existing sucod system , to enable users to edit the virtual world in which they stand . the functionality provided will allow for new buildings to be modelled and added to the virtual world ; and then later modified , both in terms of appearance , size and location . the purpose of this is to enable architects to quickly and precisely sketch new town planning ideas ; and immediately see the effects of their developments on the surrounding city area .
this project documents the designing and building of the sheffield ego dialogue system . this is a fully interactive useful and practical interface combining both natural language capability and a database of information about sheffield . the user enters into a dialogue with the system in order to find out about entertainment and going out in sheffield . the database is created in xml with its own custom built ( perl ) database manager , and there are 100 records in the database . there are a wide range of events and activities stored in the database , all of which take place in the city of sheffield , and there is a broad target audience . the database consists of activities / interests in the following categories : restaurants , bars , clubs , music events , sporting activities and leisure activities . the sheffield ego system is built using perl . the graphical user interface creates an instance of the dialogue class , which takes a string as an input and returns an output string . the internal state of the dialogue is represented as a pseudo finite state automaton ( fsa ). there are three subgoals which the dialogue attempts to find a value for , these are the activity , the location and the time . the current state of the dialogue represents the current subgoal , and both the processing of the input and generation of the output depend on this state . the aim of the dialogue system is to collect enough data through this dialogue in order to search the database for relevant activities . the results of the search then determine the next state in the dialogue , and when the dialogue reaches a final state the results are then integrated into the output .
this report describes the design and implementation of eds an electronic diary system for the department of computer science . eds allows both staff and students of this department to arrange meetings and appointments with other members of the staff . a number of architectures and design models were examined for this purpose but mvc design pattern within a three tier architecture was found to be the most suitable approach . although , a number of technologies such as jakarta velocity and xmlc could have been used for implementing the designed architecture but jsp type ii solution was deemed to be the best approach .
artificial neural networks are currently being employed in a wide range of areas . although certain tasks can be solved using single neural nets , much research is now being done into combining neural nets to improve performance . two such methods include the ensemble approach and the modular approach . the aim of this project is to build modular neural networks using some chosen data sets and to compare their performance firstly from the single net and then between the data sets . therefore i will look at the different ways in which modular neural networks can be created and choose a couple of different methods to explore with . the software package that i have chosen to work with is easynn . this tool is easy to use for creating and training neural network . five different data sets were chosen to carry out the experiments with . having conducted the experiments , satisfactory results have been obtained and evaluated . the evaluation at the results produced by each data set and a comparison is made . furthermore , the modularisation methods are contrasted to see which performed the best and what type of methods are suited to different data sets .
this research project outlines the development and implementation of an innovative new 3d graphical representation approach for computer music composition and performance . the project concentrates on inserting predefined 3d models into scene , associating sounds to the objects and creating compositions by altering sound attributes in immersive 3d virtual reality environment . this paper outlines the software development cycles involved during the development of the mistres project . the discussion of different research carried out in the computer music composition and performance area is followed by a 3d graphical and sound programming technique outline . finally the mistres project is evaluated and the results obtained from user feedback are analysed in detail .
currently , the forensic pathologists use a paper - based form to store their data about skeleton remains . but there are many different kinds of standards and criteria , and most of the time it is impossible to look at previous research because there is no common database existing . so , we can consider that each forensic pathologist is more or less isolated and their work is not easily shared with the rest of the world . considering that , the aim of the dissertation is to find out if it is possible to create an " electronic " version of the existing paper - based version which would solve the previous problems . the application will have an interface with a 3d visual examination system and an annotation system to record the annotation about each bone .
this is the dissertation for the project of stable object - based model of traffic control system that is one of the sub - projects of mobile agent traffic control simulation system project mainly focusing on the stability of the system . the system has been rebuilt based on the previous work , and produces stable result . in the dissertation an object - based simulation system has been designed . the previous system built by zheng qu , a msc student in 2003 , has shown the improvement of implementing dynamic routing algorithm instead of static one . however the variance of the system output is too large for us to move on to the next stage . the variance is believed caused by having thousands of threads . the new system built in this project has solved this problem by eliminating threads , and several new traffic light control mechanisms have been adopted to minimize the delay in the travel of vehicles . there two versions of the new system , a static routing version and a dynamic routing version . the dynamic routing mechanism does not show the improvement as expected for the current system according to the experiment results . this may be because the traffic light control mechanism is good enough to deal with the heavy traffic flow in the current simulation system . further experiments are necessary . experiments demonstrate the stability of the new system , and the distinct improvement after applying new traffic light control mechanisms .
since the creation of photographs , users have had to settle with images that hadcomponents that were out of focus . with the creation of digital cameras this can now be corrected . by using multiple images of the scene , a fully focused image can be created using a computer . the aim of this project is to create an application that can be used to create fully focused images . the scene is assumed to be static . a discussion of possible methods is given along with examples of methods that have been previously developed . a final result has been made but doesn ' t work as expected due to an incorrect assumption . since the creation of digital cameras people have been using tehm to achieve things that were not previously possible with anologue cameras . this project attempts to make use of digital cameras to get rid of an age old problem . this problem is limited depth of focus in a photo . this project should manage to produce images which have all important areas in focus . as an example of the problem , look at figure 1 . 1 . the objects in the background are blurred and the detail is not as visible . the aim of this project is to make the objects in the background as clier as those in the foreground .
automatic text summarization is a process in which a computer takes a text document as input and outputs a summary of that document . there are various approaches to it , some of which have been around for more than 40 years . in general we talk about single document summarization and multi - document summarization . this project focuses on multi - document summarization . its aim is to create a theoretical foundation for the implementation of a multi - document summarization system . i review some of the more recent findings in the area and then i further discuss the approaches that are particularly suitable for this project . finally the design , implementation and evaluation of the new system are documented .
set classpath as below . you can use * simple * html , e . g . this is the first paragraph of my great work this is the next paragraph . this bit is in italic set classpath =% classpath %; jogl . jar ; set classpath =% classpath %; jujugl . jar ; set classpath =% classpath %; jvkcave . jar ;
testing of software is an important aspect of software engineering . this is emphasized by the different techniques that have been employed in software testing . a good testing method aims to prove the absence of faults by trying to show the existence of these faults . there are different types of testing methods , from the simple straight forward ones ( e . g . user acceptance testing ) to the more complex testing methods such as state - based testing , which if properly implemented can be extremely effective . the aim of this project is to a state - based x - machine test tool that test the collaboration between objects by intercepting communication passed between them . the tool developed is a specification based unit testing tool that relies on isolating an object and applying test sequences to this object
topic detection in broadcast news is one of the fields of information retrieval ( ir ) area and has been a key research in existence for many years . it ' s main technology challenge is to identify and to follow topics that discuss the same stories from sources like newswires , broadcast news programs , radio etc which are in the form of both speech and text . this project proposes to build a system that will automatically locate topic breaks and identify related stories discussed in news stories . it presents and expands on known algorithms such as the tf . idf and weighted cosine similarity metric which are used for topic detection . the evaluation of the implemented system involves the use of precision and recall measures as well as comparison between human annotated news data with topic change information and the data obtained from the automated news tool . the corpus used for this project is provided by the linguistic data consortium ( ldc ) which contains over 10 , 000 segmented news data with topic boundaries . the results obtained from the evaluations of the topic detection tool shows that the system returned relevant documents along with their topic tags for 60 % of the documents in the training data while using the cosine similarity algorithm . this concludes that the chosen algorithm as well the approach toward this project was indeed appropriate .
abstract many people who do not use english as their first language can find it hard to communicate with those that do . this report investigates the need for real - time animation and the ways in which this can be achieved . much work has been done in the field of real - time signing as it can act as a bridge for communication . as part of this report i will analyse this work and look at the methods that can be applied to this project . beyond this i will develop a system which can achieve real - time animation of sign - language and test its usefulness .
databases model the real world , which means inevitably that changes will be made . at this point , the database administrator requires up - to - date documentation in order to make the most suitable change . an in - depth understanding of a given database system can only be gained from accurate documentation that is relevant and concise . the overhead of maintaining an undocumented system is significant , especially when the original database designers and administrators are no longer available to explain it . the aim of the project was to investigate and design an automated documentation system which , with assistance from the database administrator ( s ), would create documentation for new database systems , and compare new and old databases to document any significant changes between them .
many virtual environments involve volumes of water , ranging from fish tanks to oceans . in an effort to make these environments more believable the animators will add fish to the water , such as the simple scripted shark animation that was added to a couple of large tanks in a level from the game " 007 everything or nothing ". a different example would be all of the characters in " finding nemo ", where the animators studied the movement of real fish in order to create scripts for every fish character in the film . it is the aim of this project to create fish animation in real - time , which will possess the ability to swim around a watery environment in a way that is believable . this should be achieved by taking advantage of the programmability of a modern gpu to render and animate fish that look and move in a way that can be considered realistic . this project has succeeded in creating some of the most important aspects of fish movement through the water , and achieves deformation in the model of the fish that has proven to look to many people to be very life - like . the movement was all created in real - time by a series of procedures in the code . no specific scripting needs to be done , the fish is a completely self contained program that will decide what it should do , and has the functions at it ' s disposal in order to carry out those actions .
heart disease is the leading cause of death in industrialised countries and often occurs suddenly and with no way of predicting its occurrence . the majority of these sudden deaths are due to the heart disease ventricular fibrillation . it is thought that the condition termed re - entry could be the cause of ventricular fibrillations ability to maintain itself . re - entry occurs when an electrical pulse reaches a region of damaged tissue . the damaged tissue restricts the ability of the action potential to be conducted . the use of computer models has become important in research . it was recently found that instabilities in action potential duration play a critical role in the spiral breakup of the electrical wave [ 2 ]. hence by using a simplified ionic model can experiment to see what the factors are that effect the onset of re - entry .
this project forms part of an ongoing project which aims to allow visually impaired people to metaphorically see an image by touching a virtual 3d representation of the image . the end solution will allow visually impaired users to interact with the extruded image and use it to help replace or augment their sight . this project is focused on the 2d to 3d extrusion . the program output should be an accurate 3d representation of the input image that could be recognisable by touch alone . the implementation allows a user to input an image and returns a 3d model displayed on the screen . the model is generated using a combination of greyscale level extrusion , edge detection and object identification .
e - commerce has been burgeoned and boomed into a mass state and its convenience gives people unprecedented business activity experience . though the achievement in this field is startling , there is a trend to coalesce the semantic meaning of different e - com sites with same interest . this project aims to build up software being able to digest two websites ' semantic meaning and present a combined view or result to the user . this approach of information integration would dramatically add the ease of price comparison and product search and provide a more generals view of multiple online stores ' resources .
although the lower urinary system , in particular the bladder and the urethra , is hugely important to the quality of life ; tools to diagnose medical conditions within this system have not been successfully implemented . the simulation introduced is based on the development of the underlying assumptions , both mathematical and theoretical , building the foundations to produce a formal computational model . the outputs of the model categorises whether an obstruction is present by considering pressure / flow rate graphs for simulations with different obstructions . the project scrutinises the mathematical foundations to ensure the assumptions are appropriate to the requirements of this biological system , in order to produce a computational model . in hand with producing and evaluating a computational simulation the project offers an introduction to the theory governing possible extensions to the model .
this report documents the research , design and implementation of a spatial audioresearch tool . the aim of this project was to use computer technology in order to develop a research tool that would aid researchers with experiments concerning spatial sound perception and reproduction . the aim was to create a tool that would allow researchers to easily construct 3d sound scenes of multiple moving sound sources , within a 3d environment with a high degree of realism upon audio playback . using state of the art audio rendering technology , the successful implementation of such a tool was achieved . results of testing were positive and the system seems to produce realistic sounding audio scenes .
the field of story generation has existed in cognitive science since 1973 when klein released the automatic novel writer ( klein et al , 1973 ) – the first automated story generator . this was later followed by generators which focused on two main approaches – a story - grammar approach ( such as correira , 1980 ) or a simulation - based approach such as meehan ' s ( 1977 ) tail - spin . this project proposes a story generator to model the fixed narrative sub - domain of the guillaume d ' orange cycle of the old french epic ( or chansons de geste ) and in doing so will try to determine which approach produces the most interesting , and authentic epic whilst still preserving its intelligence ( ryan , 1991 ) as a program . the product of this project is a moderately successful narrative generator which bases its approach on the controlled - simulation model of narrative generators .
computer facial animation is not a new topic ; it began in the early 1970s with the first computer basedrepresentation using polygon meshes to create 3d facial models . recent highly successful animated films such as ' finding nemo ' and ' a bugs life ' contain characters that are immediately likeable . their individual expressions help bring them to life , emphasising their personalities giving the viewer a feeling that they can relate to the character . this project aims to produce a 3d animated model of ricky gervais capturing his true mannerisms and character illustrated in his unique expressions and facial movement .
plagiarism is a huge problem in educational institutes and since the dawn of the internet , has grown massively . websites allow students to download term - papers and simply the publication of papers online increases the temptation to copy - and - paste . the automatic detection of plagiarism is not a new topic in computing . there are already systems out there that will scour the web and other papers in search of passages that have been used without permission or references . they do this with advanced pattern - matching algorithms and have generally been very successful . however , an area which has not had so much attention is detection plagiarised work in a single text with nothing to compare it to . seeing as this is exactly the method that is used by teachers to detect plagiarism it seems worthwhile to investigate a program that will do it this way . this project with explore the possibility of single - text plagiarism - detection focusing on stylometric techniques used to detect changes of authorship in collaborative writing .
in computer graphics 3d worlds are becoming more and more realistic and accessible to everyday users . many of these worlds are using procedural modelling techniques to save having to store building models , and to generate more realistic looking features . these worlds can consume a lot of power and memory to explore so methods are required which ensure the minimal amount of data is rendered to maintain the ability to walk through the worlds at an acceptable rate . a problem introduced by procedural modelling is how to deal with models that have no structure as they are only generated when needed . this project aims to provide a solution to the problems by integrating walkthrough techniques with the procedural modelling of buildings . the techniques that have been used in this work , include frustum culling , hardware based occlusion queries , and a hierarchical nested scene tree .
the internet now plays a vital role in every day life for businesses and individuals . along with the growth of the internet , online crimes are rapidly increasing . confidential information is vulnerable to theft and corruption , communication channels can be monitored and services disrupted . a firewall plays a key role in providing security as it can block common attacks and dissuade potential intruders . this project aimed to create a fully functional , easy to use , windows firewall . in - depth research was undertaken and documented covering networking protocols , common attacks , the windows architecture and windows drivers . a walkthrough of how to communicate between a windows driver and a user application is also provided to aid other developers in this area . in conclusion , a working firewall was developed , and the journey taken has been documented .
automatic text summarization has been around since the late 1950s . it has become increasingly popular in recent years due to the large increase in the amount of information available . the volume of information available today is too much to be processed manually . automatic text summaries can help process this information more effectively . the aims of this project are to gain an understanding of the theoretical underpinnings of automatic summarization and use this understanding to produce an automatic summarization system . an automatic text summarizer has been produced that , given an electronic document , can produce an extractive , variable length summary . also , the features used to determine the importance of a sentence are modified by weights , and these weights have been learnt using a machine learning algorithm . the use of a machine learning algorithm in automatic summarization is something that should be further explored in future projects .
abstract this project will detect camera motion , pan tilt and zoom , in video footage . it will be able to build both technical descriptions of movement involving direction of movement and degree of movement in relation to screen size . along side a description in laymen ' s terms such as camera moves left sharply . given a longer piece of footage a description of all movement should be possible . the work will be built on the analysis of motion vectors created with / with the aid of pre - existing software . algorithms will be built to handle the analysing of the data presented by the motion vectors to a very accurate standard , exceptions such as object motion will be taken into consideration and algorithms will be built to deal with such anomalies .
the audio summarisation tool is a software tool which has been developed with an intent to automatically produce an audio - based summary of an audio ( speech ) file , using any technique which may be considered suitable . this report details such techniques , and describes the difficulties experienced in through their implementation . the report also researches into techniques which can be used to improve the overall outcome of the summary file in terms of its intelligibility . such implementations are discussed with reference to the audio summarisation tool . tests have been applied to the system in a manner of fashions , from user - testing , to functional testing , to algorithm testing . these tests reveal the software tools ability to apply the two different summary techniques developed to different audio files , and to have the summaries produced by each method analysed in different ways , each revealing different characteristics of the system . it is far to say that the audio summarisation tool achieves its objective to produce a summary of an audio file , automatically , using information derived from the file itself . the question of to what level of success this is achieved is something that is discussed in this report .
speech - based consumer information systems have been available for many years now , but until recently , optimising these systems for different situations has been a difficult task . this report presents a project with the aim of creating an automatic method ( using machine learning techniques ) to optimise the performance of such systems , and to evaluate this method with an example system , ' sheffield fun '. ' sheffield fun ' will be a system which harvests data from the world wide web and is able to present this information to a user based on a spoken dialogue between the user and the system . at this moment in time , the system can already harvest data from a website containing all sorts of things to do in sheffield .
the main purpose of this project is to model how a spacecraft finds out its location in the solar system and its objective . this project expects to design a 3d solar system that contains the sun and the nine major orbiting planets in virtual reality environment . the autonav algorithm implemented in this project is similar to the one used in deep space one1 , by which spacecraft is capable of continuously judging its relative location within the solar system by making use of the celestial references , such as zodiacal constellations and the sun .
this research project explores the innovative construction and manipulation of 3d graphical representations of insect eggs and sperm of various forms of the drosophila species . such a system could aid current research into fertilisation patterns between different species , and provide a means of comparison that has never before been available . the project concentrates on assembling a series of two - dimensional image sections into a three - dimensional representation , which can then be viewed and manipulated for the comparative desires of the user . in particular , the project aims to make quantitative comparisons of sperm from different species , in order to explore the factors that influence the differing success rates of hybridisation .
recommender systems have become an important asset of the information age , and this dissertation is about the application of such a system within the domain of restaurants . the first part of the dissertation aims to explain and discuss some of the important research that has been done on recommender systems since they were first introduced in the early 90 ' s . the analysis will focus on the general need and application of such systems , before taking a closer look at collaborative filtering and knowledge - based recommender systems , how they work , what their shortcomings are and what remedies have been proposed . the second part of this dissertation is the implementation of a restaurant recommender website which , apart from successfully implementing a recommender system , aims to evaluate the utility and usability of such a system within the restaurant domain .
the heart is the most important organs in the entire human body . it is nothing more than a pump , composed of muscle which pumps blood throughout the body . heart has to keep contracting to make sure the pump never stops . as cardiac action potential is the key point to make the heart contracting , so it is important to build models to provide a powerful tool to create and test the hypotheses . this paper investigates the detailed process of the action potential and the way to model it by using object - oriented method . it aims to model cardiac electrophysiology model which written by cellml language and gives user a friendly user interface to allow user to control the simulation , and finally action potential changing graph should be plotted out as 2d graph on the user interface . compare with the procedure way , object - oriented way has more advantages . all the parts in the model are independent and easy to exchange , so new models can be built without too much effort .
in today ' s modern world , people value time and enjoy sport . watching sport can be time consuming . one such sport is snooker , which is very long and monotonous . it would be useful if we could cut down the time it takes to watch . one way would be to index the important parts , the shots , to allow us to navigate through them . this project involves building a system that will index snooker shots automatically . it will engage signal processing techniques to automatically detect a shot using the audio stream of a video , then place an index point into the video allowing the video to be played back and the shots skipped through forwards or backwards .
computer facial animation started in the 70 ' s with the first polygon mesh face . this area then grew to making new methods in animating real movement . further development made visual text - to - speech ( visual tts ) possible , taking a text as an input and producing a visual and audio output . pixar and pdi makers of finding nemo , antz and toy story have gained millions on their 100 % computer graphics films using computer techniques over the last few decades . they have touched every child and even adults with their movies . this project will produce a children ' s story reader . this will achieve visual tts and emotional visual tts using a simple mark - up language , which specifies the text to be read , and the emotions and gesture to generate . the mark - up language adds more detail to the text ; it describes the mood of the storyteller . the face will be a cartoon face that will appeal to children . this will be done using polygon meshes . procedures of animation will change the appearance of the polygon mesh to achieve emotion . the storyteller , the finished program of this project , produced audio expressive visual speech in real - time . the expressions were formed using six basic expressions ; anger , disgust , fear , happiness , sadness and surprise . by using a combination of these expressions an unlimited amount of faces was created . an addition feature from james hall ' s story reader was the implementation of gestures . the face can now , wink , look in a direction , squint and move the eyebrows when commanded in the mark - up language . this improvement has given the project a new aspect for creating visual tts .
everyday more than 300 millions people search over 60 million pages of information , or two billion documents , on the web for information they need . powerful search engines , such as google and yahoo , have played some roles as a " yellow pages " on the web . however off line searches and continuous news of updates are not possible . some webpages can be updated rapidly and people want to be aware of them in good time . this project arms to provide a personal web monitor which could proactively carry out off line searches to identify updates on specified websites by users . by setting up the email alert , users can be informed and view those updates on their convenience . moreover people can receive webpages periodically , which are similar to their specified ones . the personal web monitor achieves this by storing the content of a webpage at the moment when it ' s specified as a text in the database . at the time indicated , it then retrieves the content of the same webpage again and compares these two texts by using the n - gram overlap algorithm . if the score is above the threshold , it replaces the text in the database with the new one and sends a notification about the update to the user .
computers have been used to play games for decades , and in some cases such as chess and backgammon , have reached world class standard . this project attempts to create a program capable of playing the fives and threes variant of dominoes using bayesian networks to model the probabilities involved . dominoes is more than a game of chance , and a good player needs to work out the probabilities of opponents having certain dominoes in their hands . bayesian networks provide the ideal basis for working with incomplete information and allow probabilities to be generated and updated with ease .
self assembling machines are seen as the next step in construction of nano and micro and macro scale devices , current lithography techniques used for micro scale assembly are reaching there limits due to imperfections in optical lenses and the wave length of light that is useable to etch circuit designs on to silicone . the most complex systems know , biological enteritis and ecosystems , are produced over varying time frames by the interaction of different molecules and the replication of cells . embryonic cells groups of different organisms have many similarities with each other and it is only after months of the application of the rules encoded on the organisms dna that visual differences can be noted . the aim of the project is to produce a generic self - assembly algorithm and use it to produce a framework that is able to solve a set of simple self - assembly problems . the framework will allow a user to alter the size of simulation environment and the number and type of component parts in the simulation . the user may switch between single and multiple aggregations of parts in tilling and flocking scenarios and view the number of loops the simulation performs .
presence is a degree of belief , measuring this degree is particularly difficult as itvaries form person to person . this study investigated the sense of presence derived from audio and video stimuli . in order to measure a participant ' s sense of presence objectively involuntary responses from the human physiology were measured including skin conductance , respiration rate , and blood volume pulse . despite participants reporting in questionnaires they felt more presence with the audio condition , there biometric data tells a different story . heart rate and skin conductance both support a higher level of presence within the video condition . there was no noticeable change in respiration rate detected .
3d scenes are being used more then ever for creating realistic prototypes of buildings before they are created as well as in the film and computer games industry . lots of these applications require the implementation of background characters that perform the same role as extras in the tv industry . this project looks at how the behaviour of these virtual extras in 3d scenes can be created autonomously .
due to the popularisation of the computer use in the households , there is an increasing demand for natural language human - computer interactions . in this paper , we focus on the question answering process and therefore on the text comprehension task . we have implemented an information extraction system using some fairly simple natural language processing techniques . we worked using resources used for a competition some years ago . we reached the pretty good performance of answering 57 % of the questions that we had to treat to meet the requirement of this competition . alongside with the implementation of this system , we made enough testing over the different combinations of techniques used to create a database . thus , by querying the database , we highlighted some tendencies on the best techniques to use to get good performance .
dataflow algebra ( dfa ) is a form of formal specification currently being developed within the department . dfa is an attempt to integrate diagrammatic and formal methods , which has now reached a stage where it can be implemented within a case tool . a partial implementation has already taken place yielding good results , but was unable to represent the sequence of dataflows . this project aimed to further the current wincase implementation by adding sequence diagrams , which would allow output dfa to be more complete . unfortunately this project ran into difficulties with the wincase engine , severally curtailing progress .
granular synthesis of sound involves the generation and combination of thousands of short " grains " of sound . each grain is typically less than 100 ms in length with variable waveform , amplitude and envelope . the linear combination of these grains form complex granulated sound effects . this project outlines the development into a new way of performing granular synthesis using an immersive virtual reality environment . grains of sound are sprayed visually at different locations in space . the resulting granular space is then synthesised to produce a granular sound representation .
one of the goals of language processing is to build systems that can automatically answer a user ' s question . the answer should be obtained from the collection of documents available on the internet . to encourage research in this area , question answering , the last six trec competitions have simulated this task and allows researchers around the world to compare their systems . this dissertation works with the sheffield system , entered in trec - 13 , and investigates techniques for improving the system performance on definition type questions . this dissertation begins with a set of sentences judged " possibly relevant " by the sheffield system and experiments with a variety of ways to filter this list to produce the " vital nuggets ", judged to be important in a correct answer . this dissertation measures recall and precision for the system ' s response to the trec definition questions and compares the results to those of the original sheffield system .
this project outlines the continuation of development to the mistres project , a 3d graphical approach to computer music composition and performance . this project concentrates on extending the manipulation of sounds at a low level . this is achieved by implementing a fourier transform of the sound and displaying it in 3d where it can then be altered . outlined are the software development cycles involved , a survey of current literature and technology , and finally an evaluation of the system .
the genre of house music is an expanding area with new producers creating new records every week for many record labels , and eager listeners . until the creation of the world wide web , purchasing records could be a mundane and laborious task . consumers are now able to browse large lists of available records , and preview the record before it is purchased , either by listening to the record , looking at the sleeve , or simply recognising the artist , title or label . more and more people buy their records online , and this has lead to the creation of online record stores , all competing for custom . the aim of this project is to adapt relevant techniques to replace the process of browsing many different online records stores , and contain records from several different web sites in one place .
the aim of this project is to create a 3d modelling system based on the idea of 3dsculpting . this project aims to closely follow the techniques and feel of real clay so that artists or other people with modelling skills of this type should be able to use the system relatively well with little practice . the project will at its limits explore the use of sensable ' s phantom desktop haptic device and ghost sdk , so force - feedback can be incorporated in the project , thus giving the user the feel that he is actually ' touching ' the object he is editing .
[None]
the aim of this project is to create a tool for archaeologists to use that will help them to record , recreate and study excavation sites . a lot of research was carried out into the field of archaeology to identify features that would benefit them . the project will be implemented using java and eventually make extensive use of virtual reality to simplify the process as much as possible . the design stage will take advantages of certain ideas from the discovery method to help organise thoughts and concepts before they are implemented , but the implementation stage will be done using mostly xp techniques . the key feature of the software will involve showing how sites change over time by interpolating two scenes ( each at a certain date in history ) to create an animation . due to time restrictions , few features were implemented and the user interface was built with a purely functional purpose . the project bears some notable success , but a lot more work is needed before it becomes useable . unfortunately the virtual reality hardware was unavailable during the development period so the software could not be calibrated to use its features .
the rendering of fire has always been a popular , yet challenging effect to achieve in the field of computer graphics . there have been many attempts over the years to recreate the effect of flames , both in traditional animation and more recently in computer - generated scenes . examples of some of these recent successful attempts are covered , including the methods of solid turbulence textures , polygon meshes and particle systems . a simple stateless particle system is presented , utilizing a java - opengl binding and nvidia ' s gpu programming language cg , to produce a three - dimensional real - time animation of fire . further work is also suggested in the form of an algorithm for a state - preserving particle system .
the outcome of a football match is dependent on a large number of factors . theserange from ones which may be expected , such as current form of the teams or which team is playing at home , to the more obscure , such as the weather or whether the referee chooses to award a penalty in the 51st minute . we are interested in how successfully the results of football matches can be predicted by modelling a small number of these factors . the aim of this project is to develop and implement probabilistic models that will successfully predict results of matches in the football league championship to a high level of accuracy . the models developed use poisson processes , as used in queuing theory , to model the scoring and conceding rates of each team , both in home and away matches . this is combined with the maximum likelihood method and bayesian inference to make predictions about future matches . the models were implemented in java and integrated into a system which tracks predictions over a season and provides information about the success of the predictions .
the internet continues to expand at an incredible rate , both in performance and accessibility . while web browsers and other related software are becoming more powerful and accomplished , the number of people using the net is growing all the time . this can lead to problems , as users can be overwhelmed by the technology and expertise required in some situations . the aim of this project is to construct a computational system that is capable of displaying structured hierarchical diagrams within web pages , something which is not easily achievable within the limitations of html . a key criterion of the finished system will be to ensure that the interface is easy enough to use by someone with only very basic knowledge of computers .
increases in gpu hardware performance and the ability to program the vertex andfragment processes of the graphics pipeline have enabled increased amounts of realism be demonstrated in real time . droplet animation and rendering are processes which often implemented using offline techniques due to computationally expensive calculations need for ray tracing and collision detection . by harnessing the programmable nature of graphics hardware a system is implemented in real time , which utilises gpu processing power to incorporate many aspects of droplet rendering and animation .
growing interest in multimodal interface design is inspired largely by the goals of supporting more flexible , efficient and powerfully expressive means of human - computer interaction than in the past . in this study a speech user interface will be designed to query on a bbc news archive . integration of classical text retrieval techniques with modern speech recognition technologies , namely ibm viavoice , will produce the desired result . perl and java are used as the programming languages . the study is a continuation of previous dissertations but a new approach is taken . in the previous dissertations much emphasis is given towards improving the speech recognition of the system . i aim to show that creating a efficient retrieval system balances the poor speech recognition . text retrieval techniques and algorithms cover the bulk of this paper .
this project explores the idea of artistic creation by computer software . the theory of intelligent agents is discussed and a relevant subset of concepts is identified and applied to the design of such a program . the artistic inspiration for the project is wassily kandinsky , an artist born in russia in 1866 , who was a pioneer of abstract art . this report explains the analysis kandinsky ' s work , and the design and implementation of a program to emulate his style . the project has been evaluated against the overall aim and the stated program specification . the program was also assessed by usability testing , which revealed several minor shortcomings , but suggested the software was generally satisfactory to users . a small survey of non - experts shows that the images created by the final program bear a significant likeness to kandinsky ' s art .
the amount of information available to us through the internet nowadays is quite overwhelming , and the average user has never been so lost with regards of retrieving this information . more specifically , when attempting to retrieve images of people from the web , a user will almost always be bombarded with a whole world of irrelevant results . this paper attempts to coalesce the windowing approach used in the development of the semantic web for the disambiguation of information with techniques from the areas of information retrieval and information extraction in order to develop a software that solves the problem of ambiguity in the retrieval of images from the web . a number of approaches are suggested and evaluated . from these , a number of pitfalls are identified ( with possible solutions ), and yet fairly confident results are presented , where the developed system does manage to effectively disambiguate images .
during the first semester of the project i looked at how the sheffield university department of computer science uav had developed and researched methods of developing and testing the original specification uav to a point at which it may have been possible to get the uav flying . during the second semester the project changed direction with the introduction of 2 different phillips based microcontrollers and i then researched methods of testing and integrating these microcontrollers into the uav with the possibility of one of them replacing the tini . i achieved partial success with the phillips microcontrollers as i had problems getting them to communicate over i2c which would be a major part of the uavs control structure . i hope that i have left enough information for whoever develops the uav further to built on my ideas and code in order to get the uav working .
since the beginning of their widespread use , computers have been used toemulate human opponents in classic card and board games as well as computer games . this project tackles the problem of creating a computer player for the game of 5 ' s and 3 ' s dominoes . in this game , players need to make estimates as to the location of certain dominoes in order to play at an advanced level . the aim of this project is to create an expert 5 ' s and 3 ' s dominoes player that deals with this problem of uncertainty and estimation . bayesian networks are an ideal tool for use in this problem , and this report will explain the decision , design and implementation of using bayesian networks to create an expert computer dominoes player .
this report covers the research , design and early prototyping of a system designed to estimate the real - time speed of the cue ball in a game of snooker on specific shots . the main focus will be to estimate the speed of the break ( first shot in the game ), though may be extended / adapted for use with any shot . this is intended for use as a " point of interest " for commentators of the game but could be extended for use in building up statistics of players and different styles of play ( variations in shot speed between players ).
this report details the ways in which information visualization is possible for a collection of gathered information . the need for information visualization is based upon the inherent way that humans perceive a collection of information more naturally when it is given a visual representation . the report investigates one method of visualizing a collection of movie publications that have been gathered from the web .
background extreme programming ( xp ) emphasises the test - first strategy of developing software where if code passes unit tests developers gain more confidence in their software . for java code there exists a testing framework called junit , which allows software developers to cheaply and incrementally build test suites for this purpose . by using junit programmers are adhering to the rules of xp and are also helping themselves to measure their own progress by virtue of whether or not they have confidence in their tests . confidence in tests means confidence in code . jester is a test tester for junit tests and thus allows developers to confirm their confidence in their tests and consequently in their code . jester finds code that is not covered by junit tests and thus indicates either missing test cases or the redundancy of code that currently exists . the eclipse ide enables developers in any language to independently build tools that when combined together work as if they are part of a single integrated tool set . the implications of this open source ide as an aid to software engineering are infinite and thus provide an ideal platform to nurture xp practices on . the integration of xp practices within eclipse is encapsulated by the adept ( agile development environment for programming and testing ) plug - in currently in development . junit has already been seamlessly integrated with eclipse , but jester has not and many developers find it difficult and frustrating to use due to setting up its independent use . an excellent way to incorporate jester within xp practices for the benefit of confidence in code , as well as making it more attractive for xp developers to use is by integrating it within the adept plug - in and thus also eclipse . this project aims to do this in a sophisticated way by making use of eclipse ' s advanced internal ide elements and therefore taking advantage of the test testing idea of jester to the full . this integration of jester within the eclipse environment must take into account and maintain the independence of jester , as replacing older versions by newer more sophisticated versions can then be easily done . this also maintains jester ' s open source nature rather than customising it for eclipse dependence . a communication link was established throughout the course of the project with the original developer of jester ( ivan moore ). the project is dependent upon the junit testing framework within eclipse . the jester plug - in itself allows for a user to launch jester on source code contained within eclipse java projects and obtains useful results about changes it has made that show code not covered by tests . these results are placed within the eclipse tasks view . the plug - in supports many ways of launching jester , automatic jar dependency additions to projects and corrupted source code reversion to its original form if any problems occur whilst running jester . the plug in is also highly configurable via preference pages and contains many more supplementary functions . this has all been accomplished using a hybrid development methodology , which is a cross between eclipse plug in design guidelines ( a more standard water model of software engineering ) and extreme programming test - driven development .
there are various methods available in order to perform the transformation , conversion or morph of one speech signal to another . within this study several of these methods are looked at and discussed in order to gain an understanding of the process to be undertaken . it has become apparent through study in this area that methods that make use of the formant structure of speech are successful . provided for this study is a piece of software that will determine the formant parameters allowing manipulations to be carried out on a source signal . with these manipulations complete the signal will then be re - synthesized with the software provided . it has therefore been decided for this study to use formant analysis and re - synthesis . this study firstly looks at the methods with which voice morphing can be achieved before going forward to design , build and implement a piece of software to work alongside the provided software in order to achieve the goal .
over recent years , there has been a large increase in the number of motor vehicles using road systems worldwide . much of this increase is created by people travelling a relatively short distance but , due to the high numbers of people on the road at peak times of the day or after an event , they often experience long delays . many large towns or cities have extensive road networks where it is possible to arrive at the same destination by any one of a number of routes . by utilising a system that can communicate with the vehicles on the network and identify their destination , it should be possible to advise them of the best route to their destination and then try to optimise their journey time across the network . this project builds on previous work by students in the department of computer science at the university of sheffield to implement such a system . along with increasing stability and reliability of the existing system , we are looking to expand it by adding dynamic routing for vehicles and providing a communication network for the junctions . additionally , a graphical interface is required for viewing the simulations in action . in this report , we review the work done and alternative approaches to the problem , look at the technology involved and begin to analyse the system . decisions are made as to what approaches to take and basic algorithm structure . the system is then designed , implemented and tested . once built , tests are run to investigate whether or not the lane saturation level can be used as an accurate estimation factor for the delay along a lane . the results show that in certain circumstances it does produce a good dynamic routing factor , although is response to varying traffic conditions is slow .
fortran was one of the first high - level languages to be developed ; the foundation of the language was based on mathematics . matlab was then developed to be a replacement by allowing easier access to the fortran libraries that it is built upon . however there were still problems within matlab , therefore the main aim of this project is to develop a language that solves these problems . the purpose of the project was to design a language , m # xtreme , specifically to carry out matrix manipulation , which could be incorporated into the microsoft . net framework . the basis of the syntax for this language was to follow similar constructs to the ones that are used in matlab . the second aim for the project was to develop a compiler for a small subset of m # xtreme to demonstrate some of the features . the main achievement of the project has been to solve the inefficient for loops in matlab but still using a syntax that is easy to use and understand .
individual - based modelling is a popular approach to many different problems involving a number of beings interacting with each other . use of this type of model has become widespread in research , including 3d graphics , biology , ecology , artificial intelligence , traffic management , and robotics . this project seeks first to study examples of individual - based modelling of dynamic systems in a wide context , and then to build on this knowledge in the creation of a more advanced software implementation . in 1987 craig reynolds introduced a model of bird - like creatures flying around in 3d space . his intention was to create realistic - looking birdas for use in cgi effects form films . the ' boids ' that he created were programmed with a few very simple rules of behavior , but as a group they exhibited more advanced , emergent ' flocking ' behavior . from then on , many people were interested in the subject , and there are numerous examples of different problems solved using similar techniques . each example of a boids - like model is programmed to demonstrate certain behaviors . they are very specific , and lack the ability to add new features without adding to the source code ( if available ). simulation environments are available that simplify the process of creating simulations , but they are very general so as to be able to create a wide variety of different simulations , so programming is required . this project takes inspration from previous work , to create a universal boid simulation tool , capable of hosting many different types of boid in the same environment . boids can be created and modified by the user , and then saved and loaded to / from files . numerical measures of flocking performance are available , in order to evaluate specific rules and parameters , and find optimal values . the simulation tool is capable of emulating behaviors from other boids - like examples , and providing the facility to experiment with new behavioral rules .
the project proposal outlined in this report is for research into information extraction from sms messages in the nightclub promotional message domain . an investigation into a system designed to deliver the extracted information to users will also be proposed . sms is a technology that grows ever popular despite the limitation of tiny mobile phone keypads and a character limit of 160 characters . sms is particularly interesting in the context of information extraction , as sms messages tend to use abbreviations and syntactical reductions . this project hopes to investigate the effect that this will have on the information extraction process .
this webundergrad project from dr . marian gheorghe aims for the design and build of a web - based system to assist undergraduate student admissions . the system records the number of applications accepted / rejected on each date as well as appropriate detail for pending applications . based on these data , various statistics can be generated . other than that , information regarding courses , ucas opendays and users is also captured by this system to enhance flexibility . after initial investigation , the development process was decided to be carried out applying xp agile approach . the system was designed and implemented using mvc paradigm with php and mysql language . the final release of the product runs on the department ecommerce server ( ext . dcs . shef . ac . uk /~ u0029 ) and achieves all the mandatory requirements from user .
since the web is the largest repository in the world , it is difficult for end user to search , organize , and classify the information of interest on the web . the semantic web , which is the second generation of the web , aims to solve this problem by making the content understandable by computers . information visualization exploits the human ' s congenital visual perception capabilities in order to provide a way to make the information more accessible . the project outlined in this report concerns designing a system using the semantic web technology and information visualization technology to build up a semantic search engine to extract large bases of data of london restaurant . relevant technologies are described and discussed in chapters , followed by the requirement and analysis of this system . afterwards , the design and implementation are presented in two different chapters . and then the evaluation of the system is discussed . finally , a conclusion that sums up this whole project is drawn at the end .
word sense disambiguation is the process of automatically identifying the intended meaning of such a word in context . various techniques have been proposed to solve this problem . one firstly introduced by lesk ( 1986 ) is known as dictionary definition overlap , which seeks the most shares of the neighbouring words in context . this project discusses the variations of lesk algorithm , especially simulated annealing method ( cowie et al . 1992 ). rather than using traditional dictionaries , we introduce the lexical database wordnet , which not only stores words and definitions as the normal dictionaries , but also groups the related words together as individual synonym sets ( or synsets ). finally , we will evaluate our algorithm by using the english lexical sample data from the semcor word sense disambiguation exercise together with other methods .
over recent years more and more work has been done in developing both virtual and synthetic characters that are able to interact with both users and environment in a realistic manner . many emotional models have been designed and incorporated into these characters to allow these interactions to take place , ranging from both biological and functional models , however recent work done by romano , sheppard , hall , miller , ma ( 2005 ) represents an innovative model that is capable of creating social relationships with the characters it interacts with in a virtual environment . this report discusses the correction of errors within this proposed model and the testing of these changes . it also addresses the incorporation of this model into a synthetic robot head known as cim ( creative interactive media ) developed in order to express emotion via facial expressions . using this model and robot this paper will discuss the creation of a frame work to control the robot , the creation universally recognizable expressions and their tests upon a group of subjects along with an analysis of this test .
as technology is advancing , it is trying to model computer systems to be more and more similar to human beings . in robotics robots are made to move like humans ; in a . i . computers " think " like humans . it only seems logical that they should also talk and understand speech like hmans . in the 1930 ' s a princeton researcher created the first speech synthesizer and sparked a whole sequence of events that led to current hidden markov model - based speech recognising systems like the famous ibm viavoice . but the use of voice recognition systems has also expanded to other areas - systems control , teaching , medicine ; and it is on these last two that this report will focus on . the purpose of this hmm based speech recogniser to provide feedback on pronunciation of isolated words , for users who want to practice their english pronunciation . automatic speech recogniser creation will be discussed in detail along with all the relevant technologies and techniques - the final challenge will be to produce an interface to provide karaoke - style feedback .
there can be little doubt that the human eye is immensely important in human interaction . they are the natural focus point when addressing a person , and are extremely important in recognition and conveying emotion . because of the attention that eyes receive , and their inherent detail , it is immensely helpful when rendering computer - generated humans if the eyes appear convincing . the aim of this project is to develop a model of the human eyes and surrounding tissue which is realistic in appearance and can be rendered in real - time , along with a parameterized interface which allows the model to be animated sufficiently to display a full range of human expression . the project will use an existing facial animation program as a starting point . due to time constraints much of the implementation of the project was not possible , so much of this report looks at the project from a more speculative viewpoint . the system is implemented with limited functionality .
agent - based modelling is a very powerful tool - it is being employed by nasa intheir satellite program in space exploration . the aim of this project is to apply agentbased modelling to the problem of disease - spread , thus creating a computer system to enable biologists to investigate the factors that affect disease spread in greater detail and with greater ease than current mathematical methods of investigation . the project will concentrate on the use of a framework for agent - based modelling , created at the university of sheffield and will centre on the spread of disease between honeybees .
this paper investigates the past and present techniques of translation , ranging from human translation ( ht ) to machine translation ( mt ), before moving on to cover the various methods used to evaluate these processes including both human and machine evaluation . the analysis of these techniques is then used to begin formally specifying a research project into the evaluation of current online mt systems , as there has been a large increase in the usage of these systems in recent times . although these systems are aimed at giving a ' general ' overview of a piece of text they should still be of some quality , however the current systems have been judged to offer a lack of quality due to certain errors within them . this research aims to evaluate the current leaders in online mt by the use of specific tools and evaluation measures , in an attempt to evaluate the current standing of online mt today .
blinking is a subconscious action with its main purpose to keep the surface of our eyes moist but the average blink rate of an individual has been linked to many different fields of research such as estimating levels of fatigue , stress and deception . this project created a semi - autonomous blink detection system that is robust and user independent . it is able to detect the user ' s blinks within the time domain with an area under the roc curve of up to 93 %. this project investigates various techniques for blink detection . among these techniques will be algorithms for eye detection such as using a skin tone threshold , eye tracking using a bordered variance algorithm and eye analysis using differenced eye image frames . this project has uncovered further areas of research such as blink duration detection and improved multi - cue blink detection algorithms .
open office is becoming increasingly popular , and as such the need for compatibility between microsoft and open office applications is becoming ever more apparent . however , whilst open office allows microsoft documents to be opened with their products , macros are not currently supported . the purpose of this project is to create a tool that allows the conversion of microsoft excel macros into a format that is supported by open office calc . this project has not managed to create a complete solution that is able to convert macros from open office calc to excel . this project has however succeeded in creating a basic framework , converting some basic functions and allowing for further functions to be easily added .
real world processes often result in a lot of unstructured data , data that do notcorrectly meet those constrains of well known relational databases . unfortunately , these priceless storehouses of unstructured , or semi structured data , inevitably slow down business processes , or might even be considered useless in terms of time needed to reference or access those data . minutes held by various committee meetings fall within the category of unstructured data , inheriting just the same problems . this project aims to provide the means of overcoming this problem , within the committee minutes domain , by introducing xml databases and examining related technologies . further , by addressing various human computer interaction issues , all the technicalities are hidden from the end user , who is able to take full advantage of the features available through xml databases and the related technologies , with minimal computer experience . the substantial examination of the available technologies , followed by the analysis of the user requirements , resulted into an application that can be successfully and productively used for storing committee minutes .
transformation - based learning ( tbl ) is a rule - based machine learning technique suitable for application to a range of classification tasks . the approach involves the generation of a sequence of context - sensitive " transformation rules " which may be used to correct errors in the initial tagging of a corpus to improve the accuracy of the classification . application of the technique typically requires the production of a custom implementation adapted for the specific task . the objective of this project is to develop a java implementation of a generic tbl tool that will support research into the use of tbl for various tasks in nlp and other fields . the tool must enable application of the technique to any task without requiring significant adaptation . the tool must also provide the flexibility to modify and plug in different versions of its functional components to enable further development . the primary aim of this project is not research of transformation - based learning , but rather the development of a tool that will support such research . this project begins with an investigation of the requirements of various tasks to which tbl has been applied , and an examination of the features provided by existing generic tbl tools , which the intention of identifying the potential requirements of a generic tbl tool . the tool is then specified , design , implemented and tested . finally , the results of this project , primarily the extensible transformation - based learning ( xtbl ) tool , are presented with critical discussion and proposals for further development .
the process of detecting repeating segments in audio is a fundamental part of all audio analysis in both nature and computers . although , almost all repetitions found in natural audio are not identical , with varying substructures and spectral characteristics , and so a method with adequate tolerance to these imperfections is required . algorithms have been presented in all fields of computer hearing with a specialization for handling particular types of audio such as music and human speech . the aim of this project is to create a flexible analysis tool capable of extracting the structure within audio samples of varying levels of complexity . the application will have the front - end functionality to modify the behaviour of existing algorithms to help us explore the performance in correctly extracting repeating segments .
many organisations use paper based forms despite their inefficiencies when compared to computer - based forms . this may be due to the expense and time involved producing computerised versions . whilst software exists to design computer based forms , there has typically been a need for some code to be written or further technical knowledge required . this project attempts to address these problems by developing a system whereby users with limited technical knowledge may produce forms with advanced features such as validation of input and repeating fields . to increase the end - user audience of the forms , the system will be a web - based application that may be used to display forms on an intranet or on the internet . this report investigates the requirements of the system , and proposes a number of technological solutions to various aspects of the system . these are then investigated and a full system involving a native xml database is developed based upon the conclusions made .
real time realistic animation of gaseous phenomena is an important and challenging problem in computer graphics . the computational expense associated with accurately modelling the underlying physical processes is prohibitive to real time interactive applications and has prompted researchers to explore many different techniques . this project differs from previous attempts in the area by seeking to dynamically simulate the effects of rapidly moving projectiles passing through the volume , leaving visible vortices in their wake . the new wave of graphics hardware with powerful programmable gpus paves the way for many such effects in computer games and virtual environments . this paper provides a brief analysis of the different techniques available , before presenting a gpu implementation of a state - preserving particle system which successfully fulfils the main project requirements . the finished system produces visually plausible representations of large scale smoke effects capable of interacting with static and dynamic objects in the environment , and is fast enough to run comfortably in a game environment on high - end consumer graphics cards .
nowadays , language researchers often process on large bodies of text as raw data . this is what we call a corpus . following the development of information technology , the world wide web has become the indispensable source for the researchers as well as the source for daily useful information in our lives . powerful search engines such as google and yahoo ! have played important roles as a yellow page on the internet . however , the retrieved texts obtained by the conventional search through these search engines are largely unordered , consisting of variable qualities , mixed with irrelevant information and redundant documents , and as a result , the desire for an off - line search engine which retrieves only the relevant information from the internet is growing . the aim of this project is to build a corpus generator that can automatically generate a large corpus using available web search technology , such as yahoo ! api and google api . it will accept a set of the web pages or text files as the input , summarize the contents of these input files , generate an expended query , and finally , retrieve further relevant information for the user . the corpus generator achieves this by implementing various searching optimization features : generating expanded query from the initial documents provided ( technologies involved in this part covers statistical term weighting , pseudo relevant feedback , and morphological analysis of each word ), further retrieval using the generated query built upon the google api and yahoo ! api , and applying document similarity measure and filtering algorithm to the retrieved information before return the final search result , the corpus , to the user . other features for search optimization implemented in this project include html parser ( a technology to remove java script and html tags ), mysql database , embedded ie browser , and a number of other search optimization options that have been described in this report .
copy the text here . you can use * simple * html , e . g . this is the first paragraph of my great work this is the next paragraph . this bit is in italic
this aim of this project is to develop and explore models of the neurogenic network hoping to determine under what conditions the neurogenic network generates different behaviours . we would develop models with the use of equation solvers in matlab . basically , we would be using a process called lateral inhibition , which is a process which cells go through to determine their fate as to whether they would be a neural cell or an epithelial cell . we would then introduce time delays to this process due to the fact that delays have a lot of impact on the way the neurogenic network works . there are 4 stages in this project which are i . process model ii . 2 variable ( dl - n ) model iii . oscillators iv . the full neurogenic model firstly we would be looking at the process model and supporting every information we get out of it with a graph and also take a look at how lateral inhibition does occur between the cells . from there on we would then move on to look at the 2 variable cells which is the delta and notch model and take a look at the reactions of both delta and notch and also how lateral inhibition occurs in these 2 cells and also how much time is taken before it occurs . this takes me to stage 3 of the project , which basically are focused on oscillators which are the single oscillator cell and the coupled oscillator cell and also the bi - stable loop . we would be looking at each of these cells and taking a good insight as to how they work and how lateral inhibition does occur between them . finally we would then add all the information we have on the first 3 stages and then bring it together to get the neurogenic network going . we would be taking these steps one at a time and by the end we would be hoping to achieve our aim of working of this project .
there is a huge amount of human opinion on the web , on message boards , blogs andpersonal web sites . it would be useful if we could have a computer automatically sort through all these opinions and report back with a general overview . this project investigates if text can be accurately classified based upon the sentiment expressed within it . movie reviews are used as a test bed for examining the classification accuracy of a variety of algorithms and classification styles . the results are promising , being able to predict a star rating correctly almost 60 % of the time in some circumstances .
" i believe that you can reach the point where there is no difference between developing the habit of pretending to believe and developing the habit of believing ..." umberto eco , " foucault ' s pendulum ( london 1989 )" generally , people perceive abstract art as a contemporary phenomenon , which breaks the idea that paintings must depict reality . it is considered as an innovative art with remarkable development in the 20th century , and leads people to appreciate artworks in a new way . the intention of this dissertation is to develop a piece of autonomous software which can generate artistic graphical images in malevich ' s suprematist style . by transacting interactions from a user , the software automatically simulate and construct graphical images according to some artistic evaluations based on a series of strategies . these strategies depend on restrictive built - in rules that extract painting approaches from existing suprematist opuses . further more , users can employ the image processing adaptor to edit or extend generated images according to their inspiration and taste .
the production of transcripts has often been thought of as an instantaneous process . however , recent research has shown that within the process of transcription there is a relatively lengthy time delay . this has led to a recent surge in the amount of research papers trying to model this newfound knowledge . furthermore , it is debatable as to whether any of these attempts has been fully successful , as the introduction of stochasticity into the process has provided a difficult obstacle to overcome . these models will be analysed in this project and indications as to the problems with them and ways in which they may be fixed will be discussed . the ultimate aim is to produce a model that reduces the level of stochasticity in the system by varying the time - delays to correspond with those that occur in the biological process itself .
spectral clustering and multidimensional scaling are mathematical techniques that have been used primarily in thesocial sciences to explain similarities or dissimilarities between objects . pagerank is the technique used by today ' s most popular search engine , google . it this report we will try to see if these techniques or even a combination of other eigendecomposition techniques could be implemented to check whether comparisons between the department of computer science web pages could be made . this would involve taking all the text from each web page and comparing text between web pages . then the text would be changed to numbers in order to be processed with matlab . also , the pagerank algorithm could be part of the algorithm . methods that could be of use could be stemming and the use of bigrams .
football is a ball game played between two teams , each attempting to win by scoring more goals than their opponent . nowadays people enjoy watching football matches not only thought it as a popular sport game but more regard it as a kind of culture and spirit . the aim the project is to utilize the techniques from computer visions to detect and retrieve the track of the football player from computer videos . in general , the whole procession can be divided into five main parts : recourse analysis , video processing , image processing , data collection and modeling rebuild . the whole system was regarded as a multi - media information retrieve system rather than a pure image processing system . thus , it considered more issues during system design including a simple network architecture that can provide a concurrent processing . however , the main system is still concerned on image analysis and processing by image dividing techniques that developed in this project .
the primary aim behind this project is to make the real time flight stimulator ( rtfs ) situated in the department of automatic control and system engineering ( acse ) at the university of sheffield more accessible to students , staff and researchers across campus to share the resource efficiently and effectively via networking . this resource is currently accessible by a few selected people , thus it is not a major support to the academics . this project would enable the students to access and apply their algorithm and program from the comfort of any computer on the university campus , rather than the actual room where simulator is currently in operation . this accessibility would also need to be accompanied by a migration of this simulator to java as this would enable more students to experiment on the simulator as java is the main language of teaching in most of the departments in the university . the primary task for this project is to develop java classes to wrap existing classes which have been developed in c / c ++ and modula - 2 with java to enable students to work with & extend the simulator in the language taught at university and hence eradicating the need to learn any other supporting languages . secondly , the main focus of this project is to project is to provide functionality to enable remote access to the simulator and hence eradicating the need for students to visit the simulator lab . further more , this project will investigate into enabling multiple users to concurrently use the simulator and hence allow more efficient use of the simulator as compared to the current simulator allowing only a single user . this would enable academics to run lab classes for practical and marking purposes as the students can complete the various assignments or the function while in a lab using the simulator concurrently over the university network or over the internet , depending on the connection speed of the remote computer . various simulators have also been reviewed to decide features and functionality required by the simulator . these features would eventually be incorporated into this system to provide full functionality of the simulator from remote machines over a network .
at a time when access is available to innumerable hours of video data , it is extremely important to end users to be able to quickly scan through large quantities of video data in a very short time . video summarisation techniques endeavour to extract the most important and relevant details into a compressed form so as to provide quick and easy navigation . this project delivers a tool which , when presented with an episode of a ' talk - show ' television programme , outputs a series of still images that ideally depict the host and all guests who feature in the episode , to a recognisable standard . the system performs significantly better than summarisation approaches which use temporally evenly spaced images , and whilst optimal proficiency is not attained , the system may be used for practical applications . the findings of this project are additionally very beneficial to guide future work in the field of automatic video summarisation .
the purpose of this dissertation is to create a system that will allow a user of little computer ability to create , edit and use a database system for any area of their business ; whether it be accounting , sales or another . the dissertation will take an in depth look at the different areas of computer science that are needed to produce such a system . several different methods and ideas that could be used or included are analysed and the best ones chosen with good reasons . it describes a system that allows through the use of a designer program the creation of a database and analyses the different design decisions and options that are available . the implementation of the project is then discussed and finally the whole project is evaluated . it also includes plenty of future expansion in several different directions .
combining haskell and java has been a topic for investigation for some time , whether it be by converting byte codes or source code . in this report we argue that the conversion of source codes enable a java programmer to make use of a functional language where it can express either data structures or algorithms more aptly . we have achieved a successful translation of a subset of haskell by making extensive use of the java class system ; overcoming difficulties with java ' s strict nature by delaying computation until necessary . whilst much work has already been done , given the opportunity there are many more haskell constructs which are un - modelled such as anonymous functions and lambda abstraction . the focus of attention for future continuation of this project should be on modelling these constructs .
background mini uavs ( unmanned air vehicles ) are small aircraft that are flown either by a remote operator , or by an automated system . they are used mainly for reconnaissance and surveillance both commercially and by the military . mini uavs are cheaper to build and use than conventional aircraft , and can be flown in areas that are too dangerous to fly with manned aircraft . mini uavs are very difficult to fly manually , requiring constant correction of the controls in order to keep them in stable flight . also , the operator must be able to see the uav in order to fly it , which limits where it can be flown . this has resulted in increased interest in autonomous mini uavs . these are mini uavs that are flown by an automated system rather than by a person . a number of autonomous uav systems have been developed previously and there are many competitions devoted to autonomous flight , such as the international mav competition and the international universities mini uav competition . project aims the main aim of the project is to investigate how to create a control system that will allow a uav to fly autonomously . only the development environment required will be developed this year . the objectives for this year include choosing the best hardware / software , finding out how to use them and creating a development environment from them . the first objective requires that for each area of hardware and software that is needed to create the initial development environment , the available hardware or software for that area is searched to find that which is most suitable to the project . the second objective will require the reading through of documentation and example programs and writing of test programs to find out how the hardware and software works . documentation that will be looked through includes readme files , manuals , books , programming guides and tutorials . the third objective involves writing code that allows easy access to all of the areas required to produce a uav system , and integrating the hardware and software in such a way that they all work well together . main achievements the main achievements of the project were the creation of a development environment for the system . this consists of the hardware and software chosen to compile upload and run programs on a microcontroller . these have been configured to work together and example code and documentation has been created that explains there use . there are also the results of research into the development of uavs and of programming with microcontrollers .
many night clubs and radio stations employ disc jockeys ( djs ) to play music to their audiences . djs will often mix songs together as smoothly as possible , with the aim of creating a continuous , seamless mix of music , with no gaps between each song . this project aimed to develop a software system that is capable of mixing songs to a standard comparable with that of a competent human dj , by emulating the tools and processes that djs employ when mixing . through evaluation , the implemented system was deemed capable of producing mixes between songs comparable with those produced by human djs , in some instances . this report describes the background research , design , implementation and evaluation of the system , whilst also discussing possible future work .
the problem of traffic congestion is an increasingly serious one . it is also a problem where there aren ' t any concrete methods on how to solve it . it is not possible to simply build more roads as this is expensive and space is often at a premium in the places where congestion arises . limiting the number of vehicles on the roads can work but at present the only method of doing this is by congestion charging which proves unpopular with the public . this project carries on from work done in previous years . first tools which will aid the development of a new dynamic routing algorithm , including a traffic network generator and improvements are created . then a new routing algorithm is created and tested . the routing algorithm created did not show improvement over a static routing method , however the extremely slow runtime of simulations of large traffic networks caused the focus of the project move onto distributing the system over a cluster of computers in order to improve the runtime of a simulation . at the end of this project a good base of both knowledge and program code has been built up allowing future researchers to carry on with the work and produce a working distributed system and carry on with the problem of dynamic routing .;
many people use mobile devices for retrieval of information from a variety of sources ( most commonly the internet ). display space on small devices such as mobile phones is often limited , and text may ' run off ' the screen easily . also , when sending sms messages , consumers generally go over the set sms size limit . it is clear that some form of automatic sentence compaction would be useful for mobile device users . the system developed in this project uses various text compaction strategies for usage in the situation described above . the methods implemented include reducing sentences via redundant word dependencies , substitution for abbreviations , and word - internal vowel removal . a user - based acceptance testing strategy was performed , the results of which aided in the choice of most useful compaction level .
in modern computer games , realism in the environments is proving a major factor in the quality of the game . as computers become more powerful , these environments can be created in immense detail . recently , realistic interaction with objects in the environment has proved to be very useful for realism and for the game play experience . one area of this interaction is the destruction of objects in the game world . this area has not been properly explored in computer games and truly destructible environments have never been created . the aim of this project is to create an algorithm for the real time destruction of three dimensional objects . this algorithm will be able to be applied to any 3d mesh . a fracturing algorithm will be employed to realistically crack the object upon impact determined by a weakness alpha texture . the visual results of this system are good but not without fault . the performance of the system is extremely encouraging and with a limit on the number of polygons to be cracked the technique can be considered to run in real - time .
the construction of sentences with words is restricted by robust and powerful mechanisms handling the structures of natural languages . the process of identifying syntactix structures of a sentence , according to some grammar , is known as ' parsing '. however , ambiguity in natural language processing gives rise to a combinatorial explosion of possible interpretations . abney proposed to simplify parsing approach by starting with nding correlated unit of adjacent word called ' chunks '. this approach can be combined with robust parsing in order to provide powerful mechanisms for identifying syntactic structures on large collections of unrestricted documents . it seems that grammar derives from the use and not the reverse , this is why rather than to formalize manually the grammatical rules in a programming language , machine learning algorithms induce these rules which are able to provide the analysis of a text already annotated in this project the transformation - based learning methods and theory refinement are examined in order to design , build and evaluate a robust parser working with the chunk approach .
facial animation is a developing field in computer science and several approaches have been used to achieve this goal . machine learning it is one of the possible techniques to achieve it . in this paper we are going to use a novel algorithm , the gaussian process latent variable model , to achieve the animation of a simple cartoon face . the paper will start from the process of building a dataset of training data acquired manually for our purpose and then it will go through the process of machine learning used on this training data and it will show how the computer created the missing data for the animation . the main efforts have been spent on animating the face , resulting in a very simple interface for the program .
this project asks for the development of a haskell developers toolkit , we intended creating a programming environment with features a tracing utility and a source code editor . we managed to create a part of a tracing utility . in this document , we start by investigating how a trace of a functional language should look like and by presenting few other haskell tracing utilities . we continue by showing the design of our tracing utility which consists of two haskell modules . then we present its implementation and finally testing to reveal any faults .
this project is in the field of computational biology , by using the computer simulation model to display the biological systems ' spatial and temporal aspects in detail . the aim for this project is develop a simulation of a vital part of the immune system by using xmachine framework and tools such as xparser and xml . by converting the exist models in matlab code into xml , and then use an xparser parse it to a runnable c source coded programme . three models are involved in this project : chemical interaction model , nf - kb signalling pathway model and nf - kb & map kinase signalling combined model . the first two models have exist matlab model to be converted , but the last model is needed to do some research and add a new pathway into nf - kb .
in the recent year people have paid more attention in the generation of computer games as a result they are getting more and more realistic . in the last 5 years the power of graphic cards used in pc has grown exponentially , and games are increasingly more realistic . while playing games people make decision as if they were in the real world , also games can teach people how to act in the real world . such connection between the simulated and the real world can be exploited as a method for training people . the aim of the project is to discover how people learn and gain abilities while playing games , and how to use different games to educate people with the best result . there are 2 sections of this report , the section 1 is based on 2 experiments on tv and pc games with 30 volunteers , and the section 2 is holding 2 experiments on mobile phone games with 60 volunteers .
this paper gives a detailed description of the current process of the project of large scale named entity recognition . people search for information on the web but usually it is difficult to exactly locate the information they need because entities with the same name appear . those names serves as an identifier of the entities , however normally they are not able to define the entities uniquely . the aim of the project is to set up an algorithm suitable to classify the named entities in large scale and build uniform resource identifier ( uri ) for the entities instead of their names . presently the work is focusing on documentation classification . some literal material of uri is also read so far .
with the democratisation of the internet , one of its by - products the world wide web has become widely available . the www is commonly used everyday to search for information but also to make information available online . nowadays , practically anyone equipped with a keyboard and a computer can add to this existing global network of information . from this particular characteristic emerges a problem for search engines and www users . a great deal of information is available but all of it restricted by the language barrier . for example , an english speaking user might want to query a search engine to find a price list of hotels in madrid . if the query is formulated in english , the pages returned will be restricted to english consequently omitting hundreds of pages in spanish that could have been of interest . to put this problem in perspective , consider the fact that ethnologists have recorded approximately 6800 different languages many of which are written . the field of cross language information retrieval looks to research and implement ways of addressing this problem . clir systems usually rely on bilingual dictionaries to translate user queries and reformulate the query into the required languages . one problem encountered by this type of system architecture is the occurrence of proper names in user queries . since proper names do not usually appear in bilingual dictionaries , translation systems can only resubmit the query either with an approximate translation of the word or without translating the name at all which can often lead to less than satisfactory results . this project looks at the steps necessary to implement a translation lexicon and focuses mainly on the generation of proper name lists in english and in french . as this project , is worth 20 credits it has a narrower scope than typical 3rd year project usually worth 60 credits . this reduced scope allows a very specific area of clir to be explored extensively .
sms textual practices on mobile telecoms devices and internet messaging clients are forms of mainstream communication that overcome the limitations of the message service provided , as well as the screen display size imposed by the devices used . this is done without significant loss of information within the messages being conveyed between users . this report describes a project that develops an automated system that compacts a text source to a form that can easily fit the screen of a small telecoms device . this is established using technologies from current research into the subject , together with research into the sms language
the time that a computer operated in isolation seems to have passed irrevocably . the internet is one of the basic motives behind the purchase of a home computer while few companies do not install and use at least a local computer network . the ease of information access and exchange has revolutionised the way people and businesses work . in the beginning it was information that was shared between computers and made available through a network as if it resided locally . software is now moving in the same direction . java and applets may have only contributed their small share and perhaps shown the direction . recent trends also indicate a move away from home pcs and the advent of portable consumer devices that will have equal privileges . in this project an attempt is made to capture some of these trends and show how both information and software can be distributed dynamically over a network , received and utilised without restrictions by different devices . the project builds on the idea of a system that deals with student coursework in a department . it goes through various stages . it starts by capturing the requirements of the system and designing it . following that , various technologies are researched , with java being in the spotlight , in order to achieve the best solution in providing services that will give students the ability to submit coursework electronically and deal with a number of issues related to it . the aim is to provide such services that can , as much as possible , be used without location or device restrictions .
this dissertation project has been proposed by dr martin evison ( department of forensic pathology ) of the medico - legal centre ( sheffield university ) [ 41 ]. dr evison specialises in forensic three - dimensional facial reconstruction . this involves recreating a countenance , the facial characteristics , of an individual using the skull . the department of forensic pathology possesses a specialised piece of hardware called a ct ( computed tomography ) scanner . this can be used to scan the surface of a skull and output three - dimensional coordinate information in a binary format .
this project describes a method for detecting sentences of text by one author inserted into a text by a different author , as the first stage of a plagiarism detector . the problem of plagiarism detection is closely related to that of authorship attribution , and this work investigates whether it is possible to characterize an authors style from a given document and uses that characterization to identify of the text that do not fit the style . an authors ' style ' is measured by counting the occurrence of particular stylometric features in the text , and this paper investigates the hypothesis that the inserted sentences can be identified by analysis of these features , and describes how this method can be used in the creation of a plagiarism detector . test data sets were created from project gutenberg texts and a variety of stylometric features were counted . selection of the feature that best fits the style of the document is achieved by cusum analysis , and then a statistical model of that feature is built using linear regression . finally sentences are classified as plagiarised or not depending upon how well they fit the model . two versions of the software were produced , one which considers a document in isolation , and a second that trains on a body of text by known to be by the main author of the test set . the results are measured against a random classifier , which selects the same number of sentences as the regression classifier at random , and the detector performed consistently better than the random classifier . related topics : stylometry , authorship attribution , text classification
this paper is experimental in nature and focuses on the strengths and weaknesses of a recently proposed boosting algorithm called adaboost . starting with a literary review of boosting , we provide an in depth history of the algorithms that lead to the discovery of adaboost and comment on recent experimentation in this area . boosting is a general method for improving the accuracy of a given learning algorithm and when used with neural nets , adaboost creates a set of nets that are each trained on a different sample from the training set . the combination of this set of nets may then offer better performance than any single net trained on all of the training data . the later half of the paper looks at what factors affect the performance of the algorithm when used with neural networks and radial basis function networks and tries to answer the following questions : ( 1 ) is adaboost able to produce good classifiers when using ann ' s or rbf ' s as base learners ? ( 2 ) does altering the number of training epochs affect the efficiency of the classifier when using ann ' s or rbf ' s as base learners ? ( 3 ) does altering the number of hidden units have any effect ? ( 4 ) how is adaboost affected by the presence of noise when using ann ' s or rbf ' s as base learners and ( 5 ) what causes the observed effects ? our finding support the theory that adaboost is a good classifier for low noise cases but suffers from overfitting in the presence of noise . specifically , adaboost can be viewed as a constraint gradient descent in an error function with respect to the margin ,
the aim is to construct a case tool for the forthcoming version 2 of the discovery method that follows an event - driven approach to derive the passive entities necessary for data storage and the manager - entities for handling the coordination of messages . the approach is based on ' event - driven - design ' ( simons and snoeck ), where the events are pulled from a state domain until all objects are linked in a compositional hierarchy and the chain of responsibility is established . the case tool should accept a task model and narratives as input ; from this is should derive an object - event table and build the composition graph , by identifying subsets of objects that are involved in the same set of events . this thesis takes from a thesis currently being developed by david carrington ( titled castoff ). his thesis works on the initial development of task structure and flow , providing the groundwork for this thesis , which takes the output of carrington ' s project and manipulates this to produce the state model and associated object - event table and existence - dependency graph . though not complete , references will be made to carrington ' s work and its association with this project throughout .
in natural language many words have multiple meanings , or senses . this project will attempt to automatically assign the correct sense to a word in a given context . this is not an easy task and there are many complications to be considered before word sense disambiguation ( wsd ) can be carried out successfully the purpose of this project is to investigate word sense disambiguation , and particularly a statistical method to carry out such disambiguation . a theoretical background is given on the subject of wsd examining the difficulties and successes faced by previous nlp scholars , and a history of the various approaches that have been used a recent approach to this task was by david yarowsky , who attempted to build statistical models of the likely category or categories of a word in roget ' s thesaurus using information about the sorts of words that occur around the given word in a large corpus . in order to investigate how wsd can be carried out , and how successfully , this project will reimplement his algorithm using the british national corpus ( bnc ) to provide training data . this will include the design implementation and testing of algorithms for building the statistical models and for applying them to new words . the results from this process can then be compared with yarowsky ' s original results , and results claimed by other wsd methodologies . unlike yarowsky , verbs will be used in the wsd process resources used will include electronic versions of roget ' s thesaurus and the bnc , and the perl programming language . related topics :
the purpose of this project was to implement an automatic text summariser . the system was to be capable of reading in an ascii text format document and to produce a sentence extract summary that was representative of the input source text . the summary length was to be variable and the sentences chosen for it were to be the n most valuable sentences in the input document . several different algorithms for sentence selection were to be implemented allowing the thorough analysis and evaluation of the different summaries produced . recall , precision , f - measures and content similarity measures were used to assess the output produced by the summariser and to justify the result . evaluation was drawn from the comparison of the automatically generated summaries and manually annotated ones . it was found that all of the scoring algorithms were valid ways for selecting sentences for an extract . all scored significantly better than the control method of randomly selected sentences out of the text .
how do professional photographers produce the mood and feel that they capture in their photographs ? to what degree do darkroom techniques help bring out the aesthetics sought for in the subject matter of the shot ? this project is aimed at creating a teaching tool to simulate the techniques and equipment found in the darkroom , and presenting it on - line to the general public . it is hoped that this tool can give amateur photographers a taste for the variety of tricks that can be used to improve their prints . the project considers a number of design conventions taken from various software design disciplines , and attempts to integrate their conventions into the development process of this application . these are assessed as to their use in developing an application of this scale . a user of this application should afterwards be more familiar with the concept of producing a print from a negative , and should they find themselves in the darkroom for real , they should have a much better idea of how to start producing fine prints .
since the publication of rumelhart and mcclelland ' s seminal work on neural network theory , in 1986 , the field has grown and grown as people come to realise the potential that neural networks have for dealing with complex pattern recognition problems . the ability to model highly non - linear , non - parameterised data in ways previously impossible using more traditional , mathematical methods , means that neural nets are being used in applications that though previously to be impossible to model computationally . the stock market is the most glamorous and , at the same time , the least understood of the financial markets , and for years and years people have tried to forecast the future behaviour of the market , often with disastrous consequences . the stock market is so complex in its exact function that it is considered by many to be more or less random , although in recent years people have been identifying non - linear patterns within the activity of the market . actually modelling the stock markets therefore , seems like the sort of problem where a neural net may well succeed where others have failed . various kinds of network are used to try and achieve this , and the results provide useful insight into the abilities of networks when dealing with this sort of problem .
in 1936 , alan m . turing published the first study of an abstract machine model for computation , and proposed the class of automata that is now known as the turing machine . this work went on to become the foundation from which the first computers were built . in 2000 , chris howard created an application called turing 2000 [ how00 ], as part of his 3 rd year undergraduate project . this package was designed to benefit students , allowing them to create standard transition diagrams and composite diagrams ; also including auto - encoding facilities for universal turing machines , and multiple tapes . an application has been created called amsoft , which is an extension of turing 2000 and adds facilities for finite automata , making it possible to create , edit and simulate transition diagram representations of finite automata . it also looked and tried to provide facilities for pushdown automata , though this was unsuccessful . it is hoped that this work will be carried on in future projects , and eventually become part of an application that covers all aspects of automata theory taught on the com 234 : theory of machines and languages course , the university of sheffield [ gai98 ].
for obvious reasons it is desirable that a user is able to speak to their computer and the computer is able to understand and interpret the speech . there are currently several speech recognition software packages commercially available that claim to allow just that . while they are all reasonably successful in a quiet environment they all develop serious problems if there is any background noise . if the noise could be removed before speech recogniser attempted recognition it ' s performance would be dramatically improved . there are other uses for a method sound separation as well . if a recording is made of a musical performance but someone coughs close to the microphone during the performance then removing the sound of coughing without damaging the sound of the music would be desirable . sound separation could also be used on modem lines , capacity would be increased if crackle were removed without changing the signal being transmitted .
this project extended an existing system that converted latex source files into the rich text format . additions and improvements were made in three increments , improving the operation of the system at each stage . the commands implemented during this project include displayed material , footnotes and an improved representation of tables and arrays . numbered equations were catered for along with cross - references and the bibliography environment . finally , as technique was developed to allow complex mathematical formulae to be included in the rtf document . as a result of this project , the general robustness of the system has been improved , and it is now much more capable of converting any supplied latex source file .
this project is concerned with developing a richer interface to an ir system with a view to enabling relevance feedback . the premise for this is the increasing difficulty to effectively navigate and filter information as the amount of data on the internet and elsewhere becomes larger and larger . the report describes an attempt to produce an application which provides an interface to a ir engine that has been used to index over 2000 hours and with a view to providing an interface which greatly improves the existing ones found at internet search engines .
this project concerns the development of an extensible software system that tests and compares three different types of artificial intelligence algorithms for their suitability as the control systems of agents in computer games . game intelligence normally revolves around the use of either fuzzy state machines or more ad - hoc solutions . three types of system are introduced , goal subsumption architecture , fuzzy state machines and neural networks , each of which is designed to deal with a homogeneous set of problems associated with games . the project is split into two main sections . the first section deals with the development of a software framework in which the three systems can be tested . the second details analysis of each system , by looking at quantitative measurements of each control systems performance in terms of complexity , and examines their behavioural performance with some discussion of how the various behaviours are achieved . development covers the use of ' fuzzy ' decision criteria in both the goal subsumption and fuzzy state approaches , showing how it allows an agent ' s behaviour to be unpredictable , yet still suitable for the purpose . each system has a set of benefits and problems , each of which has a trade off in terms of complexity . this project highlights the problems found when using neural networks . problems detailed are caused by the used of genetic algorithms and the fitness function each leads to comparatively high development time and variable results . specifically , defining a fitness function involved developing a large part of the solution to a problem .
visually impaired and blind computer users wish to play computer games . however , due to the graphic nature of many of them this is not possible . auralisation or the representation of a realistic environment audibly will be used to create a navigable world using binaural ( 3d ) sound . the final implementation serves as the first proof of concept that blind and visually impaired computer users are able to play multiplayer computer games using binaural sound . haptic or force feedback devices are supported and provide an alternative source of input and output between the virtual and real world . research is undertaken into the areas of computer games for the blind , online gaming , how humans hear and are able to determine the location of a sound , and how an environment can be represented realistically through the use of audio cues .
the website manager ( wm ) is a tool designed for the web site architect . the wm clarifies , develops and implements the ability to create and manage the structure of a web site , which no other application currently provides . the wm therefore extends the suite of applications that are available to the web site architect . however , the wm goes one step further . every application available to a web site team acts independently , and therefore the benefits of one do not extend to another . the wm solves this problem by simultaneously developing a standard data structure that can be shared by any web application . applications which are developed to use this data structure provide the web site team with a powerful suite of integrated tools . this dissertation successfully develops three important developments . the first is the ability to abstract the view of the content of the site from the architecture . the second is the architecture of the site content , and the third is a proposal for a standard data structure .
databases are one of the most useful applications of computer technology today . they are used almost ubiquitously in our society at present , and have been for a number of years . almost every business uses similar technology , and every day millions of people interact with them , even if they are unaware of this basic fact at the time . whilst development of database technology has progressed , the basic idea behind data storage is generally realised in the use of relational database management systems , based on a concept first widely published in a paper written over 30 years ago . at present , computer systems are many times as powerful as 30 years ago , and so many of the underlying factors of legacy systems ( e . g . small amounts of memory , slow processors ) are no longer major issues . in addition , the world and its data requirements have become very volatile , especially in the business domain . the purpose of this project is to investigate different methodologies of storing data , and evaluate them in respect to current dbmss , particularly in relation to performance , and the flexibility of expanding them to adequately cope with new data requirements .
between 1913 and 1944 some of the most influential , intriguing and theoretically complex works of art of modern times were created by one man with a vision and a theory concerning the reduction of art into its purest form , "… creating a pure reality , using primary means of expression ." [ loch94 ] by reality mondrian meant a permanent harmony of universal relations which must form the basis of the visual reality . using this idea mondrian produced many paintings , all adhering to his theories and all demonstrating a new artistic genius that , at the time rivalled and eventually overtook the work of even picasso with regard to new ideas of artistic expression . this report concerns a project that concentrated on research into the possibility of encoding a relatively small proportion of the ideas and theories of piet mondrian into an ' intelligent ' agent . the intention being to create a piece of software that is capable of recreating images in the style of mondrian . these images should demonstrate properties similar to those of original mondrian paintings while at the same time being individual works of art , produced independently of user intervention . related topics :
automated text categorisation ( atc ) is the " task of building software tools capable of classifying text documents under one or more of a set of predefined categories or subjects " [ 10 ]. there are several machine learning ( ml ) techniques currently in use in atc and different experiments have been carried out on them to explore their individual characteristics as well as various aspects of the categorisation task itself . this dissertation examines the performance of the naïve bayes classifier on the classification task of assigning topics to articles in the reuters - 21578 collection . the effects on the algorithm ' s performance of using document frequency thresholding as a feature selection technique ; stemming and stop list use are investigated . the experiments presented here show that document frequency thresholding greatly reduces the dimensionality of the term space and results in a significant improvement in the naïve bayes classifier ' s performance . the use of a stop list is also found to improve the effectiveness . stemming of words in the collection causes a slight decrease in algorithm ' s performance over the whole collection though some positive effect is observed in less common topic categories .
the main purpose for the world wide web is the transfer of information in a similar way to the purpose of a library of books . however it is clear that there are many factors that can effect the efficiency of this transfer of information from the web page to the users memory . in order to make their web sites as effective as possible , web designers must aim to optimise all these factors to make their sites as efficient as possible . this project investigates the effect of organisation and layout of the information on the amount of information that the users of the site can absorb . the investigation looks at three possible layouts . the three differing layouts are a site with the minimum number of web pages to display the information , a site with an average number of web pages to display the information and a web site with a maximum number of web pages to display the information . one of these should provide an estimate of the optimum number of web pages for this site . in conclusion , the results found that a moderate amount of web pages yielded the highest amount of recognition by a significant value indicating that the layout of information is a highly important design issue . although a number of failings in the testing process have become apparent whilst completing the project , the results appear to conclusively state that it may be possible to define an optimum number of pages for a web site . this value is dependent on the amount of data stored in it as well as the type of information the web site is about .
java has three components which facilitates it ' s use in writing web - pages with dynamic content – java applet , java servlets and javaserver pages . applets are useful for delivering web - based applications to a clients machine , providing that machine has a java compatible browser . java servlets are server based components which are usually used in some form of data - management roll , producing dynamic content from data - operations . javaserver pages are built on - top of java servlets and are able to separate the dynamic generation from the page design , making them idea for distributing data . there are a number of other languages able to produce dynamic web page content , such as asp and coldfusion script , but none with the cross - platform compatibility of the java solutions .
several researchers have experimentally shown that substantial improvements can be obtained in pattern recognition problems by combining the outputs of multiple classifiers . the use of such a technique , often called the ensemble approach to combining predictors , has been shown not to require expensive supplementary computation to when techniques for creating accurate unitary classifiers are employed . an investigation of two major aspects of the ensemble approach , member creation and combination , is undertaken . the presence of coincident errors between ensemble members is established to be the main restriction to the improvement of ensemble performance . it is seen to be likely that a population of neural networks has coincident errors . during an investigation of methods to minimise the effect these errors have on the aggregated output , a new procedure for ensemble member selection is proposed and evaluated along with a new variation of the majority vote scheme . the two new procedures are tested against other similar methods ; the new methods were seen to perform better .
this project continues the work of andrew kinghorn , in producing a replacement for the palasm software used to teach the basics of logical programming to undergraduate students . in the previous academic year a visual system was produced that could simulate logic circuits by connecting various components together . the addition of a programmable array logic ( pal ) chip allowed the user to enter logical programs in sum of product form . the project had two main goals , the first being to produce a logical programming language similar in syntax to java which could be used to produce a jedec file to program a pal chip . the second goal was to integrate this into the visual system and to add extra functionality . a rapid iterative design methodology was used due to the sheer volume of work that needed to be completed to make the system usable . the visual system had many bugs , and this situation was not helped by the lack of documentation provided . it was possible to create experiments to show the system worked and also that it could be used as a teaching tool . the minimisation of equations proved to be a complicated task and whilst the finalised minimiser works well , it does not always produce optimal results . some features had to be omitted that would have been useful , in order that a complete integrated system could be delivered . whilst the delivered system functions as intended it is clear that there is much room for future expansion and improvement of it .
this project enhances the wincase case tool , with an extended repository to allow the use of the dataflow algebra . the dataflow algebra , is formal specification technique defining a system as the data flows between its components using a formal algebraic syntax and data flow diagrams . wincase is intended to become a meta - case tool . both wincase and dataflow algebra are under research at the department of computer science , at the university of sheffield . detailed analysis is carried out on wincase and the implemented repository for parallel communication sequential code . design , implementation and testing are discussed with results . concluding in a successful extension to wincase and providing base for further work with both software and documentation .
a hybrid machine is a combination of discrete and continuous systems . this project has produced a software tool which allows such machines to be created and simulated . although the tool has limitations which are discussed and evaluated it provides all of the essential functionality required for such a system . the systems development is reviewed so as to provide sufficient insight that it may me maintained , altered and improved . various examples of hybrid machines are looked at and simulated within the tool .
dataflow algebra ( dfa ) presents a novel way of specifying a system from the top down . dfa achieves this by providing a language which can be used to formally specify dataflow diagrams . this project was concerned with producing a tool which allowed dfa specifications to be processed by a machine . it is hoped that this will feedback into the research by decreasing the time required to develop specifications . in order to process the specification the need for a parser was identified . this highlighted several possible solutions to the design and implementation of the tool . of these a design was chosen which used the antlr parser generator to produce the core parser of the tool . this was augmented by a expandable framework which allows easy modification of the parser . the design is such that future tasks such as translation and theorem proving could be easily integrated . the tool produced : dfatool , is able to check a modified version of the specification for dfa previously presented . the parser checks the input , lexically , syntactically and semantically for deviations . basic facilities to integrate cadiz and z into dfa are also given . whilst the operation of the tool is not perfect it offers significant benefits to the dfa author . the design of the tool should also allow for easy changes as the specification of dfa evolves .
the objective of this dissertation is to investigate feasibilities of making demonstrative tools in java for the taught speech courses in the department of computer science at university of sheffield . this dissertation is going to reveals graphical components of matlab auditory demo ( mad ), an exemplary previous practice , and the other tools and literatures . the literatures include the necessary theories and phases of development of java auditory demo ( jad ) with the intension of providing only one or two demos . one particular demo that included in the jad is dynamic time warping ( dtw ) introduced by sakoe and chiba ( 1978 ). a simple automatic speech recognition ( asr ), is the process by which a computer maps an acoustic speech signal to text , to draw the map and plots the cost path . however , another demo for hidden markov models ( hmms ) is excluded for further development .
this project involved the production of a java package containing interactive applets to help first year students in the computer science department understand vector ge - ometry concepts in three dimensions . each applet deals with one problem type , using the java3d extension libraries to con - struct an interactive three - dimensional diagram representing the given problem . this diagram can be rotated and magnified in order to give the student a better feel for the problem at hand . the applets also provide a way for the students to test their skills at solving problems of these types , providing corrective feedback if necessary . the package contains three such applets , each dealing with a different problem , and has been structured in such a way as to make future extensions to the package much simpler to implement .
the idea behind subdivision surfaces for 3d polygon meshes has been around for many years but the possibilities of its application in computer and video games has only been looked into recently , thanks to the rapid growth and development of 3d graphics in the games industry . this dissertation describes the implementation of a new subdivision scheme outlined in a paper entitled ' maps : multiresolution adaptive parameterisation of surfaces ' by aaron lee , wim sweldens , peter schroder , lawrence cowsar and david dobkin . while not strictly a paper about subdivision or the handling level of detail ( lod ) in computer graphics , it introduces a new subdivision method which uses parameterisations for perturbing the new vertices to optimal positions when subdividing the surface of the base - domain mesh . the base domain and parameterisations are derived by performing a mesh - simplification of a high - resolution mesh while preserving information regarding the positions of the removed vertices . the implementation described in this project only covers this parameterised mesh - simplification . this project will provide a brief introduction to the theories behind mesh simplification and lod handling followed by a literature review of relevant work . the main body of this dissertation will describe the design and implementation of various data structures and algorithms used during the course of the project as well as the results obtained . this will end with an evaluation of the project and some conclusions drawn from throughout the dissertation .
there are many different methodologies that can be followed to produce a piece of software , some of which are easier than others to follow . this project has looked at one specific lightweight methodology created by kent beck known as extreme programming , or xp . throughout this project , the various shortcomings of xp are highlighted and possible solutions are offered . specifically , attempts have been made to tighten the definitions of terms invented within the xp world but never succinctly explained , as well as an attempt to bind the gap between the user stories ( initial system specification ) and the final software . it is hoped that in binding this gap , more hope can be offered to the software developer wanting to produce correct software . the analysis of xp is based on experience gained whilst applying it to produce a particular piece of software , known as piy - ii . piy - ii is the successor to piy , the product of a previous dissertation project . piy , ( program it yourself ), is a tool aimed primarily at non - programmers to allow the creation of applications without the need for code . by using xp practices , piy - ii has been written with a great deal of success , and has grown into a fully extensible software development environment .
the aim of this dissertation is to construct the first c . a . s . e . tool in a series , which will cover the forthcoming version 2 of the discovery method , and to provide a repository framework that can be utilised and extended as required throughout the method . the first c . a . s . e . tool allows the editing and manipulation of tasks from various integrated perspectives . tasks are elicited in relation to the actors that perform them and are clustered , presented structurally according to their similarities ( alternation ) and their decomposition into subtasks ( hierarchy ). all tasks have narratives as their primary backbone . complementing this , tasks are presented in their sequential order , like a flowchart or activity graph . the c . a . s . e . tool should support the visual manipulation of tasks in both of these views , maintaining consistency across the structure and flow viewpoints , supporting the documentation of tasks via simple narratives . all diagrams created from the flow viewpoint should be exportable allowing the second c . a . s . e . tool in the series , that of o ' brien ( 2000 ), to import and continue with the discovery method ' s processes .
the purpose of this project is to extract the silhouette from existing polygon mesh models and enhancing them . it does this by bringing together npr work done in object space , ( more specifically in the field of silhouette edge detection ) and then by using ideas from previous image - based approaches seeks to enhance the silhouette images produced . building on the work done by lee markosian et al at brown university , this project first builds a data structure to allow it to implement some of the algorithms explained in their work and then seeks to solve some of the problems associated with them . it takes two approaches in attempting to abstract out strokes , one by directly searching the existing data structure and finding nearby silhouette edges and the other by first building a map of edges to traverse . the project compares and contrasts these two techniques and then shows how by altering parameters within them we can make silhouette detection both more efficient and silhouette images more suitable for enhancement and manipulation . finally , it applies a number of enhancements to the strokes that make them look more visually appealing and then concludes by suggesting ways the techniques developed can be used for future work .
while much has been done to make authoring web sites easier , little has been done about management sites as a whole . sites with a large amount of funding can afford a solution in the form of a bespoke application , but smaller ones can ' t and have to make to with static pages . this dissertation proposes a system called toucan , designed to solve the problems associated with management with a generic application suitable for any site . this system has three parts , a scripting language used to describe a web site , a developer aimed at it professionals for authoring the scripts and a client aimed at novice users for making minor alterations . toucan script allows developers to logically structure a web site using templates to encapsulate common elements , and variable substitution to effect localised changes to them . this dissertation covers the development of a prototype toucan system , which includes the scripting language and the client application only . it shows the value of the ideas developed and makes proposals for the extension of these ideas in future work .
natural language communication between a human and a computer is a field of research that has been ongoing for the past few decades . even with the advances in technology over the previous decade or so , linguistic communication between a human and a computer is still considered to be a difficult task . the aim of this project is to develop a game that interacts with a human player through natural language , in particular by receiving requests from the player and acting on them . it is important to note that natural language consists of many words corresponding to the same meaning and also that the meaning of a sentence can be acquired by ordering the words in different ways . it follows from this that the player ' s request can be said in many different ways . the game is required to act on these requests regardless of how it is said . the final product was tested by 10 people who entered 150 sentences into it . out of these , 123 ( 82 %) resulted in a correct response from the game and 27 ( 18 %) resulted in an incorrect response . it was found that from the 27 sentences resulting in incorrect responses , 17 were not understood by the game and so no actions took place . however , the other 10 sentences resulted in the wrong actions taking place , so the game acted differently to what the player expected .
this paper firstly introduces the main branch of mathematics that lead to the discovery of wavelets and their transforms , fourier analysis . the paper gives a detailed account of the development of different fourier transforms and describes the motivation for a need for a different transform due to the multiresolution problem . the theory of wavelets is then introduced as a solution to this problem together with a detailed account of the continuous and discrete wavelet transforms . the paper also documents the development , testing and evaluation of two computer assisted learning tools that students could use to learn about wavelet theory .
this project involved the production of a distributed reporting application . the purpose of this application was to efficiently create annual school reports for claybrook first school . this involved recognising areas of the current manual system that could be improved . based on these findings a new automated system was developed . following a requirements analysis and a review of relevant e - commerce technologies , a new system was designed . this system was focussed on the needs of a single school . the design of this system was aimed at providing a solid framework . this was so new functionality could easily be implemented at a later date . the final system shared report creation tasks between specified members of staff . common tasks were centralised so they could be executed once and applied through the system . java server pages , java servlets and supporting java classes formed the core of the application . system information was stored in a mysql database and accessed using java ' s jdbc api . javascript was used to provide client side validation . these technologies were structured using a multi - tiered architecture and the model view controller design pattern .
algebraic specifications allow software developers to define the behavioural semantics of objects by declaring equivalences between object states which are reachable through an application of an objects ' methods . the checkmate project aims to build a tool which performs syntactic and semantic checking on algebraic specifications of objects . the system parses and type checks the specification , then proves the mutual consistency of the set of axioms specified . it is also able to check the validity of subclass relations . a specification which is verified by the tool gives a system designer a greater level of assurance of correctness , and an insight into the behaviour of the specified object .
people today use the internet to find information , but to find what they require they have to read through pages of text to get the answers they want . what is needed is a system that could sort though the internet and find that data we need more precisely than the current search engines . this is a report of a project that is one solution to the above problem . it involves the development of an interface for an existing question answering system and adapting it to work over the internet . this project hopes to result in the production of an on - line question answering system that can be used in demonstrations anywhere in the world .
mathematics is the language of nature . everything around us can be represented and understood through numbers . if you graph the numbers of a system , even one as chaotic as plant growth , patterns emerge which can be used to accurately recreate complex structures .
along with the ever - increasing technological advances in computer hardware , come the demands of the media and public to push these improvements to and beyond their limits with the help of the latest software . this requires computer game developers to produce software that looks and feels more realistic , but also runs faster and more smoothly . as hardware capable of displaying and manipulating complex , three - dimensional geometric objects became more readily available , subdivision surfaces became much more significant to game developers with their ability to produce approximations of perfectly smooth , topologically arbitrary surfaces , using relatively simple recursive algorithms . the paper presents a qualitative evaluation of four of the most common subdivision schemes ( the butterfly , catmull - clark , doo - sabin and loop schemes ), regarding their appropriateness and particular benefits , when used to model smooth surfaces that are subject to manipulation . in order to compete the aims of this project a tool that would enable the investigation of each of the different subdivision schemes was developed . the investigative work carried out using the tool that had been developed , along with the detailed knowledge of the workings of each subdivision schemes gained throughout the course of the project , enabled a detailed analysis of each of the schemes to be carried out ; producing some interesting results . when choosing a subdivision surface to model a deformable object it is essential to know in what situations the surface will be used , and how it will be deformed . interpolating schemes , produce a surface that is more the smoothed vision of the base mesh and will have more well defined curves ; meshes at low levels of surface division closely resemble the limit surface . approximating schemes produce a surface that is a continuously curved interpretation of base mesh and will not have any well - defined features ; meshes at low levels of surface division bear little resemblance to the limit surface . in general approximating schemes tend to smooth deformations into the surrounding surface as it is divided , although greater definition can be gained at higher levels of subdivision , whereas interpolating schemes produce a more well defined deformation .
computer games today feature complex terrain as a main part of the game environment . it is important that the terrain is realistic and is varied . a good terrain will help prolong the longevity of a game , and it is therefore important to make the terrain as close to the " real thing " as possible to gain and maintain the interest of a game player . rendering a large terrain takes time . there are at least 10s of thousands polygons to render . in real - time applications speed of rendering is a very important factor . real - time means that several frames are being rendered per second . even with help from specialised hardware the amount of polygons that make up a good terrain , are too large to be considered every time the frame is rendered . there exist several algorithms that will reduce the set of polygons and preserve the quality of the terrain , e . g . level of detail ( lod ) and potentially visible sets ( pvs ). this project set out to find a new possible algorithm that will reduce the number of polygons that are rendered on each pass . the project will investigate whether pre - calculated information is possible solution to a fast algorithm that determines which parts of the terrain are visible . the application provided a good test suite so that all the necessary information about the algorithm ' s performance could be extracted . it also provided another algorithm for comparison . the results were compiled and investigated and concludes that the algorithm does not produce a satisfactory result .
computer algorithms are one of the most crucial aspects behind how efficient a program executes . it has been well documented that different algorithms performed on the same sets of data have varying effects with respect to time and space . to get a better insight as to how these algorithms work one of the better learning tools is through animation . it is far easier to understand complex routines by an animated representation than a series of complex lines of code , and for the this reason the research field known as algorithm visualisation came about in the early 1980 ' s . it is intended that this report will take a look at the current state of research into algorithm visualisation and make some assertions as to what makes a visualisation work . from here the development and implementation of several algorithms will be undertaken which can then be placed on the internet for the use of others .
this work discusses the field of case ( computer aided software engineering ) including its principles and aims , and how to apply these ideals in the context of a web page design tool . research on current market products , within both fields are discussed and a specification for a web tool was developed and implemented within the object orientated language of java to support the availability aspects regarding case principles . results , including the functionality and the usability of the tool are recorded from the field tests . conclusions to the development of the tool are related back to the case principles and future areas of interest for the tool and subsequent projects are discussed .
using the output of the natural language processing group ' s information extraction system within the department of computer science , university of sheffield , this project produces a visualisation tool that will support link analysis using information extracted from the on - line news sources . the visualization tool enables key information from across a range of news sources to be seen " at a glance ". the tool is intended for users who desire readable results from information extraction and so meets high hci standards . the project incorporates a standard software development process including specification , design and implementation and testing phases , and an adequate visualization tool is produced to meet these requirements following an investigation into the background of information extraction and the techniques that may be employed to develop such a visualization tool .
each programming language is based on a formal theory of some kind . unfortunately , no single theory encapsulates the object oriented paradigm fully and hence each language is subject to certain flaws pertaining to the theory it is based on . once the theory that the language is to be based on is decided the surface syntax of the language needs to be designed . this can be as flexible as the designer requires it to be but must obey the rules defined in the theory . once the surface syntax is designed a compiler needs to be written . this takes a piece of source code and checks it to see if it syntactically correct . if the program is valid according to the syntax it is then checked for semantic errors . these are the errors pertaining to whether the code , although syntactically correct , contains any errors relating to the meaning of the code . once the code is syntactically and semantically correct the compiler must then generate the code into a language that can be run by the target system . this dissertation is concerned with the processes and design issues that are involved with the design of an object oriented language and the implementation of a compiler to use the language .
this report describes a system for comparing different algorithms for textual document retrieval . the system was developed throughout the year , and this report contains discussions on the work achieved . background to the project and the field of information retrieval is included , along with discussions of the experiments carried out and the results and conclusions . the evaluation section describes how the project progressed along with the difficulties that were encountered along the way . background information that was essential for the project , but not related to the field of information retrieval is described as well as a brief overview of the information retrieval process .
plagiarism : the reproduction of material without proper reference to the source . the problem of plagiarism is on the increase in recent times due to the evolution of on - line material and has serious become in serious problem in many fields . the objective of the project on which this report is based is to use a computer program to automatically detect plagiarism . this is a broad objective because plagiarism can occur in many forms such as collusion , cut - and - paste or paraphrasing , it can even occur accidentally . there are also many different techniques to try and detect plagiarism , one such technique is to try and match words and strings between documents . the solution to this project takes the form of scientific programming principles where - by a series of hypotheses are outlined . these are implemented and tested one at a time . in this case the resulting program for each hypothesis produced better results than the previous one . the final program is an implementation of the greedy string tiling algorithm and this program produces the most significant results . this is due to the fact it combines and extends the advantages of the other matching methods . this algorithm proves a satisfactory solution to problem outlined in my report .
statecharts is a visual language for specifying the behaviour of reactive and synchronous systems . it is based on finite - state machines and their visual counterparts , state transition diagrams . the success of statecharts in the software engineering community lies in its graphical appeal and its capacity for modelling the complex control aspects inherent in many software systems . different dialects of the language have been employed in several software design notations supported by different commercial tools , including statemate , stateflow , esterel and uml statecharts . the aim of this project is to systematically analyse the abovementioned dialects of statecharts regarding ( 1 ) their language features , ( 2 ) their use and employment in engineering practice , and ( 3 ) their semantic underpinnings .
this dissertation is based on a project to design and build a simulation of the enigma encryption machine , along with a tutorial on the how the machine works . this was achieved by building a website and series of java applets , which describe the function of the machine component by component . the resulting website performed well under user testing , however due to problems during testing , it is unclear as to whether the simulator is one hundred percent accurate . in general , had other testing options been available such as comparison with a genuine enigma machine , and this would have been a very successful project .
java provides support for musical instrument digital interface ( midi ) programming with the java sound api . this study investigates the facilities that java provides for midi programmers and looks at using other java packages such as java swing for interface design and implementing user interaction . java ' s suitability for building midi musical compositions will be demonstrated by producing a simple music composer application allowing basic facilities for music construction , playback using midi synthesis and the saving of musical compositions . the investigation highlights design issues that are involved with interface design and midi programming . these include constructing musical notation , printing a musical score to paper , conforming to midi standards to save and load standard midi files and producing musical notation from a given midi file . the results of the development work and the research behind the simple music composer is presented together with the specification , design and implementation details in this report .
plagiarism is a problem affecting institutions across the globe and higher education institutions are no different . the internet is already having a massive impact on plagiarism , making it easier for students to copy work from a plethora of sources . the rate of internet growth means that this is not a problem that will go away . most plagiarism detection tools use a simple process of pattern matching to detect plagiarism . the purpose of this dissertation is to explore the possible use of stylometry to achieve an automated plagiarism detection tool . the area of stylometry is relatively new . it proposes that when an author writes a document that author has an underlying ' style ' in which he or she ' has ' to write . this ' style ' is predetermined by some unconscious habitual features and is hence consistent throughout a given document . the changes in the ' style ' of an author can be illustrated using the cusum technique . the project seeks to explore the hypothesis , " can deviations in the ' style ' of a document be used to determine mixed authorship in that document , and hence signify that plagiarism has occurred ?" a variety of interpretations of cusum data are made in an attempt to find a satisfactory plagiarism detection system .
the current improvements and innovation in automatic speech recognition ( asr ) technology have spawned new and original ideas in the ways in which it can be used to aid people in their lives . the technology has reached a stage where it is robust enough to work accurately in noisy environments and also be integrated into devices such as mobile phones for automatic dialing features or car stereos for control of radio station , volume , etc . these 2 examples of asr integration into our environment shows the possibilities of using this technology to control many aspects of our surroundings . those who have a particular interest of the use of asr in controlling their environments are those persons who cannot manipulate their surroundings due to physical disability . however there is a subset of those people who are physically disadvantaged who are unable to use normal environmental control units due to having a speech disorder as one of the symptoms of their primary disability . the speech disorder that this paper looks at is called dysarthria , which is derived from " dys " meaning disorder and " arthria " meaning of speech production . there are 6 types of dysarthria depending on which of the articulators lack control . these are : ataxic , extrapyramidal hypokinetic , flacid , mixed and spastic . asr can also be a tool used by the speech therapist to aid in diagnosing and improving the quality of the speech produced by dysarthric patients . there exists a large european project called ortho logo paedia ( olp ) that is creating a set of computer - based therapist tools and a robust speech recognition system for the use with assistive technology ( environmental control ). one of the first jobs the therapist must accomplish is to diagnose and evaluate which type of dysarthria the patient has . this is currently achieved using a set of tests that isolate the functionality of the articulators so that the areas that lack control can be identified . the project this paper describes is the automation of 5 of these tests from the frenchay dysarthria assessment ( fda ) tests to classify the dysarthria with the aim of integrating this into the olp project . a tool was created in matlab for use by a speech therapist in recording and analysing each test to give an objective and empirical evaluation of the sounds produced to assess the type of dysarthria .
speech corpora are databases consisting of audio - data along with aligned text - tags or ' transcriptions '. there have been many speech corpora constructed by many research institutes around the world , but no harmonisation of these diverse databases has taken place . as such , there are now hundreds of corpora , most of which use their own proprietory database storage representation , along with their own custom - built query tools and query languages . the project ' s aim was to provide an intuitive search system for the shatr multi - simultaneous speaker corpus , and additionally to provide a framework for a generic search system applicable to any speech corpus . as such research work was conducted using the shatr corpus , which is considered to be the most complex type of corpus the system should have to deal with . various existing and emerging technologies were investigated as to their suitability for use in the project , along with the research of current speech corpora and their search facilities . the goal was to create a single framework , with an intuitive interface , which could used to deliver many large speech corpora to researchers all over the world . a database structure was created into which other speech corpora could be inserted , and an intuitive web - based interface was built which allows complex searching of the corpora . in addition , the interface can profile the user and retain settings between sessions .
the feasibility of using bezier patches in 3d computer games will be evaluated by implementing and testing several subdivision algorithms . the different algorithms will be compared to each other , to determine which algorithm is best suited for use in 3d computer games . comparisons between patches and polygon representations will not be considered , since comparing the visual quality of polygons will be difficult at a continually varying distance from the viewpoint which is automatically done for bezier patches .
the purpose of this project was to develop a system which could count the number of lines in a java file . the necessity for a system like this is highlighted through the increasing need for measurement and analysis within the software engineering industry . the system was intended to not only count the number of lines in a java source file , but also to restructure that file into a defined format . this would then allow for measurement and predictions to be made based on fair comparisons . in order to develop the system , the whole subject area needed to be researched to fully understand the implications of building such a system . the scope of the project itself then had to be defined , to ensure that clear achievable goals were set for the system . the system was analysed , designed and implemented , based on the object oriented approach . throughout the system development all attempts were made to produce an object oriented system . the final product was a working system that met the initial requirements specified for the system . the process of developing the system , was not always as object oriented in nature as desired . the final system how ever is an object oriented implementation . some of the desirable features were not implemented , though the project still successfully met its goals .
when human listeners are confronted with musical sounds , they rapidly and automatically orient themselves in the music . we are able to make numerous judgements about the composition within a few short seconds . this dissertation focuses on machine listening of music and making some of the same judgements as humans are able to . the project consists of two separate parts . the first part examines a number of methods for the recognition of pitch and key from pieces of monophonic music producing a system capable of detecting simple musical keys from a number of songs . the second part examines methods for the extraction of beat and tempo from pieces of monophonic and polyphonic music implementing a system capable of tracking the beat and calculating the tempo from a number of different polyphonic compositions .
the aim of this dissertation project was to design a window - based software tool to aid the construction of asynchronous circuits . the software tool had to be designed in such a manner that it would be possible for a future dissertation project to extend it to an asynchronous circuit simulation tool . a background to the theory of asynchronous circuits is provided followed by the design process that were employed when implementing the system . the software tool produced provides the functionality required , and is designed in such a way that the simulation extension will be possible within the scope of another dissertation project .
a compact rule - based encoding of dungeon levels for computer adventure games is presented . the rules effect topological transformations on a graph representation of a dungeon , a system of rooms and connecting corridors , populated with challenges , puzzles and treasures . a major feature is the way in which the nonterminal nodes of the graph abstract over the rest of the dungeon level , such that the paired elements of a threat defending a treasure , and a key solving a puzzle may be introduced at single points , but subsequently become physically separated in the dungeon . nodes in the graph may eventually be refined as single fixed places , or as whole substructures within the dungeon . the random application of transformation rules generates dungeons of different topological structure and with different levels of nested puzzles .
postit ® notes have proved to be a huge success in offices across the country . postit ® notes are a useful way of leaving messages and information for colleges . the aim of this dissertation is to use the principal of leaving messages for other people , and translate it for use in webpages . an electronic form of postit ® note system could be incorporated into a website to allow visitors to leave messages attached to different areas . these messages could be anything from suggestions on how to improve a part of the website , to comments on what other people have said in their postit ® notes .
the purpose of this work is to design and develop a system which can be used by amateur code - breakers to encode , decode and break several different types of simple ciphers . this dissertation begins by introducing and describing a number of different ciphers , from simple substitution ciphers such as the caesar shift , through to more modern systems such as des and public - key encryption . the stages of development of the software system are then described , from the initial requirements analysis , through design , implementation and , finally , testing and evaluation phases . the final workbench system was developed in java , providing mono - alphabetic substitution and vigenère encryption and decryption , as well as effective code - breaking modules for mono - alphabetic substitution , using frequency analysis ; the caesar shift , using a brute - force approach ; and the vigenère cipher , using a " hill - climbing " approach to discover the best keyword . the system developed will be tested using a variety of texts written in different styles of english , to evaluate the performance of the code - breaking modules .
a vast number of people are now using e - mail as a communication medium , for both business and personal purposes . this has in turn led to an increase in the amount of unsolicited , ' junk mail ' being sent . thus , it has become extremely desirable to create an effective e - mail filtering tool , capable of automatically removing unwanted e - mail from a user ' s system . in this report , existing approaches towards information retrieval and text categorisation are reviewed , with particular emphasis on the recent emergence of statistical approaches . this leads to the analysis of a relatively new statistical approach towards text categorisation that uses the chi - square test . the possible application of the chi - square approach in the context of e - mail filtering is explored , and a functional e - mail filtering tool is subsequently developed . the final system is thoroughly evaluated , using both e - mail and news article corpora . the results suggest that the chi - square approach can be used to effectively filter e - mail , and extensions that could be made to the system to further improve accuracy and functionality are proposed .
the naturally symmetrical form of plants makes them ideal candidates for mathematical modelling , which in turn can be can be used in computer graphics applications . realistic computer - generated plant images can be used in a variety of contexts , from leisure to the scientific community . this motivates the existing body of work on ' perfect plants '. so far , most work in the field has concentrated on the use of l - systems to achieve the task , but this approach has been explored nearly to its limit . their simple form makes it difficult to use them in the generation of ' imperfect ' images that have been damaged by weather , pollution or animal activity . salim vanak proposed a technique by which l - systems may be combined with the x - machine modelling technique to generate plant images that were modified by environmental factors . this work was built upon into a model that attempted to accurately model the environment surrounding a plant using x - machines . this project continues work on modelling modified plant images , and puts forward a model that uses x - machines in plant and environment definitions to produce images that take account of their environment as they grow . problems of propagating nutrition and light through the model are dealt with , in a manner that can also prevent collisions between plants . a design for an implementation of the model is reached , and although not fully implemented here , the subject is left open for further development in future projects .
dna microarrays are a recent advancement in biotechnology that could revolutionise many areas of biology , from the treatment of disease and illness , to the cataloging of the function of every single gene in the human body . the first arrays were created around 25 years ago and since then technology has improved to allow ever increasing sizes of arrays and ever increasing accuracy of observations from these arrays . array technology now makes it possible to obtain expression levels for thousands of genes in just one experiment . microarrays have thus transformed existing biological techniques and have made the formation of new and improved ones possible . the challenge now is to develop methods and algorithms that can be used to analyse the huge quantity of data currently being churned out from research using microarrays . most importantly , methods are required that can facilitate the discovery of patterns amongst this data . from these patterns predictions can be made about the genes ' function and , by comparison with the patterns of genes with known function , these predictions can become accepted fact . this is where my project enters the scene . the aim of the project was to implement and evaluate some of the methods that have been created . there are a vast number of these methods available . i have focused on two techniques that are at the forefront of this work , self - organising maps and support vector machines . i have based my investigations into the feasibilty of using these techniques to this end upon previous research . the two research areas i followed involve work done on the molecular classification of cancer by golub et al . ( 1999 ), and work done on the functional classification of yeast genes by brown et al . ( 2000 ). a further aim of the project was to critically evaluate their work and their claims of success . i successfully implemented the self - organising map and support vector machine algorithms plus a simple class prediction algorithm . i found that all gave very accurate results when used on the specified data sets but did not generalise well . it therefore appears that pattern recognition techniques can be used to analyse microarray data , but the particular method used must be carefully chosen . this is a major limitation as an in depth knowledge of the vast range of techniques available is required and a lot of unnecessary and time - consuming preliminary investigations are a distinct possibility . further work must also be done if these techniques are to be considered accurate enough to become the only test necessary . currently they must be used in conjunction with other tests .
the future of computing is highly distributed and increasingly mobile . at the same time the number of heterogeneous computer platforms is growing , due to the increasing prevalence of pdas and mobile computing . these facts combine to cause increasing difficulties in designing and implementing distributed applications . software tools and frameworks are required that abstract way the complexities of distributed computing , even in the most heterogeneous computing environments . javaspaces have been created with this aim in mind . they transparently overcome the major problems of distributed computing , including latency , synchronization and partial failure , by providing asynchronous communication between remote processes . furthermore , javaspaces can be implemented across the plethora of platforms on which the java programming language can run . the usage of tools such as javaspaces will grow because of their suitability to modern day computing applications . it is of interest to know exactly how the javaspace works and what limitations it has . in order to answer these questions a research project was undertaken to examine the functionality provided by javaspaces and to investigate the limitations of the technology and the computing model it promotes . the projects approach was to create a series of software tools which attempted to investigate different aspects of javaspace functionality , or which examined potential improvements .
from investigating what speech is , and what it is commonly used for , it is proved possible to create a game that matches the interface in the case of a speech based computer game where you can command bots using speech . to do this , the multi - modal potential of speech has been realised to overcome problems that existed in various bot command systems of interface conflicts . bots have been implemented to be asked information and given orders to interact in a certain way with the game world and it ' s contents . to create the system , appropriate experiments and design methods are shown to identify potential problems with the interface and with the system which can then be considered during the design process . the project is then discussed and it is shown how improvements may be made to this project .
automatic classification of proteins into homogenous superfamilies , by looking at their amino acid sequence has long been a goal for scientists studying proteins . the current method is very time consuming and complicated . several statistical models have been created to aid in the classification but so far they have only had partial success . they can give a clue to the protein ' s superfamily but not a definite answer . recently jaakkola and haussler proposed a method to combine a hidden markov model with the discriminate technique of a support vector machine . this method builds on the previous work of using a hidden markov model but uses a different approach to decode the model . it is claimed that this method is able to provide better results than just using a hidden markov model . this method is known as the svm - fisher method . a partial implementation of the system is described in detail including the problems encountered and the differences in the techniques used between this implementation and the jaakkola implementation . the differences were caused by not being able to implement several mathematical functions as they did not seem to be effective . the results achieved suggested that this method is able to produce better classification results than just using a generative hidden markov model . a re - run of the experiment presented in the jaakkola paper was performed and compared but the results achieved were not as good .
effective teaching of speech and hearing principles is hampered by the multi - disciplinary nature of the field . encouraging students to explore these principles for themselves is to be encouraged and computer - based demos are an excellent way of doing this . the combination of audio and visual stimuli and the ability to interact with parameters and see their effects in real - time provide a service which cannot easily be provided in the classroom . the department of computer science at the university of sheffield has produced a suite of demos for just this purpose , written in matlab . this project is an investigation into the feasibility of implementing a similar set of demos in java . the study is built around the production of a framework to aid the developer of these demonstrations . java ' s api is designed to be more flexible than that of matlab , but as a result of this development can often be much more time - consuming . the production of a framework for this specific purpose should speed up that development process .
motion capture data is used widely throughout the games industry to enable human characters to take on a realistic appearance and action . the sequences used to animate the characters are large and expensive to produce , which limits the amount that can be used within a single game . this study aims to remove these limitations by allowing these existing motion capture sequences to be modified , in real - time , to allow the game characters more freedom of movement . the study bases its research on the example of a goalkeeper performing a save , and aims to prove the existence of a catch envelope – an area of possible points that the goalkeeper can reach , and hence convincingly catch the ball . the study bases its research on two sequences , each of which has only a single point in the sequence where the ball can be caught . by merging the two sequences using varying degrees of dominance , any point between these two original points can be reached by the goalkeeper , hence satisfying the definition of an envelope . the study begins by showing that merging mocap sequences is possible , and provides realistic results . it then moves on to show that the technique can be used within a game environment , by showing that the goalkeeper can automatically select the best merge factor to reach a specified ball position . finally , it shows that the technique does in fact provide a significant amount of new sequences from just two original ones , proving the concept of a catch envelope . the study concludes that the technique is a success , and currently the best method for mocap modification that can be used in real - time . future research is recommended , with exciting possibilities .
non - linear transformations ( nlts ) are a relatively unstudied method for improving ensemble classifiers ' performance at a learning task . they work by perturbing ( non - linearly transforming ) the data so that each classifier to be combined in the ensemble makes a slightly different generalisation about the task and therefore the number of coincident errors in the ensemble is reduced . the research tackled in this project is assessing the validity of nlts for creating diverse neural network and decision tree ensembles . the nlt method is compared to the well known disjoint sets technique . the overall results show that nlts are an effective way of creating a diverse ensemble . nlts generally work slightly better for neural networks than for decision trees , and have been shown to outperform the disjoint training sets method .
the problem of attributing authorship to pieces of text was known long before the computer was invented , and has only recently become of interest to natural language processing based researchers as the power of computation now allows large corpora of text to be analysed in a short period of time . the majority of authorship disputes are over classical pieces of literature , but are also found in modern , especially forensic , cases . there are various automated stylometric methods of attributing authorship to texts , all of which are examined in this thesis ; but one particular method , the cusum method , is focused on and a detailed examination and computerised implementation is undertaken . the cusum method is then tested on a popular authorship debate : henry viii by shakespeare . a number of possible authors are tested and detailed results are produced , which although not being conclusive in identifying the authorship of the play , lead to a conclusion that raises questions over the validity of the method and of the automated authorship attribution field as a whole . related topics :
this project is concerned with extracting meaningful & accurate gis ( geographical information systems ) data from scanned maps , and improving & building on the techniques already in use . the current data input processes for gis projects are very time - consuming and create a bottleneck before any such project can get under way . the focus of this investigation is to find the best way to process a scanned map to extract the necessary data for analysis in a gis application , whilst retaining the accuracy and reliability of the original printed map .
efficient routing of packets in computer networks is a prerequisite for wide deployment of network - enabled devices , especially mobile and distributed computing . current approaches to this problem have been partially successful , but are liable to breakdown when under heavy load . boyan and littman presented the q - routing algorithm , a network routing algorithm based on q - learning , a method from the emerging field of reinforcement learning . q - routing learns to route packets in an adaptive manner allowing slow spots in the network to be routed around , and has performed well in simulation . neural networks have been used with some success to perform q - learning , and would seem to be a possible method to allow q - routing to scale well beyond it ' s initial table - based implementation . the results achieved are disappointing , but some insight is gained into the difficulties of using neural networks as a function approximator in reinforcement learning tasks .
could a computer game be successfully devised that uses computer speech and hearing as the primary mode of interaction between game and gamer ? this project aims to produce and test such a game , and evaluate how today ' s speech recognition and synthesis technologies either restrict or enhance the concept of a computer game . the game devised is an educational game for children . a scene is displayed , typically involving a character performing an action , along with a jumbled set of words describing the scene . the player ' s task is to mentally assemble these words into a sentence , and speak this sentence to the computer . if the correct sentence is spoken , the player is so informed , and proceeds to the next level . successive levels feature an increase in sentence length , and complexity of grammar . the game is tested on a small group of children . their reactions and opinions are collated and discussed , and the success of the game is evaluated . the suitability and performance of today ' s freely available speech technology is inferred from these results , and discussed . the game is successful , test subjects attest to its enjoyability , although technological limitations are apparent . the main problem with the chosen speech recognition technology is the necessity for training of a personal voice model , without which the speech recognition performance is relatively poor .
peer - to - peer networking offers both a new and an interesting way to connect computers in an ad hoc way across the internet , but also a unique set of problems , including the relative lack of bandwidth and high latency on peer connections . these problems , combined with the fact that the decentralised networks that typify many of the current peer - to - peer products generate large amounts of traffic , pose serious barriers to the scalability of peer - to - peer networks . although some peer - to - peer networks have started to move towards packet based communication , the majority still use persistent stream based communication and various routing schemes to create networks . the pasta peer - to - peer stack is presented here , a general purpose peer - to - peer application library built on top of packet based protocols , and the merits and possibilities of packet based ( as opposed to stream based ) peer - to - peer networks analysed .
this report traces the development of a web - based tutor for com1060 , a first year module in the department of computer science at the university of sheffield . the requirements of the system are discussed and similar systems currently in use are analysed . a specification for the system is then put forward and the subsequent development process is discussed . several technologies are considered for the various components required and are then implemented into a full system . technologies covered include servlets , jsps , mysql and jdbc . the model view controller architecture is also studied in depth . the completed system is then tested and documented before a final analysis of the system takes place .
designing and developing user - friendly software is difficult . the use of human - computer interaction ( hci ) within software design and development , usability engineering and evaluation techniques all play an important part in producing usable software . this paper explores how hci and usability fit within the software lifecycle . it examines the advantages and disadvantages over traditional software methodologies such as formal methods , as well evaluating some of the techniques in the development of software . a meetings almanac manager for a computer science university department was developed using iterative prototyping and user - based interviews and observation techniques . the use of such methods concentrated the design on the needs of the users , and through feedback ensured the final system was user - friendly and implemented on time , incorporating additional functionality .
cryptography is probably best thought of as the art of secret writing ; how a message can be written in such a way so that only the intended receiver can actually read it . this dissertation seeks to give an overview of the area of cryptography , exploring various cryptographic techniques that have been used throughout history . this will be presented in the form of a web site with various examples of cryptographic techniques , as well as descriptions of many others , along with other links and resources any potential visitors may find useful . to make the examples interesting they will be in the form of applets embedded with the web pages enabling users to gain a greater insight into how the techniques work . with cryptography becoming more and more prevalent in today ' s high - tech society , this dissertation seeks to explain what exactly cryptography is , what it has been used for , and what it is used for today ; from the caesar cipher in roman times , to the enigma machine used by the nazis in world war ii , to public key cryptography used online today . most importantly of all , it is easily understandable to anyone with no knowledge of cryptography and possibly minimal computer experience .
the thought of copying another persons programming code has always been an appealing idea , with the reward of an instant saving in time and effort for the perpetrators . however , with the potential of universities and other academic establishments to unwittingly award qualifications because of such plagiarism , it is recognised that there must be an attempt to identify any culprits . this project therefore considers the detection of plagiarism that could occur between pairs of java algorithms , for example those that have been handed - in as part of a university assignment . to achieve this , two methods - one attribute counting and one structure metric , were chosen from a number reviewed . the aim was to implement these on a set of predetermined java source code files , to identify which type of method performs best , and whether or not a more concrete set of findings can be found if the two method ' s results are combined together . the report concludes that although plagiarism was able to be detected , the results are in some ways , not as desired .
biographies have always been important as a means of recording every aspect of a person ' s life . historians and academics can learn a tremendous amount about a person by what has been written about them . increasingly this information is becoming available in electronic format on the internet . databases containing many thousands of biographies are being used to power websites so that a visitor to the site has , at their finger tips access to a vast amount of biographical information . however different websites contain conflicting information , and it is time consuming and tedious for a person to trawl through all the different data sources to try and find the information that is required . the aim of this project is to provide an automated wrapper system that is capable of extracting from several information sources biographical information . by using existing software and methods that are being used in the semantic web , a system will be constructed to extract biographical information .
computer games are becoming increasingly lifelike as technology improves and graphical techniques develop . work continues into simulating real life processes that are both efficient and realistic . the aim of this project is to develop a real - time animation of a building being demolished by a large object , which can be used in computer games . the animation should maximise realism by incorporating physical ideas from the real world . there has been some work in this area which attempts to realistically animate this process using physically - based models . this study will attempt to emulate this realism but in a real - time environment . the project develops an animation system which operates on a three - dimensional wall . a piece generation algorithm is implemented which determines how a wall should be demolished based on certain input parameters . these include the force of the impact and the strength of the wall . some simple rules are implemented to decide how cracks should develop on the surface of the wall after impact . each broken piece is animated falling to the ground using some simple dynamics . the results of the study show that the techniques developed create realistic animations whilst maintaining real - time efficiency . these real - time results are compared with a physically - based demolition example , which took many hours to compute . the animation system developed in the project is shown to be on a par with the complex physical model , even though it operates in real - time . the algorithms developed are only tested on a single wall ; a complete building animation is a possibility for future research .
the teaching of speech and hearing courses is an inherently difficult thing to effectively teach successfully with such a practical subject matter . the students ' understanding of auditory concepts would be greatly improved by interactive demonstrations which encouraged them to explore key concepts freely . such a subject area is a prime candidate for distance and computer aided learning systems . the department of computer science , at the university of sheffield have already written such a system in matlab and have produced a series of demonstrations . this project is involved with reproducing these matlab based demonstrations , and producing a framework to aid in the development of such an interactive demonstration system , and producing a small number of demonstrations with said framework . in the course of producing sound components for this framework a general - purpose graphical display package , jadgraph , was created too to aid the demonstrations .
animating the ai techniques is a project to assist the students to learn more efficiently on the course ' artificial intelligence techniques ' taught in the university of sheffield . this project demonstrates the two common problem solving techniques used in ai . the techniques include the breath - first search and the depth - first search . this is applied to the water jugs problem and animated by the java applet .
this project presents the development of visual speech animation within a games engine . a brief discussion of previous work in the area is given . this is used to produce the aims of the project . the system produced by emmanuel tanguy is used as a basis for this work . thirty - one distinct visemes are produced . the visemes are used to create a number of animations . the animations are synchronised with speech within a games engine . the quality of the visemes and animations produced are evaluated through subjective testing . the successes and limitations of the system are discussed and improvements suggested .
we consider the automated assessment of a student ' s response to a short answer question . allowing a system to score a students work will require a computer to understand a students explanation of a concept relating to the topic of the question . to do this a fine grained analysis of the predicate argument structure will be needed as well as an understanding of relevant terms relating to the topic of the question . thorough research of the work done by developers researching areas similar to the paroject outline will be vital to the progress of the project , as such a full review of the papers deemed relevant will be performed . we describe a set of tools and methods for implementing such an approach into the system . a thorough implementation of the system will in due course be needed to assess the success of the system . we also describe a initial testing routine for which we can determine the overall success of the system and the project .
javaspaces have been developed by sun microsystems as an implementation of the tuple space idea in the java programming language . they have been written to provide a simple method for producing concurrent systems and do this by providing a mechanism for transferring objects between nodes on a network . however if we want to use them as a persistent object store they have a number of problems . once running javaspaces provide a simple interface to a program that wishes to access them . the three most important methods in the interface are the write , read and take methods . the write method adds an object to the javaspace , and the read and take methods get objects from back . however the structure of the javaspace lacks the complexity to maintain object pointers and provide complex searches of store objects . this project aims to investigate these problems by producing a number of pom objects , which not only solve the problems of maintaining object pointers but also provide a method for performing complex searches .
email has become one of the most widely used methods of communication , due to its speed , low cost and ease of use . however , with this medium also comes the problem of unsolicited bulk email ; the medium is just as useful for legitimate users as it is for corporations wanting to advertise . this has in turn led to the requirement that these unwanted emails should be automatically filtered to leave just the relevant ones for the user . this report attempts to find a filtering solution that is able to automatically classify email into spam and legitimate categories . the main areas of retrieval and filtering of information are reviewed with the model being based on approaches of information retrieval , an alternative to the general machine learning paradigm , also reviewed . it is hoped that this simpler and novel approach will yield just as effective classification results . the chosen benchmark corpus has been utilised on other classification problems , so cross - method comparison is possible . this comparison should determine whether or not the produced filter is effective . further investigative possibilities are put forward .
machine learning predictors were originally single networks or decision trees that were trained on all the data available . the work of several researchers , notably leo breiman , have convinced many that it is preferable to develop several predictors that learn a slightly different function , and then combine these to produce an ' ensemble '. many studies have shown that the performance of an ensemble is usually greater than a single , ' monolithic ' predictor . further evidence is presented here . ensembles created using methods such as bagging , dds and addemup are rarely perfect though , performing at less than 100 % accuracy on future instances . it can be shown , and will be , that for most applications it is preferable not to include every predictor that has been created in the ensemble , but only a select few . two methods are proposed to determine , prior to testing their generalisation ability , combinations of predictors that are preferable to the full ensemble . these are an exhaustive search and an iterative method . the results of the testing phase are quite similar , but with some modification the iterative method becomes a very good predictor of an ensemble ' s future accuracy . the proposed methodology tests the ensembles and selects the most appropriate , suggesting an ensemble that is expected to be superior to the full predictor . in the tests carried out , the improved iterative method always selected predictors that at least matched the performance of the full predictor , which justifies the added computation of the testing and selection process .
the genre of dungeon games , or first - person shooter games as they are more commonly known , has emerged over the last ten years to become one of the most popular types of computer game . at present , the levels in this type of game are generated manually , which is a very expensive and time - consuming process for games companies . if levels could be generated automatically then this would not only reduce development costs , but allow levels to be generated at run - time , giving game players a new playing experience each time a game was played and greatly improving the replayability of the game . in this project , a technique known as graph grammars will be used in order to allow descriptions of randomly generated game levels to be created automatically . as part of the project , algorithms to assess the size , difficulty and fun - value of a level will be developed , to allow individual levels to be tailored to particular requirements . this project is being undertaken in cooperation with infogrames .
the project is based upon the capabilities of the object i / o library for the functional language clean . traditionally graphical user interfaces are built with imperative languages ( this includes object - orientated style programming ) – the question is whether functional languages are better suited to this task or not . furthermore , the project will investigate the more complex features of gui generation with a functional language to see how capable the object i / o library is . to achieve this a system built in an imperative - style language ( java ) was compared with a system built in clean – the code and running program itself were compared with clean showing some impressive results . also , a second system was built to further push the object i / o library – this was also a successful demonstration of the library ' s abilities .
this project is an investigation into mobile agents and a specific application of this technology , a traffic - flow system . mobile agents represent a new approach to many distributed computing problems and are thought to be able to reduce the time and bandwidth requirements of a system that has objects distributed throughout its domains . the technology used in the investigation is the aglets development kit 1 . 1 , a java - based platform that supports agents and the systems required of agents . central to the development kit is the tahiti engine , a content engine that can be deployed on multiple different computers that are networked or multiple times on a single computer . traffic - flow simulation and modelling is a traditional subject of investigation in computing . the standard approach to modelling is to using queuing theory to model the behaviour of vehicles . this project takes a different approach , making the vehicles intelligent and giving them the power to make decisions , such as routing decisions , by themselves based on the knowledge of traffic network conditions that the system provides to them . in the project a basic , expandable traffic - flow system is created that shows how agent technology is effective in modelling in this problem domain .
this paper first introduces the main branch of mathematics that lead to the discovery of wavelets , fourier analysis . detailed accounts on the development of different fourier transforms are described , along with the motivations for an alternative transform to solve the time - frequency resolution problem . the theory of wavelet is then introduced as a solution to this problem , and the development of the wavelet is told . the second halve of the paper introduces the traditional noise removal algorithms , followed by how wavelet can be applied to digital image noise removal . alternative approaches are then considered , followed by a conclusion which summaries the paper ' s findings and a possible follow up dissertation topic .
this dissertation project presents the topic of text alignment , comparing and contrasting different approaches to the subject . there have been many different ways in which parallel texts have been and created and compared . an initial aim is to produce a detailed analysis of how they work and how they differ . one form of program design will then be approached , designed and implemented . evaluation and analysis of the results can then be cross referenced with various other techniques in order to highlight the pros and cons of the different methods . this will have reference to previously found examples .
with increasing computing power the prediction of room acoustics has become common place for all critical listening spaces such as recording studios and concert halls . many techniques have been presented for this modelling , and this dissertation focuses on just one of them – ray tracing . this was the first method employed in modelling sound spaces , and yet it has never been totally superseded as it presents the most versatile manner for the task . newer methods have tended only to be useful for particular tasks such as generating impulse responses and the like . the program will model sound reverberation characteristics of an enclosed space on any sound file , generating an output that should sound like the real space . as an extension to the basic ray tracing technique an attempt to increase the spatial aspect of generated sounds will be made . based on the application of head related transfer functions the output is processed such that when listened to through headphones a full 3 - dimensional sound field should be produced .
words in language can often represent different meanings or senses . this project employs statistical method to automatically disambiguate words amongst free text and to correctly assign the notion of sense to a word . a background to the area of word sense disambiguation is given , followed by a description of the statistical technique utililised by david yarowsky . this technique is then re - implemented using samples of novel text as the training data in order to evaluate the effect of having context specific training data on the overall disambiguation result . the algorithms and statistical methods are rewritten using perl enabling powerful text processing methods to process the large amounts of data required .
a study of speech technology and its uses and successes , alongside a brief look at computer games and in particular computer game interfaces it has been found possible to create a computer game which uses speech . it may seem improbable , if not impossible , for such a game to be created for the primary aim of usage by a visually - impaired person , but tat is exactly , what this project aims to show , in the form of a game of golf . the games success is through the fact that whilst not being the most enjoyable or successful game created , the fact that the game and interface are massively restricted by the reality that the intended users will be blind , represents significant success for the final product .
dialogue systems provide one of the few existing examples of an intelligent agent embodied in a machine that can interact with humans to carry out a variety of tasks , or simply to entertain and amuse them ( walker , 2003 ). the focus is on producing appropriate linguistic behaviour or understanding what a human agent said in a human computer dialogue . some dialogue systems are called recommender systems because they make recommendations about particular domain options . there uses area highly apparent in situations when many options are available to the user , such as places to eat in new york city or london . the aim of this project is to produce a natural language interface restaurant recommender system for the london domain , tailored to the individual user needs .
in the recent years , there has been a rekindled interest in asynchronous circuits by both academic and commercial institutions . huge amount of time and money is being spent on research . this project provides a brief introduction to the various areas of asynchronous design and explains why it is considered to be better than synchronous circuit . . the aim of this project is to build a software tool , which will allow the construction and simulation of an asynchronous circuit . the requirements of the system are first defined , followed by the interface and class design . the various stages that lead to the development of the software are discussed . the final system is then tested and evaluated , to show how it satisfies its requirements . this also discussed the problems encountered during the course of the project .
question : what does " dlnmq nwcnkqmqfbepyxkomlhnp " mean ? answer : … keep reading to find out ! have you ever wanted a helping hand solving those annoying cryptogram puzzles in the paper ? well today ' s your lucky day because the purpose of this project is to create a cryptographic tool , the crypto tool , which will allow you to encrypt and decrypt messages , and more excitingly , solve cryptograms ( both mono - alphabetic and poly - alphabetic !). the project draws on a number of design conventions and testing strategies , from various design disciplines . the final system aimed to meet its requirements . these targets were met , and then some .
this project deals with the design and implementation of extending an existing container library in c ++ to contain persistent containers , allowing for data stored in them to persist after execution of the owning process has halted . a brief look is taken at various persistence mechanisms that have been used in other languages , and then the various theoretical aspects of persistence are discussed . after this , the design of system is shown , including discussions of the various trade - offs that were made . the implementation of the library is shown , giving details of the exact approach used . finally there is a look back over the project to see how successful it was .
[None]
handling the three - dimensional meshes which often occur in computer graphics in common applications such as storage , rendering , editing and transmission , can be awkward because they are often complex and large in size . this problem has been prominent for many years and as a result various methods have been developed with the aim of easing it , some being more successful than others . as an alternative to the traditional polygonal representation of objects , subdivision surfaces allow highly detailed and perfectly smooth surfaces to be defined , and then approximated by polygons to the desired level when rendered . the maps framework proposed by lee et al uses a process of simplification followed by parameterised remeshing to produce a mesh with subdivision connectivity , which is much more suited to the common applications mentioned above . this project describes how the maps framework can be implemented . a suitable underlying data structures is chosen , and then attention is focused on a simplification algorithm which parameterises removed vertices , and a remeshing strategy that uses a combination of subdivision and vertex perturbation to produce a mesh which accurately approximates the original . simplification and subdivision algorithms have been implemented that follow these methods , and the results of applying them to a number of different models are presented .
the rapid growth of the internet in recent years has provided businesses with a new and potentially lucrative way in which to sell goods and services . however , many high profile e - commerce sites have proved to be expensive failures for a variety of reasons , which has resulted in trepidation amongst businesses and consumers . this dissertation consists of three main sections . firstly , an investigation into the key issues affecting the success of e - commerce applications , both from the aspect of the business and that of the customer . without a clear appreciation of these important points , an e - commerce developer cannot expect to develop a successful solution . there is a multitude of technologies available for the development of e - commerce systems , each with their own advantages and disadvantages . the second section of this dissertation is an in - depth study of these alternatives with a view to choosing the most appropriate technology for the basis of a successful e - commerce system . this chosen technology is then used in the third section to develop an e - commerce application to sell photographs . the development process of this system aims to test the conclusions of the first two sections , determining whether meeting the criteria set out in the first section , and using the technology chosen in the second , assists in the creation of a suitable solution .
social insects provide an interesting model for robust , distributed systems . this model does not rely on intelligence to achieve goals , but instead produces complex adaptive behaviour through cooperation between simple individuals . mobile intelligent agents , which have the ability to transfer themselves between hosts on a network and perform useful work autonomously at a remote site , are increasingly being used in a variety of applications where it is not feasible to transfer and process data in a centralised fashion . it is the aim of this project to develop a mobile agent framework which will allow the principles of social insect behaviour to be simulated and evaluated in a computerised environment . it is hoped that this approach will allow systems of mobile agents to achieve the robustness and flexibility seen in social insect communities .
as many websites are becoming increasingly complex and as such considered business critical , issues arise about how they can be verified , to make sure properties regarding reachability and usability hold . formal methods provide a rigorous way to formally verify systems . one formal method technique , temporal logic model checking , is becoming ever more popular for the verification of safety critical systems such as aircraft autopilots . temporal logic model checking requires a system to be modelled as a finite state machine , and verifies properties encoded in temporal logic . this project investigates whether model checking is suitable for the formal analysis and verification of websites . a range of properties existing in good websites have been identified , classified and formalised in temporal logic . the project presents two tools specially written for translating a website into a finite state machine - one tool for websites that use frames , and one tool for non - framed websites . a case study demonstrates how the finite state machine model of four existing sites is successfully verified in the model checking tool smv , thereby testifying to the utility of model checking when reasoning about website designs .
with the dramatic drop in the cost of computing resources , and the great expense entailed in biochemical and medical research , the feasibility and demand for digitally modelled chemical systems increases . this project will demonstrate that existing technology is mature enough to support a system enabling the user to design and simulate strongly constructive artificial chemistries using an ordinary desktop computer . such a tool has the added bonus of helping the inquisitive to understand how such chemical systems can operate , both in isolation , and as part of a wider environment of interactive subsystems . the system is provisionally titled vollenhoven1 , after a legendary rugby league player of south african origins who represented the lancastrian club of st . helens with great distinction .
graphics performance in pcs has been increasing since the release of the first 3dfx voodoo cards in 1995 . while , this increase in horsepower has allowed pcs to run graphics at an ever - increasing rate , similar gains to programming flexibility have not been experienced . this is due to a fundamental limitation in pc graphics accelerators , in that for the most part , they have been mostly fixed - function . programmable shaders are intended to give 3d application developers the ability to devise and implement a practically infinite range of visual effects . now , for the first time , consumer hardware has reached the point where it can begin implementing the basics of programmable shading similar to the renderman graphics language with real - time performance . this paper will investigate the uses and application of this new technology .
ever since the invention of the personal computer , it is always been a human dream to be able to talk to the computer and get the computer to carry out tasks . with the advancement in modern technology , humans are one step closer to achieving this goal . it is well known fact that the majority of the automated speech recognition systems ( asr ) are modelled on the human auditory system . even though these asr are modelled in this way , the majority of these systems fail to work efficiently and effectively as the human auditory systems . in particular these systems tend to perform very poorly under noisy conditions . this can be explained by the mismatch that occurs between training and application , meaning a system that is trained in a noise free environment will find it difficult to carry out recognition in a noisy environment . there are many techniques that are currently available which can rectify this problem . this project concentrates on one technique ' missing data mask '. this approach works by classifying speech into reliable and unreliable sections . once classified the reliable section is used to form a missing data mask , which then used to carry out speech recognition . currently this classification is carried out using statistical operators . this project looks into ways of extracting the relevant information from a spectrogram using image processing techniques . throughout this project a number of different image processing techniques were used to achieve this goal .
the netlab toolbox provides a set of functions to model many different types of neural network . this project adds genetic algorithm methods to this and shows how it can be used to evolve the weights of the neural networks . an attempt is made to tackle the problems encountered when crossing over neural networks encoded as real valued strings . these problems revolve around the fact that a neural networks ' functionality is based on the pattern recognition features of its hidden nodes , and splitting parts of a node up is likely to loose all of that . an algorithm to sort the nodes of all networks in a population into the same order is presented . results prove to be not as good as had been hoped for .
quadratic distortion for free - form curves , surface and solid objects is a non - linear deformation method , whereby the degree of the polynomial representation of the corresponding object is squared after distortion . the bernstein basis functions are derived depending on the dimension we work in , and using a fixed number of points on the object and to its exterior ( to describe the new object ), the distortion matrix is calculated , which when applied to the original object , deforms it quadratically . hence the aim of the project if to investigate and evaluate if this technique is successful in visualising the underlying mathematics of quadratic distortion and verifying its correctness . parametric representation techniques of bezier curves and surfaces are used to draw these free - form objects .
mobile computing is a relatively new area of research fuelled by technological developments in portable computers and wireless network technology . it is the precursor to a paradigm shift in the way we interact with computers , thorough extensions to the technology computers will move from being static devices to being integrated parts of our daily lives providing locale specific information delivered dynamically to the device by proximal wireless network access points . to enable this paradigm shift we first need to be able to automatically integrate with local networks and to be able to dynamically receive services from them without specialist knowledge or preconditions on installed software , allowing all devices to access a common wealth of services provided from the ether . in the development of this project we are trying to develop a system which moves closer to the development of the ' killer app ' for mobile computing that provides the framework to allow mobile devices to be enhanced by their surroundings , to provide that framework that supports applications enabling users to simply walk onto a train and automatically buy the appropriate ticket , without having to specially configure their portable computer . to help develop the technology that will enable the paradigm shift .
this project seeks to find ways to remove unwanted information from geographic information system ( gis ) maps . during any geographical project a vast amount of time and money is spent extracting the information from a map in order to undertake some meaningful analysis . the gis maps are created by combining many different layers each denoting geographical features for that area , such as land use , land ownership , vegetation and soil type , into a single multiplayer map containing all of the data . however , not all of the information is required , and in some cases , the unwanted information , known as linework , hides the meaningful data underneath . the rationale of this study is to investigate computational techniques to remove from the image all of the linework features , and leave only the thematic data that relates to the soil type . in this project , the current methods for removing linework are rigorously tested and critically evaluated to identify where they can be improved . in light of this analysis , new strategies have been created to remove the linework at an even higher level of accuracy than previously possible .
images require substantial storage and transmission resources , thus image compression is advantageous to reduce these requirements . the report covers some background of wavelet analysis , data compression and how wavelets have been and can be used for image compression . an investigation into the process and problems involved with image compression was made and the results of this investigation are discussed . it was discovered that thresholding was had an extremely important influence of compression results so suggested thresholding strategies are given along with further lines of research that could be undertaken .
the purpose of this dissertation is to create a program that will allow a businessperson with little knowledge of computers to create and maintain an online shopping catalogue of their products . customers should then be able to visit the site and order products online . this dissertation describes the details of the technologies that are necessary to create such a program . different possible methods are analysed and the reasons for choosing to use xml for the databases and jsp and javabeans for the internet technology are justified . the development of the main program and the website , together known as the dynacat system , is described in detail . the author and other users then critically evaluate the program , before the dissertation concludes that the program has been successful .
in order to achieve this , it was necessary to design and build a robot suitable to the tasks suggested since none were currently available , and to achieve this with small and inexpensive robots which would leave more hardware available for future students to experiment with robots . prototypes were built , tested and revised , and it soon became apparent that the main focus of the project would be that of hardware development , with the learning tasks left as a task to be attempted if time permits . after several redesigns two successful robots were developed , with a range of sensors and actuators , all controlled via specific java classes , making for a very simple development environment for anyone with a basic understanding of java . these robots will be suited to a range of robotics projects , and eliminate the lengthy construction time demands . although there was not time to develop any learning capabilities , this area was researched , and proposals have been made indicating how this could be implemented . a task has been created for the robot to attempt to tackle using learning , and it has been shown that the robot can perform this task through hard - coding , leaving room for future students to compare different learning methods or static algorithms . .
modern club djs do not want to simply play a list of songs sequentially , but aim to blend each song as seamlessly as possible into the next to preserve the flow of the music . this project looks at how previous work on music and rhythm analysis can be implemented using digital signal processing to create an automatic software dj in matlab . this report describes the experiences gained with all stages associated with the design of the engineering system , including the requirements analysis , specification , design , implementation and testing phases . the final product may be too simple for professional djs to use , however the results are encouraging and clearly illustrate the practicality and potential of such a system . the user field testing revealed that the software was very useful , especially for people who want to be a dj , but do not have the time , patience or money for the real thing . song transitions can be customised , providing room for creativity , whilst keeping user intervention to the minimum . the foundations for a good application are in place , with all of the important algorithms ( e . g . the tempo estimation , beat alignment and crossfading algorithms etc .) working with a reasonable degree of success . although there are some impressive products already available , there is still significant room for improvement in the future .
statecharts have been used for years as a means of displaying information regarding the specification and design of real life complex systems . the semantics of statecharts has to be rich enough to support different styles of modelling , yet simple and intuitive . the standard format for these diagrams to be displayed in is a flat two dimensional structure . this report proposes to extend upon ideas presented in [ 6 ] & [ 7 ] and to come up with new ways of visualising the information , with the overall goal of improving understanding and readability of the statecharts using the third dimension . research in the area has led to a number of ways in which the standard statechart can be extended into three dimensional space . these include parallel threads , different values of input , depth and importance and synchronisation . using a combination of java technologies and opengl a system was created to allow statecharts to be displayed and manipulated in a 3d environment in order to improve the ease with which they are visualised .
voicexml is a new programming language that makes internet information and content accessible via voice and phone . by combining state - of - the - art speech technology with the flexibility and portability of xml , voicexml has the potential to change the way in which we access the world - wide web . in this dissertation , a nuance - based speech recognition system is specially adapted so that it can function over a telephone line . this is then is setup in collaboration with a voicexml document server in order to produce a complete system for hosting over - the - phone speech applications . the system is evaluated using a series of isolated word and continuous speech tests . in the second part of the project , a tool is implemented that allows rapid creation of voicexml applications . the tool , visual vxml , allows users who are unskilled in programming disciplines to construct complex speech applications by virtue of an easy to use visual interface .
a report on the development of a regression testing tool written in java . the report begins with an introduction of the project and the contents of a literature review done on the information surrounding the idea of regression testing and the kinds of technology and background that would be useful while creating the testing tool . within the introduction is a detailed break down of the ideas behind regression testing and a brief summery of the aims of the project and what would finally be produced . most importantly the introduction indicates the idea of a three - section structure to the testing tool . the main content of this report is based upon the four stages that were viewed as having been important within the projects lifespan . the first of these stages is the analysis phase , and discussed within the analysis chapter is a brief summary of what was done to create the requirements capture document . this document formed the basis of the analysis phase and included the project boundaries and requirements capture tables that formed the beginning of the work done to produce the testing tool . the third chapter within this report covers the details that were discerned from the design phase , in which there is a brief discussion of the types of design strategy that were considered , a break down of the styles of packages available for use within the informal design specification method , and then a detailed view of the work done within the design phase that allowed for a complex specification of the testing tool to be developed . following the design phase is a chapter on the implementation phase of the project . the implementation chapter contains information on the actions taken during the coding and development of the testing program . the implementation program continues with the idea of a three - section program , and is therefore broken down into the development of these three sections , and the work that was done on them . the implementation chapter indicates the difficulties that were encountered during the implementation phase , and also some of the technology that had to be omitted from the testing tools implementation due to time restrictions . the final chapter on the production of the testing tool is the testing phase chapter . this discusses the results of a series of tests , don by both the developer of the testing tool and an external aid . for all four chapters there is also a brief evaluation of the methods used and the results and documentation made for them . following the main body of the report there is an additional final evaluation of the testing tool . contained within this section is a brief summary of each of the four previous chapters , and a review of the testing tool as a whole . it includes a few views on what use the final product would be and how well it fulfils the project description . this review of the project is continued within the conclusion , which also includes a series of ideas of what will occur to the testing tool after the publication of this report , and a few ideas for further studies or projects based upon what was discussed and created for this project .
this thesis follows the development of a system for visualising java source code ; java ' s object oriented nature provides an interesting challenge in developing such a system , as the static structure of an object oriented program does not necessarily relate to the dynamic nature of that program ' s execution . visualising source code on screen provides many benefits for the user of the system , and can prove to be advantageous in circumstances such as debugging and designing source code as well as teaching the concepts of object oriented programming to a novice programmer . the unique nature of java and its ability to reflect on its own source code as well as its capabilities to run other programs at runtime and intercept object instantiations and method calls , provides a very interesting technology . it is this technology that is the centre point of the visualisation system , and it is developed in this dissertation in order that the technology and its advantages can be exploited by future java developers .
paper journals are costly thus invariably involve subscriptions to read , and charges to publish articles . electronic journals can provide an answer ; by reducing the costs of publication , articles can be offered free of charge to readers and published freely by authors . however , to gain the necessary respect from the stakeholders involved , work must be reviewed by peers and approved before being published , a process that often adds cost to the publication . this project aims to implement an electronic journal unique from any other in the way that authors ' pay ' for the publication of their work by conducting reviews on others '. this aims for a win - win situation where readers can access articles free of charge , these articles will be of guaranteed quality , and the author will have not been charged for their submission . an account of the research into the field of web development and existing electronic journals is given , before moving onto the analysis of the problem leading to a detailed design . a discussion of the implementation of the journal is then presented along with and account of the methods used in testing . finally , an evaluation of the work completed is given with some suggestions for further development and conclusions drawn from the project .
the purpose of evolution is to change over time , whether it is a change in a creature ' s behaviour to adapt to surroundings , or the growth of sharp teeth over millions of years . the principle of natural selection is very important in ensuring that the properties of the ' best ' candidates get chosen to pass through the generations , while weaker characteristics don ' t propagate through to future children . can we utilise the mechanisms of evolution to create three - dimensional sculptures , starting with simple random shapes and increasing complexity through further generations ? this will involve the use of human selection , because there is no scientific way to choose which structure is necessarily ' best '. however , through identification of genes which control the sculptures ' shapes , and by breeding and mutation of these genes over time , we will be able to see definition relationships between the shape of sculptures from one generation to the next , hopefully with more complex designs as we progress . the end result will be a system of creation and evolution , from which the user will be able to initiate and guide the evolution of three - dimensional , organic - looking sculptures .
a summary is a short description of what has been stated in a conversation or text . creating and more importantly , evaluating a summary is a very useful process . a reader can gain an understanding of a long document by only analysing a small part of it . one particular application of this would be the summarisation of news broadcast so that a user could learn about the stories of the day without having to watch or listen to an entire broadcast . the maximal marginal relevance ( mmr ) method is a mechanism for locating the most relevant parts of a text while trying to reduce redundancy ; it can be used to produce summaries . this project looks into the creation of summaries via the mmr method in regards to summarising news broadcasts with the goal of evaluating and suggesting improvements for the method .
the theory of machines module covers difficult and intense material ; it relies heavily on students understanding basic mathematical concepts and ideas . therefore , there is a need to test such knowledge and indicate problematic areas . this project aimed to achieve this by introducing an online quiz for the module . the development of which involved evaluating existing quizzes , analysing the results and applying the findings . the system has been implemented successfully using java server pages , javabeans , servlets and a mysql database , and provides current students with the opportunity to assess their own understanding of the module .
this paper introduces some of the best - known encryption techniques and goes on to detail methods successful in breaking these techniques . the aim of this is to highlight that many systems believed to be secure are not necessarily secure , and , for those that were secure when first invented , the current progress of technology has changed this . a method capable of automatically deciphering messages produced using the mono - alphabetic substitution cipher is designed and implemented , with the outcome of a fully working and successful system . one that actually exceeds the requirements originally laid out during its initial conception . this system differs from many similar techniques in that it avoids the use of statistical analyses . though successful , it appears obvious that this system could still benefit from the inclusion of such techniques ; suggesting a hybrid method combining the two approaches . a number of improvements are suggested for the system and the possibility of extending this method further , to include the solution of poly - alphabetic substitution cryptograms , is examined .
" artificial life is the study of man - made systems that exhibit behaviours characteristic of natural living systems . it complements the traditional biological sciences concerned with the analysis of living organisms by attempting to synthesize life - like behaviours within computers and other artificial media ." throughout years in research on the artificial chemistry field , it has been agreed that artificial chemistry involves the development of software tools that simulate artificial life phenomena . more precisely , such system can be defined by sets of objects and reaction rules which specify how the objects interact . the project focuses on artificial chemistries in terms of biological processes , where there is a direct relationship between artificial and real life on the bio - molecular , object and reaction , interaction level . in the first part , the project gives an analytical overview of the " dna translation and protein assembly " biochemical process , along the following three application domains : modelling , information processing and optimization . in the second part of the project , a tool that simulates the former process is presented . the tool is mainly responsible for the simulation and graphical representation of mechanisms related with both the translation of dna sequences into sequences of amino acids and the process of proteins assembly in their tertiary structure under several mutation cases and input modifications .
this report is concerned with the generation of a graphical interface , which is to be used in conjunction with a language called xmdl ( x - machine description language ). although it is an essentially straightforward task , the process of creating such an interface taps on several different subject areas , all of which have to be taken into account if the end product is to be considered a success . this report aims to provide all of the background knowledge required by a reader who may not be familiar with x - machines and the technologies derived from them . it aims to guide the reader throught the designing , implementing and testing phase of the project , giving a clear , transparent account of any considereations faced and any resulting decisions taken . it also serves as an ideal case study demonstrating how x - machines can be used to design and test java - based software .
lith printing is a photographic technique that overexposes negatives onto special purpose paper . lith prints are becoming increasing popular because their visual appearance is considered to be : " beautiful , different and personal . its flexibility offers almost limitless ways of interpreting a negative " [ tr98 ]. but , there are various problems associated with lith printing . it is difficult to duplicate results , materials are expensive , the process is time consuming , and unwanted chemical reactions can affect print quality . the purpose of this project is to design a computer simulation that will teach users the principles of photographic development and the techniques required to create a lith print , also to provide a solution to the problems defined above . the emulation of the lith process will need to incorporate an interactive model representing the darkroom environment ( such as the photographic equipment , chemical solutions , etc .) and an algorithm that is capable of generating a realistic lith image . the implementation of the lith printing simulation will use the principles of object oriented programming , usability engineering , human - computer interaction and 2d image processing techniques , amongst other concepts , to produce the final application . to determine the performance of the simulation , and ultimately its potential for teaching the aspects of lith printing , the product will be subjected to extensive user testing throughout the software development process .
component libraries are a very important part of programming languages flexibility . for languages that have been in use for many years such as c ++ there are many diverse libraries created by many distinct uses . even so there are a number of properties that they all have in common . all libraries must provide containers that can hold any object that the user wishes . efficiency of these collections is also very important , as these are often the basis for more complex programs . this project looks at library design and in particular the design of the standard template library ( stl ) and simon ' s component library ( scl ) with a view of extending the components already in the scl . three containers , chain table , red black tree and avl tree have been created and the important design issues are discussed along with any complex algorithms they rely upon . the efficiency of these algorithms is then outlined with any improvements that could be implemented in the future .
the dissertation describes a project to write a pattern matching library that uses regular expressions and supports character types for dealing with new standards of globalised character sets . this is done with the overall aim of rewriting the existing , flawed regular expression implementation in the c library for the gnu operating system . the dissertation describes the background concepts , the design of the library , its implementation and the testing of this implementation . the background that is covered includes regular expressions , finite automata , and the unix world .
since the accidental development of a unique reusable glue , the ubiquitous little yellow postit ® notes have become a household name . used around the home and the workplace to leave messages and reminders , whether personal or to others , they have proved to be a huge success . the aim of this project is to investigate and implement an electronic version of the postit ® note , which can be incorporated into web pages , enabling visitors to leave messages which can be viewed by all other visitors to the site . ever since the internet became popular among home users , bulletin or message boards have been a familiar addition to a website . these functions allow visitors to leave messages to others viewing the site , and also reply to previous messages . this facility , however , is constrained to a dedicated section or page of the site . the postit ® system gives a visitor to a site almost an unlimited capability in sending feedback , wherever they feel it is necessary .
file compression is one of the most active areas of research in the computing industry , with the quest to find a manner of providing succinct representations of data fields providing programmers with a wealth of new algorithms . as these increasingly diverse and efficient methods for performing compression are implemented , it becomes desirable to have a single tool that can be used to handle many data formats . currently available tools are highly dependent on either a particular operating system or processor , and do not provide support for many of the new formats , such as bzip2 . this project intends to address these limitations , and provide a flexible , platform independent tool that provides access to a multitude of compression and archive formats .
constant colour matting is a technique widely used within the television and movie industries . this technique consists of filming an object or actor in front of a uniformly coloured screen and then digitally replacing the background . the screen is usually blue , although green and even red are becoming more popular for certain uses . using this technique , scenes that would be impossible to film with an actor and scenes which would be too dangerous are now possible . this project aims to produce a tool to provide a quick and simple method of constant colour matting . this is to be used with data from the clemson university audio - visual experiments ( cuave ) database . one of the most important aims of this tool is to be quick at matting , unlike film studios , researchers do not have a great deal of time to wait for test data to be produced . still , in view of this , a successful composite must still be produced . this project will look at canny edge detection and k - means clustering as methods to produce a successful matte . first the image must be segmented into three sections , foreground , background , and an unknown . then the pixels in the unknown region are assigned to the foreground and background using a profile of these regions built using k - means clustering .
as e - mail becomes on of the most widely used methods of communication , with it ' s ease of use , speed and low cost it has also become the target of advertisers . just like " snail " mail , e - mail has become prone to junk e - mail but unlike " snail " mail the cost of distributing unsolicited junk e - mail to vast numbers of people is relatively cheap . this has lead to the desire for these unwanted e - mails to be filtered with quickly and with relative ease . in this report existing approaches to information retrieval are explored in an attempt to find a filter solution to deal with the problem of unsolicited junk e - mail . all retrieval models are considered but the vector space model , highly successful in the information retrieval field is chose as the basis for and automatic e - mail filtering program . the system is then evaluated using an extensive range of junk e - mail and shown to be a viable method of filtering .
the aim of this dissertation was to design and implement an automatic text summariser that takes a text file as input . it then produces an extract summary containing the sentences that are deemed to give the most representative view of the text as a whole . several sentence scoring techniques were used to create the automatic summaries and the results of each were evaluated and compared with an ideal solution that was formulated from the results of several manually created , human summaries for each source text that was used . evaluation was carried out using recall and precision measures to determine the level of success of the program as a whole and the individual scoring schemes that can be used within it . the results of the automatic summaries were then compared with a base case of randomly selected sentences in order to determine how successful each was at identifying relevant texts within the document . the results were surprisingly accurate , as the random set was significantly beaten for all papers .
a very common approach when implementing an algorithm such as the rsa algorithm is to do it in a mathematical straightforward way . implementations created in this manner incur a significant performance penalty . in this paper a description of the rsa algorithm is presented alongside with other evaluated algorithms that improve its performance . experimental results show that performance can be greatly increased with the use of the more cmplex algorithms .
art has always proved a difficult subject to attempt to quantify or measure in any rigorous fashion . the creation of an artistically pleasing image is often viewed as needing a conscious intelligence to undertake this process . however , a few artists have made attempts to analyse their work and present their findings ; piet mondrian and his rules for neo - plastic art were the subject of the previous year ' s dissertation in the same subject . this year the work of the swiss painter , paul klee has been chosen as the focus , whose output included detailed analysis of his own artistic methods and his attempts at communicating them to his bauhaus students . also included is the work of harold cohen , in the the thirty year development of his drawing system , aaron . this dissertation attempts to distill a set of rules from the various sources that may be individually applied to create an image that can be called , " art ". these rules will be executed by an intelligent agent system , where each agent applies one of the rules to the image . the implementation of this system has resulted in an extensible framework , into which more agent - based implementations of the rules may be added .
the paper introduces mathematical models that are widely used to simulate antenna propagation . the analysis focuses on the simple thin - wire antenna model using pocklington ' s type itegrodiffie rential equation which is derived from the electric field integral equation ( efie ). the equation uses the method of moments [ 9 ] ( mom ) as to discretize the problem into a system of linear equations and enable a solution to the integral using computers . wavelets are then introduced with a motivation for such a technique and solutions are presented using wavelets as to improve computation run - time and efficiency of the solution . two main techniques are applied to the mom , one being the application of a fast wavelet transform to the impedance matrix produced by traditional method of moments [ 4 ]. the other being the direct application of wavelets as a basis when expanding or discretizing the function using the method of moments when applied to a boundary value problem ( bvp ).
the testing process is considered one of the most important stages within the development cycle of software due to the necessity for error - free functionality . the type of testing required for a software product can vary significantly and to account for this there are many different testing methods available . considering the increase in automation of everyday tasks in previous years it is of no surprise to find the field of software testing incorporating automatic testing . out of the two main types of testing , one lends itself particular well to the upgrade of automation , this being one of a structural nature . structural methods are concerned with the inner workings of program code , identifying specific areas such as statement loops . whether these statements are tested or not gives rise to the concept of coverage measures , these giving indications to the tester of how much of the software has been exercised . this project introduces an automated tool for structural software testing , incorporating all aspects of the manual method from instrumenting code through to computing coverage measures . the success of the software and its development life - cycle are presented within this report .
speech synthesis is a rapidly evolving field of research in computer science . loosely , it involves the simulation of human speech by computers . the most commonly cited application for this is text - to - speech synthesisers , which allow a computer to take a text and " read it aloud ," and this has already been implemented on a basic level . however , making synthesised speech sound intelligible and realistic is a complex problem . this project aims to demonstrate an important method commonly used in speech synthesis : psola ( pitch - synchronous overlap and add ). the psola method allows concatenation of different speech waveforms in such a way that the prosody ( rhythm , pitch and intonation ) can be altered along the way , thus allowing more flexible speech synthesis and therefore more realistic - sounding speech . the way in which i will demonstrate this method is via a matlab demo which will belong to a family of speech & hearing demos known as mad ( matlab auditory demonstrations ). in this project , i have implemented a demonstration of the psola method in the style of the existing mad demonstrations . i have explored several different aspects of the method within this and shown it to be a viable way of producing synthesised speech . in this report , i shall present an in - depth account of the demonstration , its various features and the factors that i considered in its creation .
extreme programming ( xp ) is a programming methodology , which places great emphasis on testing . it is widely accepted that making these tests automated is highly beneficial . whilst there exits tools to automate unit tests ( programmer tests ), there are no tools to automate functional tests ( customer tests ). traditionally testing an application doesn ' t guarantee the identification of every fault . however using a method known as ' total testing ', and given certain per - conditions exist , testing can be proven to find all faults . this project attempts to use the work on ' total testing ' to create a testing tool ( fjunit ) that will allow the generation , analysis and execution of functional tests . fjunit was successfully implemented and able to diagnose a number of errors throughout the development . in addition to the benefits of fault detection the use of this testing method produces a proven system specification .
there exists a large amateur community of writers and programmers who would like to be able to turn their game concepts into reality , but are unwilling to devote the time necessary to code a game engine from scratch . the aim of this dissertation is to produce a " toolkit " of java classes designed to aid a programmer with some working knowledge of java in producing a simple text - based adventure game . such games are common - however , the engines used to drive them are not open source . acknowledgements go to the designers of games such as adventure and zork for establishing the text - based adventure game framework , which my dissertation emulates .
the minimoog is an early analogue synthesiser . due to its mixture of easily editable parameters and good quality sounds , the minimoog became a highly desirable performance synthesiser . unfortunately , the original minimoog is no longer commercially available . there are however , many high - end copies but many of these are out of the price range of performing musicians . the aim of this project is to simulate the minimoog synthesiser on a computer . this would allow the distinctive sound of the minimoog to be accessible to a wider audience . the simulation will be implemented in the java programming language , and the project will also serve as a test for the feasibility of real - time audio synthesis in java
natural language processing has been studied for several decades now , but it is still considered difficult to interact with a computer just through natural language . the aim of this project was to develop a game in which the user would enter commands in english and the system would respond to these commands . the system has to be able to understand when given different words that mean the same thing , and when the words are in a different order . it is impractical to program in every different way to give the command . a fairly limited world was chosen so there were not too many different words to understand , and the parser was implemented using a state machine model . the game produced is text only with a report of what happens appearing after each command . the game has been tested minimally by entering in 20 sentences and 80 % of these returned a correct reply . none of the sentences returned an incorrect response , the remaining 20 % said that the system did not know what to do . the system requires further testing to gain a better idea of how well it performs .
xml ( the extensible mark - up language ) is a language similar to html in that it allows data to be represented by enclosing information between tags . however , xml differs from html because the data in xml documents is enclosed between user - defined tags . this allows for a great deal of flexibility in designing the format of the document ; the xml structure can be specifically tailored to suit the data in each individual document . there are databases , particularly in the field of biology , that are sparsely populated , i . e . they contain lots of null fields . the flexibility of the xml structure allows these null fields to be omitted from the data ; only those fields that actually contain data need to be included in the xml document . the project describes the development of a translator that will translate data from the text formats used by applications such as microsoft access and excel , into xml documents . the translator will encode the source data efficiently in xml documents with the objective of reducing the amount of disk space taken up by the data , whilst maintaining its original structure .
visualising forces and their effects in the real world can be difficult to imagine and so this project aimed to utilise the benefits of computer simulation to aid in the education of the motion of rigid bodies . this report follows the production of an educational computer program . it outlined the relevant material on visualisation , on simulation , on the relevant physics , and on the creation of a physically based simulation . the requirements , design and implementation of the program were described and discussed . the physically based simulation included basic implementation of an improved euler ' s method - based integration system and a simple method for collision detection and newtonian - based response . the visualisation methods implemented were the depiction of linear and angular velocity , linear and angular acceleration , forces , and moments . a system of 3d arrows was used to represent magnitude and direction of the visualised components . other effects applied included the plotting of movement and the rendering of a ticker - tape - esque trail . the success of the project was partly deduced from a survey that evaluated the educational benefit of the completed program .
motion capture is rapidly becoming one of the most popular animation tools available , by providing much quicker and more effective results than more traditional techniques . it enables human characters in particular , to perform much more realistic motions . motion capture is the process of capturing live motion events and transforming them into high - density mathematical data . its main applications lie in entertainment industries . the emergence of motion capture has lead to the production of increasingly realistic animations in games such as ea sport ' s fifa soccer 2002 , and ' blockbuster ' movies such as monsters ' inc . however , motion capture data files are not without their drawbacks ; the main two of which are the size of a motion capture data file and the cost incurred in producing it . the technique of motion capture merging aims to provide a solution to these problems . the data from two data files can be merged to produce a number of intermediate motion capture sequences , depending on the ratio used to merge the files . this study investigates the motion splicing technique . using two sets of motion capture files , a particular limb from one motion capture character is merged with the body of another . this technique is being investigated as opposed to full character merging which can be computationally much more expensive . the results of the project demonstrate the effectiveness of selectively merging different body parts from one motion capture character with those of another . the study shows that motion capture splicing is a very useful tool in the development of computer animations . its adaptability and ease of implementation make it a very powerful motion capture merging technique . the study concludes that the advantages that effective motion capture splicing can bring to the world of computer animation are unquantifiable .
maurits cornelis escher was a graphic artist fascinated by regular plane filling and the representation of infinity within a finite medium . with the intention of emulating some of escher ' s work this report analyses that work , and escher ' s rules for regular plane filling . escher ' s own rules are taken and used as a software specification for a program that allows a user to create their own tessellations , based upon escher ' s own insights .
without compiling writing software , the difficult pattern matching of the input can easily be prone to errors , coupled with the translation to the target language , a software tool option is the way to ensure correct matching . compiler writing software needs to understand the language which it is matching ; this is done in two stages . first of all the words or tokens of the language are defined using regular expressions and then using grammar rules the language syntax is also defined . compilers are complex pieces of software , with many stages and at each step there are many concerns to think about . current compiler technologies , language technology , and all the stages are reviewed and analysed in order to understand the problem further . yacc is the oldest and still the most widely used compiler writer tool available . the aim of this project is to create a java version of it . java has been chosen as it is considered by some to be the new hope for computing , a language rich with benefits and programs are portable over many platforms . the design has been mainly experimental , borrowing styles and ideas from patterns which are useful ways to link classes , which is part of object orientated language , of which java is a prime example . discussion of problems with the program , along with possible solutions could lead to improvements , such as the slow speed of execution .
humans detect objects in visual scenes with astounding performance and no real effort . computers on the other hand , though able to process information much faster , seem unable to perform even the most elementary object detection tasks without wasting huge amount of effort . furthermore all research for computer vision has been directed towards creating simple programs that only portray specific aspects of algorithms . thus the aims of this project are : a ) to create a modular framework for computer vision and b ) to implement a state of the art algorithm for face detection , notably [ 1 ]
the aim of this project is to make a football pitch look aged / used / worn both as an individual games progresses and as a season progresses . to achieve this time - based texture mapping based on knowledge of where a pitch wears will be employed . a layering technique that splits the pitch into constituent components has been employed . with tiles of grass and mown - effect textures multiplied together and then added to a texture that holds the lines of the pitch . two layers of alpha values are then used to merge the pitch with a soil texture to simulate wear and tear . the three - dimensional graphics library to create 3d graphics , opengl , provides some functionality for blending layers but the technique that was settled upon uses pointers to the textures in memory to merge them and then the new textures are loaded and used in opengl . this technique provides ultimate control over manipulating the layers of the pitch model .
detecting infringement in musical pieces can lead to large financial gains for the owner of the original work . record companies set departments up whose sole job it is to clear samples for use . even still , samples will often be used without permission and consequently lawsuits are filed , and ultimately , royalties forfeited . with new musical formats such as mp3 files , and internet sites such as napster ; now , more so than ever , it is of great importance to record companies to protect their works . global piracy costs the recording industries over $ 4 billion a year [ bbc ], a significant proportion of which is attributed to illegal distribution of mp3 files over the internet . this report looks into ways of detecting infringements in mp3 files by looking for similarities between features extracted from the songs , such as pitch , tempo , mel - frequency cepstral coefficients etc . it details how to obtain such features and discusses aspects of the program developed which attempts to detect similarities based on these features . the program allows files to be compared using the various different features using a technique known as dynamic time programming and deals with two different ways that could be used for spotting infringements based on varying circumstances . the program was tested under a variety of conditions and obtained mixed results ; these results are discussed in the report along with methods that could be implemented to improve the programs success . the paper also looks at the practicalities and limitations of the methods undertaken in the project and how the program would fair in a real world situation .
a system is proposed which will recognise and authenticate facial images using the eigenfaces technique , this system was constructed and then used to determine whether recognition or authentication is more secure in an access control situation . the eigenfaces technique is described in detail along with details of the other aspects of the system such as image manipulation and normalisation which can help the system to achieve better results . the system is tested with a variety of facial images and the results are discussed with the results being that the system built is accurate to an extent but has room for improvement . the conclusion drawn from the implementation and testing of the system is that authentication is a much more secure choice than recognition in access control . the problems with the system are discussed along with their possible solutions and areas of expansion for the system in the future .
using the internet , it is now possible for users to access large amounts of information in a very short amount of time . however , as users have come to expect information at great speed , they are often unwilling to spend a great deal of time looking for a particular piece of information . users can easily become frustrated when they have to wade through large amounts of information that is not relevant to them . users need to be able to effectively move to the information that they want . the aim of this project is to measure in a quantitative way the effectiveness of various navigation aids , when applied to a web - site . the study shall attempt to find out which system provides greater ease of use for users . put more precisely , during this project , the following hypothesis shall be put to the test : users will find a navigation system that only gives them selected information based on their location easier to use than a system that forces them to browse the page in order to navigate , or one that gives them all the information on the site without discretion .
speech and hearing is an area of research that is developing very quickly . in recent years , the automated speech recognition devices have improved dramatically . just a few years ago , the task was to enable asr devices to recognise vocabulary sizes of thousand words , today the vocabulary size rages at about 100 , 000 words . however , there are still problems overcome , and especially under noisy conditions one can expect the devices to have an error rate as high as 70 % word error rate . word error rate is the standard evaluation tool in the field . however , especially under noisy conditions with its high error rate , the wer faces problems , which may become substantial to its results . this document will assess another fo rm of evaluation measure based on the concepts of information theory . this measure , called the ril , is defined as h ( x | y )/ h ( x ). it eliminates a whole range of problems of the wer , but also introduces new problems . these problems could be solved by supplying additional information to the ril . a new performance measure , called the wil , is featuring an approximation of the possible solution . the potentials of the concept of the ril will be assessed using this approximated measure wil and compared to the other performance measures assessed . further , this document will look at the potentials of using a stochastic edit distance as the basis for evaluation as opposed to the commonly used viterbi edit distance . the stochastic edit distance can be applied as an input into the ril , which is then called the rilstochastic . similarly , the ril using the viterbi edit distance is called rilviterbi . a technique , generally used in asr device training , is used to derive the stochastic edit distance . the results are presented on a set of simple examples , enabling us to develop the assumptions , and also on a case study , an evaluation of an asr device results in the aurora framework .
a report on development of a java functional testing toolkit for software testing with category partition method , the process involve creating a test specification from the software • s specification and from that to generate test frames which will finally lead to creation of a test script ( a set of input data ) that could be used to test the program when it is reaches its testing stage . this project • s main aim is to implement a tool that would simplify and automate the process . the main content of this report is based upon four stages that were viewed as having been important within the projects lifespan . the first stage is the analysis phase ; the analysis chapter is a brief summary of how the requirement document was being captured , it also features the project boundaries and requirement document which provided the foundation for the actual tool . all these are supported with rich literature reviews which are detailed in the second chapter . the third chapter covers the details that were discerned from the design phase , in which there is a brief discussion of different design strategy that were considered , a break down of packages that is available for use within the informal design specification method , and then a detailed view of the work done within the design phase that allowed for a complex specification of the testing tool to be developed . the implementation chapter contains information on the actions taken during the coding and development of the testing program . the implementation program had been divided into two stages and therefore will be described in two sections and the work that was done on them . the implementation chapter indicates the difficulties that were encountered during the implementation phase , and also some of the technology that had to be omitted from the testing tools implementation due to time restrictions the next chapter on the production of the testing tool is the testing phase chapter . this discusses the results of a series of tests , done by the developer of the testing . the final evaluation chapter is a brief summary of each of the four previous chapters , and a review of the testing tool as a whole . it includes a few views on what use the final product would be and how well it fulfils the project description . this review of the project is continued within the conclusion , which also includes a series of ideas of what will occur to the testing tool after the publication of this report , and a few ideas for further studies or projects based upon what was discussed and created for this project .