{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from rouge import FilesRouge\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow version\n",
    "print(tf.__version__)\n",
    "\n",
    "logging.basicConfig(level=\"error\")\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/content/gdrive/My Drive/\"\n",
    "# os.chdir(path)\n",
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./\"\n",
    "topic_vocab = os.path.join(output_dir, \"topic_vocab\")\n",
    "abstract_vocab = os.path.join(output_dir, \"abstract_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints\")\n",
    "log_path = os.path.join(output_dir, 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178\n"
     ]
    }
   ],
   "source": [
    "# Load input dataset\n",
    "topic = []\n",
    "dirPath = \"../msc_dataset/topic/\"\n",
    "\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        topic.append(fp.read())\n",
    "\n",
    "dirPath = \"../ug_dataset/topic/\"\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        topic.append(fp.read())\n",
    "print(len(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2178\n"
     ]
    }
   ],
   "source": [
    "# The corresponding labels\n",
    "abstract = []\n",
    "dirPath = \"../msc_dataset/abstract/\"\n",
    "\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        abstract.append(fp.read())\n",
    "        \n",
    "dirPath = \"../ug_dataset/abstract/\"\n",
    "files = [f for f in os.listdir(dirPath) if os.path.isfile(os.path.join(dirPath, f))]\n",
    "files = sorted(files)\n",
    "for fname in files:\n",
    "    if (\"txt\" not in fname):\n",
    "        continue\n",
    "    with open(dirPath+fname, \"r\", encoding='utf-8') as fp:\n",
    "        abstract.append(fp.read())\n",
    "print(len(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "data_size = len(topic)\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = data_size - train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Cellular Genetic Algorithms for Test Generation', shape=(), dtype=string)\n",
      "tf.Tensor(b'Currently software testing is a crucial and labor-intensive component of software production. Software testing requires test data generation with good code coverage. Automated search techniques used for software test generation will be an improvement to efficiency and cost to software testing, and currently Genetic Algorithms are one the best options as a search heuristic for automated test generation.      The project will investigate in Cellular Genetic Algorithms, another subclass of Evolutionary Algorithms just like Genetic Algorithms, and their potential to replace Genetic Algorithms for test generation with even better code coverage. Research and implementation were done in regards to how these techniques may be applied to test generation, and specifically how using Cellular GAs may improve upon using standard GAs for test generation. The tests were done with the software tool Evosuite which runs a GA on Java classes to generate test-suites and outputs the GA\\xc3\\xa2\\xc2\\x80\\xc2\\x99s performance on code coverage, and a Cellular GA will be implemented into EvoSuite in this project and be compared to the standard GA in its performance for generating test data.      Results achieved showed general favor for cellular GAs due to the advantage of creating more diverse populations than standard GAs and also prevent premature convergence of solutions that more often converged at a local optimum instead of branching out searching for optimal solution for the standard GA.', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'An Online Portal for the Selection and Allocation of Dissertation Projects', shape=(), dtype=string)\n",
      "tf.Tensor(b'The allocation of students and staff to projects in the Department of Computer Science is a complicated procedure, not helped by its lack of centralisation. It relies heavily on emails between staff and students, and uses a number of online systems for different aspects of the process. This carries an inherent lack of transparency.\\n\\nThis project focuses on producing an online portal to consolidate the management of project allocation, allowing each user to access a personalised overview of their stages in the process. By replacing the number of systems currently in use with one single portal, the project aims to simplify the process for all those involved.\\n\\nThrough a literature review, and through several consultations with members of the Department, a set of requirements was identified and a new system produced. The result was a product of sufficient quality and flexibility to replace the existing systems, and the foundation for a portal to manage the lifecycle of projects in the Department.', shape=(), dtype=string)\n",
      "----------\n",
      "tf.Tensor(b'Pomorum: A Website Educating Primary Children through Experiential Methods', shape=(), dtype=string)\n",
      "tf.Tensor(b\"Invertebrates make up 95% of the species on this planet, and yet their importance is constantly overlooked. From shellac to birth control hormones, invertebrates are used in many unexpected places.\\n\\nTo encourage a life-long interest in and respect of invertebrates, this project aims to create a website which will teach primary aged children about the different sorts of invertebrates which we encounter in everyday life and the ways in which they change the world around them.\\n\\nBy using David Kolb's Theory of Experiential Learning to teach children more effectively, Pomorum.co.uk aims to be educational and fun, whilst also teaching them to see invertebrates in a new light. Including elements from the UK national curriculum brings Pomorum more relevance in schools, and hopefully turn it into a website which can be used in classrooms around the country.\", shape=(), dtype=string)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Transfer data list to tensor\n",
    "examples = tf.data.Dataset.from_tensor_slices((topic, abstract))\n",
    "\n",
    "# Split training data and testing data\n",
    "train_examples = examples.take(train_size)\n",
    "test_examples = examples.skip(train_size)\n",
    "\n",
    "for top, abt in test_examples.take(3):\n",
    "    print(top)\n",
    "    print(abt)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load vocabulary： ./topic_vocab\n",
      "vocabulary size：4597\n",
      "load vocabulary： ./abstract_vocab\n",
      "vocabulary size：8190\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_topic = tfds.features.text.SubwordTextEncoder.load_from_file(topic_vocab)\n",
    "    print(f\"load vocabulary： {topic_vocab}\")\n",
    "except:\n",
    "    print(\"no vocabulary, start creating\")\n",
    "    # Create a custom subwords summary tokenizer from the training dataset.\n",
    "    tokenizer_topic = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "                              (top.numpy() for top, abt in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_topic.save_to_file(topic_vocab)  \n",
    "print(f\"vocabulary size：{tokenizer_topic.vocab_size}\")\n",
    "\n",
    "try:\n",
    "    tokenizer_abstract = tfds.features.text.SubwordTextEncoder.load_from_file(abstract_vocab)\n",
    "    print(f\"load vocabulary： {abstract_vocab}\")\n",
    "except:\n",
    "    print(\"no vocabulary, start creating\")\n",
    "    # Create a custom subwords news tokenizer from the training dataset.\n",
    "    tokenizer_abstract = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "                              (abt.numpy() for top, abt in train_examples), target_vocab_size=2**13)\n",
    "    tokenizer_abstract.save_to_file(abstract_vocab)\n",
    "print(f\"vocabulary size：{tokenizer_abstract.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a start and end token to the input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(to_t, ab_t):\n",
    "    # tokenizer_summ.vocab_size for the <start> token\n",
    "    # tokenizer_summ.vocab_size + 1 for the <end> token\n",
    "    to_indices = [tokenizer_topic.vocab_size] + tokenizer_topic.encode(\n",
    "                                    to_t.numpy()) + [tokenizer_topic.vocab_size + 1]\n",
    "    ab_indices = [tokenizer_abstract.vocab_size] + tokenizer_abstract.encode(\n",
    "                                    ab_t.numpy()) + [tokenizer_abstract.vocab_size + 1]\n",
    "    return to_indices, ab_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(to_t, ab_t):\n",
    "    # force 'su_t' and 'ne_t' to Eager Tensors by py_function\n",
    "    return tf.py_function(encode, [to_t, ab_t], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To keep this example small and relatively fast, drop examples with a length of over MAX_LENGTH tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 700\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_max_length(to, ab, max_length=MAX_LENGTH):\n",
    "#     return tf.logical_and(tf.size(to) <= max_length, tf.size(ab) <= max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_examples  # output：(Summary, News)\n",
    "                 .map(tf_encode) # output：(Summary index sequence, News index sequence)\n",
    "#                  .filter(filter_max_length)\n",
    "                 .cache() # speed up\n",
    "                 .shuffle(BUFFER_SIZE) # random dataset\n",
    "                 .padded_batch(BATCH_SIZE, # padding to same length for each batch\n",
    "                               padded_shapes=([-1], [-1]))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)) # speed up\n",
    "\n",
    "test_dataset = (test_examples\n",
    "               .map(tf_encode)\n",
    "#                .filter(filter_max_length)\n",
    "               .padded_batch(BATCH_SIZE, \n",
    "                             padded_shapes=([-1], [-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "  \n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "  \n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    # tf.cast change data type\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the future tokens in a sequence\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    \n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "  \n",
    "    Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "    # scale matmul_qk\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)  # get seq_k sequence length\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # scale by sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "    # weighted average\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
    "\n",
    "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially assign the output dimension 'd_model' & 'num_heads'\n",
    "# output.shape            == (batch_size, seq_len_q, d_model)\n",
    "# attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # how many heads divided by d_model\n",
    "        self.d_model = d_model # base dimension before split_heads\n",
    "    \n",
    "        assert d_model % self.num_heads == 0\n",
    "    \n",
    "        self.depth = d_model // self.num_heads  # new dimension for each head\n",
    "    \n",
    "        self.wq = tf.keras.layers.Dense(d_model) \n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "        self.dense = tf.keras.layers.Dense(d_model)  # linear transformation after concatenating heads\n",
    "  \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "  \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "    \n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "        # divide 'd_model' into 'num_heads' depth\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # concatenate 'num_heads' depth to original dimension 'd_model'\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) \n",
    "        # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  # two linear transformations for input, add ReLU activation func\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
    "\n",
    "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # one for sub-layer, one for layer norm\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        # one for sub-layer, one for layer norm\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # sub-layer 1: MHA\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        # sub-layer 2: FFN\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked multi-head attention (with look ahead mask and padding mask)\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        # Multi-head attention (with padding mask). \n",
    "        # V (value) and K (key) receive the encoder output as inputs. \n",
    "        # Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, combined_mask, inp_padding_mask):\n",
    "        # all sub-layers output: (batch_size, target_seq_len, d_model)\n",
    "        # enc_output is Encoder output sequence: (batch_size, input_seq_len, d_model)\n",
    "        # attn_weights_block_1: (batch_size, num_heads, target_seq_len, target_seq_len)\n",
    "        # attn_weights_block_2: (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "\n",
    "        # sub-layer 1: Decoder layer\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, combined_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "        # sub-layer 2: Decoder layer focuses the Encoder final output\n",
    "        # (batch_size, target_seq_len, d_model) (V, K, Q)\n",
    "        # attention weights: the importance given to the decoder's input based on the encoder's output\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, inp_padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "        # sub-layer 3: FFN\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The `Encoder` consists of:\n",
    "1.   Input Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N encoder layers\n",
    "\n",
    "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    # - num_layers: how many EncoderLayers\n",
    "    # - input_vocab_size: transfer index to vector\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # (input_dim, output_dim)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
    "    \n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape == (batch_size, input_seq_len)\n",
    "        # all layer output: (batch_size, input_seq_len, d_model)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :input_seq_len, :]\n",
    "\n",
    "        # combine embedding and positional encoder, and regularization\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,  rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(target_vocab_size, d_model)\n",
    "    \n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "    \n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "      \n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, rate)\n",
    "        # FFN output logits number, represent the probability passing softmax\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "  \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "        # Decoder output pass the last linear layer\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 4599\n",
      "target_vocab_size: 8192\n"
     ]
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_topic.vocab_size + 2\n",
    "target_vocab_size = tokenizer_abstract.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # comput cross entropy of all position, but not sum up\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask  # only compute the loss of non <pad> position\n",
    "  \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # the default warmup_steps = 4000\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported after Tensorflow 2.0.0-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_models = [128, 256, 512]\n",
    "# warmup_steps = [1000 * i for i in range(1, 4)]\n",
    "\n",
    "# schedules = []\n",
    "# labels = []\n",
    "# colors = [\"blue\", \"red\", \"black\"]\n",
    "# for d in d_models:\n",
    "#     schedules += [CustomSchedule(d, s) for s in warmup_steps]\n",
    "#     labels += [f\"d_model: {d}, warm: {s}\" for s in warmup_steps]\n",
    "\n",
    "# for i, (schedule, label) in enumerate(zip(schedules, labels)):\n",
    "#     plt.plot(schedule(tf.range(10000, dtype=tf.float32)), \n",
    "#            label=label, color=colors[i // 3])\n",
    "\n",
    "# plt.legend()\n",
    "\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer has 4 Encoder / Decoder layers\n",
      "d_model: 128\n",
      "num_heads: 8\n",
      "dff: 512\n",
      "input_vocab_size: 4599\n",
      "target_vocab_size: 8192\n",
      "dropout_rate: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"Transformer has {num_layers} Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load the up-to-date checkpoint, the model has already trained 321 epochs.\n"
     ]
    }
   ],
   "source": [
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff_{train_size}data\"\n",
    "\n",
    "checkpoint_dir = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_path, run_id)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=20)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n",
    "    print(f'load the up-to-date checkpoint, the model has already trained {last_epoch} epochs.')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print(\"No checkpoint, start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    # use Adam optimizer for updating parameters of Transformer\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    # the loss and training acc recorded on TensorBoard\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0812 09:23:02.160223 123145330012160 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0812 09:23:02.166064 123145330012160 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0812 09:23:02.188828 123145330548736 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0812 09:23:02.192239 123145330548736 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0812 09:23:02.208520 123145330012160 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer has already trained 321 epochs.\n",
      "Saving checkpoint for epoch 322 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-322\n",
      "Epoch 322 Loss 0.3782 Accuracy 0.2953\n",
      "Time taken for 1 epoch: 1081.7677969932556 secs\n",
      "\n",
      "Saving checkpoint for epoch 323 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-323\n",
      "Epoch 323 Loss 0.3708 Accuracy 0.2968\n",
      "Time taken for 1 epoch: 632.9080617427826 secs\n",
      "\n",
      "Saving checkpoint for epoch 324 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-324\n",
      "Epoch 324 Loss 0.3683 Accuracy 0.2953\n",
      "Time taken for 1 epoch: 555.6895523071289 secs\n",
      "\n",
      "Saving checkpoint for epoch 325 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-325\n",
      "Epoch 325 Loss 0.3636 Accuracy 0.2966\n",
      "Time taken for 1 epoch: 569.1449019908905 secs\n",
      "\n",
      "Saving checkpoint for epoch 326 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-326\n",
      "Epoch 326 Loss 0.3603 Accuracy 0.2967\n",
      "Time taken for 1 epoch: 566.8436710834503 secs\n",
      "\n",
      "Saving checkpoint for epoch 327 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-327\n",
      "Epoch 327 Loss 0.3562 Accuracy 0.2972\n",
      "Time taken for 1 epoch: 553.7288808822632 secs\n",
      "\n",
      "Saving checkpoint for epoch 328 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-328\n",
      "Epoch 328 Loss 0.3577 Accuracy 0.2967\n",
      "Time taken for 1 epoch: 573.6750218868256 secs\n",
      "\n",
      "Saving checkpoint for epoch 329 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-329\n",
      "Epoch 329 Loss 0.3572 Accuracy 0.2963\n",
      "Time taken for 1 epoch: 849.211000919342 secs\n",
      "\n",
      "Saving checkpoint for epoch 330 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-330\n",
      "Epoch 330 Loss 0.3530 Accuracy 0.2971\n",
      "Time taken for 1 epoch: 741.4333708286285 secs\n",
      "\n",
      "Saving checkpoint for epoch 331 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-331\n",
      "Epoch 331 Loss 0.3514 Accuracy 0.2978\n",
      "Time taken for 1 epoch: 586.6634659767151 secs\n",
      "\n",
      "Saving checkpoint for epoch 332 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-332\n",
      "Epoch 332 Loss 0.3509 Accuracy 0.2976\n",
      "Time taken for 1 epoch: 571.6755290031433 secs\n",
      "\n",
      "Saving checkpoint for epoch 333 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-333\n",
      "Epoch 333 Loss 0.3483 Accuracy 0.2982\n",
      "Time taken for 1 epoch: 559.9690089225769 secs\n",
      "\n",
      "Saving checkpoint for epoch 334 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-334\n",
      "Epoch 334 Loss 0.3464 Accuracy 0.2982\n",
      "Time taken for 1 epoch: 582.9144678115845 secs\n",
      "\n",
      "Saving checkpoint for epoch 335 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-335\n",
      "Epoch 335 Loss 0.3461 Accuracy 0.2988\n",
      "Time taken for 1 epoch: 568.4478459358215 secs\n",
      "\n",
      "Saving checkpoint for epoch 336 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-336\n",
      "Epoch 336 Loss 0.3466 Accuracy 0.2983\n",
      "Time taken for 1 epoch: 599.4591951370239 secs\n",
      "\n",
      "Saving checkpoint for epoch 337 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-337\n",
      "Epoch 337 Loss 0.3454 Accuracy 0.2986\n",
      "Time taken for 1 epoch: 450.1846537590027 secs\n",
      "\n",
      "Saving checkpoint for epoch 338 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-338\n",
      "Epoch 338 Loss 0.3454 Accuracy 0.2986\n",
      "Time taken for 1 epoch: 448.4402720928192 secs\n",
      "\n",
      "Saving checkpoint for epoch 339 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-339\n",
      "Epoch 339 Loss 0.3444 Accuracy 0.2984\n",
      "Time taken for 1 epoch: 445.9321777820587 secs\n",
      "\n",
      "Saving checkpoint for epoch 340 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-340\n",
      "Epoch 340 Loss 0.3419 Accuracy 0.2995\n",
      "Time taken for 1 epoch: 457.69838213920593 secs\n",
      "\n",
      "Saving checkpoint for epoch 341 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-341\n",
      "Epoch 341 Loss 0.3440 Accuracy 0.2987\n",
      "Time taken for 1 epoch: 468.20322704315186 secs\n",
      "\n",
      "Saving checkpoint for epoch 342 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-342\n",
      "Epoch 342 Loss 0.3427 Accuracy 0.2989\n",
      "Time taken for 1 epoch: 436.4304790496826 secs\n",
      "\n",
      "Saving checkpoint for epoch 343 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-343\n",
      "Epoch 343 Loss 0.3394 Accuracy 0.2998\n",
      "Time taken for 1 epoch: 446.74187207221985 secs\n",
      "\n",
      "Saving checkpoint for epoch 344 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-344\n",
      "Epoch 344 Loss 0.3392 Accuracy 0.2999\n",
      "Time taken for 1 epoch: 442.22090888023376 secs\n",
      "\n",
      "Saving checkpoint for epoch 345 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-345\n",
      "Epoch 345 Loss 0.3367 Accuracy 0.3003\n",
      "Time taken for 1 epoch: 473.2319128513336 secs\n",
      "\n",
      "Saving checkpoint for epoch 346 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-346\n",
      "Epoch 346 Loss 0.3370 Accuracy 0.3001\n",
      "Time taken for 1 epoch: 469.3579659461975 secs\n",
      "\n",
      "Saving checkpoint for epoch 347 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-347\n",
      "Epoch 347 Loss 0.3380 Accuracy 0.3002\n",
      "Time taken for 1 epoch: 449.39703011512756 secs\n",
      "\n",
      "Saving checkpoint for epoch 348 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-348\n",
      "Epoch 348 Loss 0.3351 Accuracy 0.3005\n",
      "Time taken for 1 epoch: 459.82070994377136 secs\n",
      "\n",
      "Saving checkpoint for epoch 349 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-349\n",
      "Epoch 349 Loss 0.3359 Accuracy 0.3003\n",
      "Time taken for 1 epoch: 465.1148188114166 secs\n",
      "\n",
      "Saving checkpoint for epoch 350 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-350\n",
      "Epoch 350 Loss 0.3324 Accuracy 0.3013\n",
      "Time taken for 1 epoch: 434.75997495651245 secs\n",
      "\n",
      "Saving checkpoint for epoch 351 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-351\n",
      "Epoch 351 Loss 0.3327 Accuracy 0.3011\n",
      "Time taken for 1 epoch: 494.0753240585327 secs\n",
      "\n",
      "Saving checkpoint for epoch 352 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-352\n",
      "Epoch 352 Loss 0.3344 Accuracy 0.3006\n",
      "Time taken for 1 epoch: 517.3666069507599 secs\n",
      "\n",
      "Saving checkpoint for epoch 353 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-353\n",
      "Epoch 353 Loss 0.3330 Accuracy 0.3016\n",
      "Time taken for 1 epoch: 504.385822057724 secs\n",
      "\n",
      "Saving checkpoint for epoch 354 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-354\n",
      "Epoch 354 Loss 0.3315 Accuracy 0.3017\n",
      "Time taken for 1 epoch: 401.65735483169556 secs\n",
      "\n",
      "Saving checkpoint for epoch 355 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-355\n",
      "Epoch 355 Loss 0.3298 Accuracy 0.3019\n",
      "Time taken for 1 epoch: 388.6924033164978 secs\n",
      "\n",
      "Saving checkpoint for epoch 356 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-356\n",
      "Epoch 356 Loss 0.3285 Accuracy 0.3023\n",
      "Time taken for 1 epoch: 390.01584100723267 secs\n",
      "\n",
      "Saving checkpoint for epoch 357 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-357\n",
      "Epoch 357 Loss 0.3315 Accuracy 0.3012\n",
      "Time taken for 1 epoch: 386.34501600265503 secs\n",
      "\n",
      "Saving checkpoint for epoch 358 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-358\n",
      "Epoch 358 Loss 0.3301 Accuracy 0.3013\n",
      "Time taken for 1 epoch: 385.40035796165466 secs\n",
      "\n",
      "Saving checkpoint for epoch 359 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-359\n",
      "Epoch 359 Loss 0.3286 Accuracy 0.3018\n",
      "Time taken for 1 epoch: 450.62011194229126 secs\n",
      "\n",
      "Saving checkpoint for epoch 360 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-360\n",
      "Epoch 360 Loss 0.3269 Accuracy 0.3027\n",
      "Time taken for 1 epoch: 460.0043749809265 secs\n",
      "\n",
      "Saving checkpoint for epoch 361 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-361\n",
      "Epoch 361 Loss 0.3262 Accuracy 0.3026\n",
      "Time taken for 1 epoch: 427.4310851097107 secs\n",
      "\n",
      "Saving checkpoint for epoch 362 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-362\n",
      "Epoch 362 Loss 0.3270 Accuracy 0.3026\n",
      "Time taken for 1 epoch: 386.1876790523529 secs\n",
      "\n",
      "Saving checkpoint for epoch 363 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-363\n",
      "Epoch 363 Loss 0.3275 Accuracy 0.3025\n",
      "Time taken for 1 epoch: 405.3390290737152 secs\n",
      "\n",
      "Saving checkpoint for epoch 364 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-364\n",
      "Epoch 364 Loss 0.3262 Accuracy 0.3025\n",
      "Time taken for 1 epoch: 386.64969205856323 secs\n",
      "\n",
      "Saving checkpoint for epoch 365 at ./checkpoints/4layers_128d_8heads_512dff_1742data/ckpt-365\n",
      "Epoch 365 Loss 0.3289 Accuracy 0.3018\n",
      "Time taken for 1 epoch: 388.50095224380493 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6711fc30508b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 589\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EPOCHS = 500\n",
    "print(f\"Transformer has already trained {last_epoch} epochs.\")\n",
    "#print(f\"rest epochs：{min(0, last_epoch - EPOCHS)}\")\n",
    "\n",
    "\n",
    "# writing information on TensorBoard\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "\n",
    "epoch = last_epoch\n",
    "while(1): \n",
    "    start = time.time()\n",
    "  \n",
    "    # reset TensorBoard metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        \n",
    "        #print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          #        epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    # save checkpoint for each epoch\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    # loss and accuracy showed on TensorBoard, supported on Tensorflow 2.0.0-beta\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch + 1)\n",
    "        tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch + 1)\n",
    "  \n",
    "    print('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n",
    "    \n",
    "    epoch += 1\n",
    "    if (train_loss.result() < 0.1):\n",
    "        break\n",
    "\n",
    "# 2187 data\n",
    "# 150 epoch -> 29 hours\n",
    "# Epoch 151 Loss 0.7426 Accuracy 0.2098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x237577748>]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(322, 366):\n",
    "    x.append(i)\n",
    "print(x)\n",
    "y = [0.3782, 0.3708, 0.3683, 0.3636, 0.3603, 0.3562, 0.3577, 0.3572, 0.3530, 0.3514, 0.3509, 0.3483, 0.3464, \n",
    "     0.3461, 0.3466, 0.3454, 0.3454, 0.3444, 0.3419, 0.3440, 0.3427, 0.3394, 0.3392, 0.3367, 0.3370, 0.3380, \n",
    "     0.3351, 0.3359, 0.3324, 0.3327, 0.3344, 0.3330, 0.3315, 0.3298, 0.3285, 0.3315, 0.3301, 0.3286, 0.3269, \n",
    "     0.3262, 0.3270, 0.3275, 0.3262, 0.3289]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(x, y, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2374a40b8>]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(322, 366):\n",
    "    x.append(i)\n",
    "print(x)\n",
    "y = [0.2953, 0.2968, 0.2953, 0.2966, 0.2967, 0.2972, 0.2967, 0.2963, 0.2971, 0.2978, 0.2976, 0.2982, 0.2982, \n",
    "     0.2988, 0.2983, 0.2986, 0.2986, 0.2984, 0.2995, 0.2987, 0.2989, 0.2998, 0.2999, 0.3003, 0.3001, 0.3002, \n",
    "     0.3005, 0.3003, 0.3013, 0.3011, 0.3006, 0.3016, 0.3017, 0.3019, 0.3023, 0.3012, 0.3013, 0.3018, 0.3027, \n",
    "     0.3026, 0.3026, 0.3025, 0.3025, 0.3018]\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(inp_sentence):\n",
    "    start_token = [tokenizer_topic.vocab_size]\n",
    "    end_token = [tokenizer_topic.vocab_size + 1]\n",
    "  \n",
    "    inp_sentence = start_token + tokenizer_topic.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # <start> token\n",
    "    decoder_input = [tokenizer_abstract.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)  # increase batch dimension\n",
    "  \n",
    "    # auto-regressive，generate one word at each time, and add into Transformer\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # create ner masks when generating a new word\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "  \n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: , -1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        # return the result if the predicted_id is equal to the <end> token\n",
    "        if tf.equal(predicted_id, tokenizer_abstract.vocab_size + 1):\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "        # concatentate the predicted_id to the output which is given to the decoder as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generation(sentence):\n",
    "    predicted_seq, _ = generation(sentence)\n",
    "\n",
    "    # filter <start> & <end> tokens and turn back english tokens\n",
    "    target_vocab_size = tokenizer_abstract.vocab_size\n",
    "    predicted_seq_without_bos_eos = [idx for idx in predicted_seq if idx < target_vocab_size]\n",
    "    predicted_sentence = tokenizer_abstract.decode(predicted_seq_without_bos_eos)\n",
    "\n",
    "    print(\"sentence:\", sentence)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"predicted_seq:\", predicted_seq)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"predicted_sentence:\", predicted_sentence)\n",
    "    \n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    word_punct_tokenizer = WordPunctTokenizer()\n",
    "    tokens = word_punct_tokenizer.tokenize(text)\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(path, text):\n",
    "    with open(path, 'w', encoding='utf-8') as fp:\n",
    "        fp.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Cellular Genetic Algorithms for Test Generation\n",
      "2 An Online Portal for the Selection and Allocation of Dissertation Projects\n",
      "3 Pomorum: A Website Educating Primary Children through Experiential Methods\n",
      "4 Tracking Athletes Indoors Using an iOS Application and Low Energy Bluetooth Devices\n",
      "5 Using A Bio-Inspired Algorithm to Solve Multiple Travelling Salesman Problems with Online Mapping Integration\n",
      "6 Exploring Human Generosity Using Social\n",
      "                    NetworkingWebsite Designs and Techniques\n",
      "7 Autonomous Cloud Management\n",
      "8 Investigating the use of Image Classification to Aid Text\n",
      "            Summarisation\n",
      "9 The TWITTERATI: The Predictive Potential of Sentiment Analysis and Twitter\n",
      "10 Automatic Translation from State Machines to\n",
      "                    ObjectModels\n",
      "11 Building a video game in Logisim\n",
      "12 A Java Library for Internet Connectivity on\n",
      "                    Blu-ray\n",
      "13 The Domain Specificity of Sentiment Lexicons: an Investigation\n",
      "14 SkyBet: Real-Time Football Team Performance Monitoring\n",
      "15 Using an Neural Network as a Default Policy in\n",
      "                    UpperConfidence Bounds for Trees Search\n",
      "16 Computational Modelling of Galaxy Formation using\n",
      "                    FLAMEGPU\n",
      "17 Can a robot teach?\n",
      "18 BPEL translation into DFA\n",
      "19 Telepresence using the Nao robot\n",
      "20 Paro Bot and Loneliness\n",
      "21 PDA To CFG Tool\n",
      "22 XML Olympic\n",
      "23 COM3600: Research Project Building a Computer in Minecraft\n",
      "24 Building a Deep Learning Agent for Battleship\n",
      "25 Endocytosis Animator\n",
      "26 Scarab - A Simulation of the Evolution of Reactive\n",
      "                    Controllers in Robots\n",
      "27 Subject Segmentation Of Speech Recognised Data\n",
      "                    Using Self Similarity Analysis\n",
      "28 INVESTIGATING HOME AUTOMATION\n",
      "29 From 2D Photographs to 3D Caricatures\n",
      "30 Army Ant Raiding Simulation\n",
      "31 Linguistic Style and Personality of Dialogue Agents;\n",
      "32 Real-Time Rendering Of Realistic Landscape\n",
      "33 Differencing, Merging and Version Control of Pages\n",
      "                    on the World Wide Web\n",
      "34 Theory of Mind and Attribution of Intelligence\n",
      "35 MOTIVE Object specification tool\n",
      "36 L-system Ecologies\n",
      "37 Digitising Historical Maps\n",
      "38 An Exploration Of The World Of Touch In Virtual Environments\n",
      "39 INTERACTIVE WEB TUTOR FOR COM2020 ABSTRACT DATA\n",
      "                    TYPES\n",
      "40 Demolishing Objects in Computer Games\n",
      "41 SOUND RENDERING IN A VIRTUAL ABBEY\n",
      "42 Automatic News Summarization System\n",
      "43 USER MODELLING ON A MOBILE DEVICE\n",
      "44 An abstract muscle model for three dimensional facial animations\n",
      "45 Automatic retrieval and summarization of Radio and\n",
      "                    TV News Broadcasts\n",
      "46 Wrapper Induction and Automatic Annotation to Online Protein Discovery Resources\n",
      "47 An Automatic Text Summariser\n",
      "48 A Face Morphing Tool for Psychological Research\n",
      "49 Multimedia Message (MMS) Composer\n",
      "50 A Configurable XML Database for Archaeologist\n",
      "51 Cross platform and multimedia messaging\n",
      "52 Particle Filters\n",
      "53 Computer-based Speech Training and Automatic\n",
      "                    Speech Recognition\n",
      "54 Initiation of a Cardiac Arrest\n",
      "55 INTERNET VOTING FOR STUDENT ELECTIONS\n",
      "56 Speech Based Adventure Role Play Game\n",
      "57 Simulating Groups\n",
      "58 Dynamic Building Plan Generation\n",
      "59 Statechart Test Set Generation Tool\n",
      "60 Intelligent Multi-Agent Systems in Games\n",
      "61 The Theory of AI in Role Playing Games\n",
      "62 Web Based Query & Retrieval of Audio Data\n",
      "63 Machine Learning Approaches to Phrase Chunking\n",
      "64 Intonex - French pronounciation exercises for English-speaking students\n",
      "65 The Use of Java Spaces as Storage Mechanism for Travel Data\n",
      "66 Producing Automatic Biographical Summaries from Web Documents\n",
      "67 JavaSpaces For Multimodal Data\n",
      "68 Experimenting to Produce a Software Tool\n",
      "                    forAuthorship Attribution\n",
      "69 A MATLAB demonstration of Independent Component\n",
      "                    Analysis\n",
      "70 C4.5 Rule Preceded by an Artificial Neural Network\n",
      "                    Ensemble for Medical Diagnosis\n",
      "71 CAL and Gas Turbines\n",
      "72 Modelling Text for Search and Retrieval\n",
      "73 A Real-Time Physics Simulator for Jenga™\n",
      "74 An Interface For Programming Hunter Robots\n",
      "75 THE AUTOMATED EXTRACTION OF BIOGRAPHICAL\n",
      "                    INFORMATION INTO A STRUCTURED FORM\n",
      "76 Gaussian Process Latent Variable Model GTM and a\n",
      "                    Linear derivation of the GTM\n",
      "77 TIME SERIES ANALYSIS\n",
      "78 Building Translation Lexicons for Proper Names\n",
      "                    from the Web\n",
      "79 Layered-Goal Architecture as an approach to\n",
      "                    producing humanlike behaviour in a complex game\n",
      "                    environment\n",
      "80 Classical Information Retrieval Models: Comparison for QA System\n",
      "81 Video Annotation tool in Java\n",
      "82 PostIt ® Notes on Web Pages as an Aid to CSCW\n",
      "83 A web database for student practical marks\n",
      "84 L-System Ecologies\n",
      "85 A Generic Simulator for Quad-Rotor Unmanned Aerial Vehicles\n",
      "86 Building Maps Using Reactive Robotics\n",
      "87 A Graphical Simulator for the PI-Calculus\n",
      "88 Animating Artificial Intelligence Techniques\n",
      "89 Animating Artificial Intelligence Techniques\n",
      "90 Modelling Co-operative Retreival of Prey by Ants\n",
      "91 Recognition of Eyes based on Chinese Face Reading\n",
      "                    Techniques\n",
      "92 Robotic Pet\n",
      "93 Web Search Engine Question Answering\n",
      "94 Automatic Detection of Plagiarism\n",
      "95 Artificial Evolution of an Intelligent Exploration Robot\n",
      "96 Machine Learning Techniques for Part-of-Speech\n",
      "                    Tagging\n",
      "97 A Java Functional Testing Toolkit\n",
      "98 A prototype, simultaneous localization and mapping\n",
      "                    hybrid robot architecture\n",
      "99 Co-Evolution in a Predator-Prey Environment\n",
      "100 Automated Language Identification\n",
      "101 Shot detection in digital video\n",
      "102 Web Database for Student Practical Marks\n",
      "103 Group Cooperation\n",
      "104 An Automatic Biographical Web Page Summariser\n",
      "105 Modelling the Multi-Agent Paradigm\n",
      "106 Lexical Mining from the Web\n",
      "107 Adaptive algorithms for autonomous robotics\n",
      "108 Named Entity Identification from Meetings\n",
      "109 A Model of a Contracting Heart Cell\n",
      "110 Web Services for Distributed Database Applications\n",
      "111 Television Watching Assistant\n",
      "112 The Virtual Molecule Viewer\n",
      "113 A Cracking Algorithm for Exploding Objects\n",
      "114 Predicting Football Results\n",
      "115 French CD-ROM “Correspondance de Flora Tristan”\n",
      "116 Automatic Acronym Identification and the Creation of an Acronym Database\n",
      "117 Computerised Determination of Disputed Authorship\n",
      "118 Discovery Method CASE Tool - Task Flows and State Models\n",
      "119 An Automatic Text Summarizer\n",
      "120 Calculating The Correct Time\n",
      "121 James Carpenter XML Search Engine\n",
      "122 Archaeological Data and the RAVE\n",
      "123 Neural Network Method for Time Series Prediction\n",
      "124 Slippage Management: A Web-based Application For The Construction Industry\n",
      "125 Visualising Anatomical Models of the Heart\n",
      "126 CAL and Gas Turbines\n",
      "127 A Mechanical Model of Electricity for Teaching Students\n",
      "128 Vis3d Visualising Multidimensional Data in 3d\n",
      "129 An e-commerce approach to research information management\n",
      "130 Innovative Applications in Web-Based Retrieval\n",
      "131 Computerised Determination of Disputed Authorship\n",
      "132 An Accessibility Web browser\n",
      "133 Real-Time Rendering of Fur\n",
      "134 An Automatic Lecture Note Taker\n",
      "135 An Investigation Into The Condition Numbers Of Two Surfaces That Are Almost Tangential\n",
      "136 Commercial Distributed Processing Using JavaSpaces™\n",
      "137 CHILDREN'S STORY READER\n",
      "138 Extracting Metadata from Musical Audio\n",
      "139 Reactive Robot Navigation by Infra-Red Signpost\n",
      "140 WLMH-5: CAMP - A Pair Programming Support Environment\n",
      "141 JFLAP-X: Extending an Graphical Automaton Tool\n",
      "142 Designing Courseware in a Professional Issues Course for Software Engineers\n",
      "143 Multi-Agent Models; OO approaches\n",
      "144 ROBOTICS SIMULATOR\n",
      "145 A Discovery Method CASE Tool: Task Structure and Narrative Modelling\n",
      "146 The Queue Server - Investigating limited hardware to achieve maximum usability\n",
      "147 Pattern formation in sea shells\n",
      "148 Online Student Questionnaire\n",
      "149 Recognising Faces in Images\n",
      "150 Event Timelines in Broadcast News\n",
      "151 Reactive Robotics - Ping-Pong Ball Collector\n",
      "152 Snooker Highlights\n",
      "153 Autonomous Open Day Guide Robot\n",
      "154 Investigation in XP Techniques Applied to the Software Hut\n",
      "155 Wear And Tear Transformations\n",
      "156 A Software Based Modular Synthesiser in Java\n",
      "157 Audio Indexing and Recognition\n",
      "158 ‘Do you think you are a good programmer’\n",
      "159 An Interactive Tutor and Debugging Environment for Haskell\n",
      "160 A P_SYSTEM TOOL\n",
      "161 An API for Hands-Free Gesture-Based Human-Computer Interaction\n",
      "162 MULTIMODAL GENERATION FOR GIVING DIRECTIONS\n",
      "163 Building a Physically Accurate Robot Simulation\n",
      "164 Voice Access To Form-Based Internet Services\n",
      "165 A Java API for Genetic Programming\n",
      "166 Email Filtering Tool\n",
      "167 Online Politics Data store\n",
      "168 Sentence Compression\n",
      "169 HAIFA : An Interoperability Framework for Haskell\n",
      "170 Talker Characterisation for Performance Prediction\n",
      "171 A Digital Theremin\n",
      "172 ACO Visualisation in Virtual Reality\n",
      "173 Evolving an Effective Ensemble\n",
      "174 DoctoralStudents.com Bibliographic Reference System\n",
      "175 An Evaluation of online Machine Translation\n",
      "176 Virtual Reality Townscaping Project\n",
      "177 Sheffield Ego Entertainment and Going Out Dialogue System\n",
      "178 EDS - Electronic Diary System\n",
      "179 Problem Decomposition\n",
      "180 Graphical Representation in the MISTRES Project\n",
      "181 A System for Visual Examination of Bones for Use in Forensic Pathology\n",
      "182 Stable Object-based Model of Traffic Control System\n",
      "183 Hocus Focus\n",
      "184 An Automatic Text Summarizer\n",
      "185 X-Plane Space race\n",
      "186 Testing through Interception of Communication\n",
      "187 Topic Detection in Broadcast News\n",
      "188 Real-Time Signing\n",
      "189 Automated Database Documentation System\n",
      "190 Using Programmable Hardware to Render and Animate Fish\n",
      "191 Initiation of Cardiac Arrest\n",
      "192 From a 2D Image to a 3D Form\n",
      "193 A portal for e-Commerce using Wrapper\n",
      "194 A Computational Model of Bladder Function\n",
      "195 3D Sound Scene Creator for the Reflex Lab\n",
      "196 An Automated Story Generator of the Old French Epic\n",
      "197 Faces, Expressions and Likeness in movement\n",
      "198 Automatic Detection of Plagiarism\n",
      "199 Creating Procedural Walkthroughs\n",
      "200 Windows XP Firewall\n",
      "201 An Automatic Document Summarizer\n",
      "202 Video Pan and Zoom Analysis\n",
      "203 Automatic Summarisation of Audio\n",
      "204 The SheffieldFun Project - A self-optimising spoken dialogue manager\n",
      "205 Litle Lost Spaceship\n",
      "206 3D Fertilisation Biology\n",
      "207 Developing a Restaurant Recommender System\n",
      "208 Object-Oriented Models of Heart Cells\n",
      "209 Audio Based Snooker Shot Indexing\n",
      "210 Audio Expressive Visual Speech - A Child's Storyteller\n",
      "211 A Personalised Web Monitor\n",
      "212 A bayesian dominoes player\n",
      "213 self asseembly\n",
      "214 Biometric-Computer Interface (BCI)\n",
      "215 virtual extras\n",
      "216 Information Extraction System using Simple\n",
      "                    Techniques\n",
      "217 Implementing Sequence Diagrams within the WinCASE\n",
      "                    Framework\n",
      "218 An Immersive Environment for Granular Synthesis\n",
      "219 Question Answering from a Large Text COllection\n",
      "220 Manipulation of Sound In 3D\n",
      "221 Wrapper Implementation for Information Extraction from House Music Web Sources\n",
      "222 3D Sculpting Using Voxels with an exploration into interfaces\n",
      "223 Molecules and Reflex\n",
      "\n",
      "224 Archaeological Data and the RAVE\n",
      "225 Graphical Simulation of Fire\n",
      "226 Predicting Football Results\n",
      "227 An Interactive Diagram Designer\n",
      "228 Real Time Droplet Animation on a Glass Pane\n",
      "229 Voice Controlled BBC News Archive Retrieval System\n",
      "230 Agents and Artistic Creation\n",
      "231 Retrieving and Disambiguating Pictures from the Web\n",
      "232 Further Development Towards an Autonomous UAV\n",
      "233 A Bayesian Dominoes Player\n",
      "234 Snooker Shot/Break Speed Estimator.\n",
      "235 Visualization of Large Bases of Data Extracted from the Web\n",
      "236 Eclipse Platform Integration of Jester - the JUnit test tester\n",
      "237 Voice Morphing\n",
      "238 Real-time Routing in Traffic Control Networks\n",
      "239 The M#Xtreme Matrix Manipulation Language Extension for C#\n",
      "240 Modelling Boids\n",
      "241 Creative Uses of Information Extracted from SMS Messages\n",
      "242 WebUndergrad\n",
      "243 Visualization of Large Bases of Data Extracted\n",
      "                    from the Web\n",
      "244 Evaluation Functions for Word Sense Disambiguation\n",
      "245 Synthetic Social Interactions with a Robot using\n",
      "                    the BASIC personality model\n",
      "246 A Karaoke style feedback system for an isolated\n",
      "                    digit speech recogniser\n",
      "247 Expressions of the eyes: A detailed, animatable\n",
      "                    model\n",
      "248 X-Machine Modelling Of Disease Spread In a\n",
      "                    Population\n",
      "249 Evaluation of online MT Systems\n",
      "250 Blink rate estimation\n",
      "251 An Excel / Open Office Macro Translator\n",
      "252 An XML Minutes Database\n",
      "253 A Java Tool for Transformation-Based Learning\n",
      "254 Detecting Repeated Patterns in Audio\n",
      "255 An Application Tracking System\n",
      "256 Real Time Smoke Simulation on Graphics Hardware\n",
      "257 Automatic Corpus Construction from the Web\n",
      "258 A Haskell Theorem Prover\n",
      "259 Modelling the effect of time delays in the\n",
      "                    neurogenic network\n",
      "260 Can a computer be used to determine opinions?\n",
      "261 Intelligent Agents and Artistic Creation\n",
      "262 A stochastic model of gene expression\n",
      "                    incorporating transcriptional time delays.\n",
      "263 Web Visualization\n",
      "264 Football Player Tracking\n",
      "265 Development Of Flight Simulator For ACSE Flight\n",
      "                    Simulator\n",
      "266 Extraction of Individuals from Talk Show\n",
      "                    Programmes Using Shot Detection and Unsupervised\n",
      "                    Clustering\n",
      "267 Easy Data Creator\n",
      "268 A Haskell-To-Java Translation Tool\n",
      "269 Investigate How to Design a Controller for an\n",
      "                    Autonomous UAV\n",
      "270 An Automated Mixing System for House Music\n",
      "271 Dynamic routing of traffic in a simulated system;\n",
      "272 Text Compaction for small Devices\n",
      "273 A Cracking Algorithm for Destructible 3D Objects\n",
      "274 Machine Learning Techniques for Phrase Chunking and Robust Parsing\n",
      "275 Cartoon Face Animation via Machine Learning\n",
      "276 A Haskell Developers Toolkit\n",
      "277 X-Machine Model of a Biological System\n",
      "278 Analysis of Feasibility to Educate People by Game\n",
      "279 Large Scale Named Entity Recognition for the Web\n",
      "280 Building a Translation Lexicon for Proper Names From the Web\n",
      "281 Text Compaction in Small Devices\n",
      "282 PDAs, Windows CE and EPOC\n",
      "283 Java 3D Head Generation Tool\n",
      "284 The Automatic Detection of Plagiarism\n",
      "285 AdaBoost, Artificial Neural Nets and RBF Nets\n",
      "286 Discovery Event-driven Action and Data Modelling\n",
      "287 Statistical Word Sense Disambiguation\n",
      "288 An Automatic Text Summariser\n",
      "289 A Darkroom Simulation for Photographic Printing\n",
      "290 Artificial Neural Nets and Problem Decomposition\n",
      "291 Extending a Dynamic Graphical Turing Machine\n",
      "292 Separating Sounds Using Genetic Programming in Matlab\n",
      "293 Adding Further Mathematical Facilities to a LaTeX to RTF Convertor\n",
      "294 Interactive Information Retrieval (IIR)\n",
      "295 Evaluating the Performance of AI Techniques in the Domain of Computer Games\n",
      "296 Auralisation of an online virtual game world for\n",
      "                    the visually impaired and blind\n",
      "297 Website Manager\n",
      "298 Evolving Databases\n",
      "299 Intelligent Agents and Artistic Creation\n",
      "300 An Investigation of Document Classification using\n",
      "                    the Naïve Bayes Classifier\n",
      "301 Investigating the Effect of Site Organisation on Memory Recall\n",
      "302 A Comparison of Java Applets, Servlets and JSP\n",
      "303 On Combining Artificial Neural Nets: Ensemble approach\n",
      "304 Logic Simulations System\n",
      "305 Supporting Dataflow Algebra within WinCASE\n",
      "306 A Hybrid Machine Simulator\n",
      "307 Linking Dataflow Algebra with the CaDiZ Tool\n",
      "308 An exemplary Java Speech Demo, Dynamic Time Warping\n",
      "309 Interactive Vector Geometry Demonstrations\n",
      "310 Subdivision Surfaces for Computer Games\n",
      "311 PIY-II and Extreme Programming\n",
      "312 Discovery Task Structure and Flow Modelling\n",
      "313 Silhouette Detection and Enhancement in Realtime Nonphotorealistic Rendering\n",
      "314 Toucan: A Configurable Web Site Maintenance Tool\n",
      "315 Natural Language Communications in Computer Games\n",
      "316 Wavelets for Sound Analysis and Re-synthesis\n",
      "317 A Distributed School Reporting Application\n",
      "318 CheckMate: A tool for the Validation Of Object\n",
      "                    Algebras\n",
      "319 Building an interface for an on-line Question Answering System\n",
      "320 Perfect Plants\n",
      "321 Subdivision Surfaces for Computer Games\n",
      "322 Speed the rendering of 3D terrain in computer games\n",
      "323 Algorithm Visualization\n",
      "324 CASE Tools and the World Wide Web\n",
      "325 A Link Analysis Tool: Visualising Entities and Relations Extracted from Text\n",
      "326 Build Your Own Object-orientated Language\n",
      "327 Document Filtering\n",
      "328 Automatic Detection of Plagiarism\n",
      "329 Comparing Statechart Dialects\n",
      "330 Virtual Enigma: A Simulator for the Enigma Encrypting Device\n",
      "331 Creating musical compositions with Java\n",
      "332 Automatic Detection of Plagiarism\n",
      "333 Computer Based Speech Training and Recognition\n",
      "334 ShATR-Web\n",
      "335 Parametric Patches for Computer Games\n",
      "336 A Size Measuring Instrument for Java Programs\n",
      "337 Musical Feature Extraction\n",
      "338 Constructor for Asynchronous Circuits\n",
      "339 Topological Rules for Dungeon Generation\n",
      "340 HTML Documents with PostIt® Notes Attached\n",
      "341 A Cryptographer's Workbench\n",
      "342 Email Filtering Tool\n",
      "343 Perfect Plants\n",
      "344 Pattern Recognition in DNA Microarray Data\n",
      "345 Java Spaces: A Critical Analysis of Concepts & Techniques\n",
      "346 Speech-based Computer Games\n",
      "347 Classification of Protein Sequences into Homologous Families\n",
      "348 Java Speech Demos\n",
      "349 Motion Capture Envelopes: An Investigation into Real-time Modification of Motion Capture Data in a Game Environment\n",
      "350 Non-Linear Transformations For the Creation of\n",
      "                    Diverse Classifier Ensembles\n",
      "351 Computerised Determination of Disputed Authorship\n",
      "352 Extracting GIS Data from Scanned Maps\n",
      "353 A Neural Network Algorithm for Internetwork Routing\n",
      "354 Monkey Talk A speech based computer game\n",
      "355 Using UDP to Increase the Scalability of\n",
      "                    Peer-to-Peer networks\n",
      "356 Probability Tutor and Problem Generator\n",
      "357 An Almanac Manager of Meetings\n",
      "358 A Web Site on Cryptography\n",
      "359 Detecting Plagiarism in Java Code\n",
      "360 Wrapper Induction for Biographical Information Sources\n",
      "361 Demolishing Buildings in Computer Games\n",
      "362 Conversion of Matlab Auditory Demos to Java\n",
      "363 Animating the Artificial Intelligence Techniques\n",
      "364 A Talking Head Within A Game Environment\n",
      "365 Automated Assessment Of A Student’ s Work\n",
      "366 Going Beyond The JavaSpace\n",
      "367 Email Filtering\n",
      "368 Bagging and Selection\n",
      "369 Automatic Generation of Dungeons for Computer Games\n",
      "370 Rapid Prototyping in a Functional Language\n",
      "371 Mobile Agents\n",
      "372 Image Processing by Wavelets\n",
      "373 Automatic Text Alignment\n",
      "374 REVERBERATION SIMULATOR\n",
      "375 Word Sense Disambiguation\n",
      "376 Speech Based Golf Game: Primarily intended for a visually impaired person\n",
      "377 Restaurant Recommender Natural Language Interface System\n",
      "378 Simulator for Asynchronous Circuits\n",
      "379 The Crypto Tool\n",
      "380 C++ Persistent Container Components\n",
      "381 Image Retrieval using XML and Content Based Image Retrieval Theory\n",
      "382 Generating 3D Triangular Meshes with Subdivision Connectivity\n",
      "383 DEVELOPING AN E-COMMERCE SYSTEM: SELLING PHOTOGRAPHS ON THE INTERNET\n",
      "384 Creation of a Mobile Intelligent Agent Framework for Simulating Social Insect Communities\n",
      "385 Formally analysing website designs\n",
      "386 Vollenhoven – a general purpose artificial\n",
      "                    chemistry simulator\n",
      "387 An Investigation of Vertex Programming\n",
      "388 Finding Noise Objects in Spectrogram\n",
      "389 Implementing Genetic Algorithms for use in the\n",
      "                    NETLAB Neural Network Environment\n",
      "390 Quadratic distortion for interactive design\n",
      "391 DYNAMIC PROVISION OF SERVICES IN A HETEROGENEOUS WIRELESS NETWORK ENVIRONMENT\n",
      "392 Extracting GIS data from scanned maps\n",
      "393 Image Compression Using Wavelets\n",
      "394 The DynaCat System\n",
      "395 Developing small robots geared towards a co-operative learning environment\n",
      "396 An Automatic Software DJ\n",
      "397 Three-Dimensional Statecharts\n",
      "398 A Telephone-Based Speech Recognition System and VoiceXML Application Design Tool\n",
      "399 A Java Regression Testing tool\n",
      "400 Visualisation Of Java Source Code\n",
      "401 A Self Resourcing Web Based Electronic Journal\n",
      "402 Evolving Sculptures in Java3D\n",
      "403 Automatic Summarisation of News Broadcast Transcripts\n",
      "404 Interactive Web Tutor\n",
      "405 Automatic Cryptanalysis: A Dictionary Based Method for Solving Mono-alphabetic Substitution Cryptograms\n",
      "406 ARTIFICIAL CHEMISTRY & BIOINFORMATICS - DNA\n",
      "                    TRANSLATION AND PROTEIN ASSEMBLY TOOL\n",
      "407 Visual X-Machine Description Language (VXMDL)\n",
      "408 Lith Printing Simulation\n",
      "409 C++ Searchable Container Components\n",
      "410 RAHRE, A Regular Expression Library\n",
      "411 PostIt® notes on a webpage\n",
      "412 File compressor/un-compressor tool\n",
      "413 A Chroma-key tool for producing noisy audio-visual speech data\n",
      "414 E-mail Filtering Tool\n",
      "415 An Automatic Text Summariser\n",
      "416 RSA Algorithm\n",
      "417 Intelligent Agents and Artistic Creation\n",
      "418 Application of Wavelets as to aid the\n",
      "                    Computational Efficiency of current Antenna\n",
      "                    simulations\n",
      "419 A Java Structural Testing Toolkit\n",
      "420 Visualising Speech Synthesis\n",
      "421 Automated Functional Testing in Extreme\n",
      "                    Programming\n",
      "422 Producing a Toolkit for Designers of Adventure Games\n",
      "423 Real-Time Java Simulation of a Mini Moog\n",
      "424 Natural Language Processing in Computer Games\n",
      "425 Access to XML Database Translator\n",
      "426 The Visualisation of Forces in Dynamic Simulations\n",
      "427 Using Motion Capture Data in Computer Games An Investigation into Motion Capture Splicing\n",
      "428 Art Emulation\n",
      "429 A Java version of YACC\n",
      "430 JDCV: A Distributed, Modular, Computer Vision\n",
      "                    Framework\n",
      "431 Modelling Wear-And-Tear On A Football Pitch\n",
      "432 Strategies for spotting copyright infringement in mp3 files\n",
      "433 Biometric Authentication vs. Recognition in Access\n",
      "                    Control\n",
      "434 Investigating the Effect of Different Navigational Aids on the Ability of Users to Navigate a Web-site\n",
      "435 Evaluating RIL as basis for evaluating Automated Speech Recognition devices and the consequences of using probabilistic String Edit Distance as input\n",
      "436 A java functional testing toolkit\n"
     ]
    }
   ],
   "source": [
    "re = \"\"\n",
    "ca = \"\"\n",
    "count = 0\n",
    "for top, abt in test_examples:\n",
    "    count += 1\n",
    "    print(count, top.numpy().decode(\"utf-8\"))\n",
    "    reference = processing(abt.numpy().decode(\"utf-8\").lower())\n",
    "    predict = print_generation(top.numpy().decode(\"utf-8\").lower())\n",
    "    candidate = processing(predict)\n",
    "    if (candidate == '\\n'):\n",
    "        candidate = '[None]\\n'\n",
    "    if (reference == '\\n'):\n",
    "        reference = '[None]\\n'\n",
    "    re += reference + '\\n'\n",
    "    ca += candidate + '\\n'\n",
    "re = re.rstrip('\\n')\n",
    "ca = ca.rstrip('\\n')\n",
    "save_file('./reference.txt', re)\n",
    "save_file('./candidate.txt', ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    word_punct_tokenizer = WordPunctTokenizer()\n",
    "    tokens = word_punct_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.328715 2-gram: 0.056429 3-gram: 0.016211 4-gram: 0.008150\n",
      "----------\n",
      "Individual 1-gram: 0.296748 2-gram: 0.053061 3-gram: 0.008197 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.321212 2-gram: 0.048780 3-gram: 0.006135 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.250000 2-gram: 0.047393 3-gram: 0.004762 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.320340 2-gram: 0.051659 3-gram: 0.011979 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.143491 2-gram: 0.024200 3-gram: 0.012246 4-gram: 0.008263\n",
      "----------\n",
      "Individual 1-gram: 0.235693 2-gram: 0.031084 3-gram: 0.007815 4-gram: 0.003930\n",
      "----------\n",
      "Individual 1-gram: 0.232479 2-gram: 0.054068 3-gram: 0.004541 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.244898 2-gram: 0.041096 3-gram: 0.013793 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.321429 2-gram: 0.076233 3-gram: 0.004505 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.261905 2-gram: 0.040956 3-gram: 0.006849 4-gram: 0.003436\n",
      "----------\n",
      "Individual 1-gram: 0.244048 2-gram: 0.053892 3-gram: 0.012048 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.246512 2-gram: 0.028037 3-gram: 0.004695 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.307692 2-gram: 0.006452 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.191837 2-gram: 0.032787 3-gram: 0.004115 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.324111 2-gram: 0.063492 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.220641 2-gram: 0.032855 3-gram: 0.002752 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.144724 2-gram: 0.020490 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.324520 2-gram: 0.048389 3-gram: 0.006088 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.250825 2-gram: 0.039735 3-gram: 0.006645 4-gram: 0.003333\n",
      "----------\n",
      "Individual 1-gram: 0.151802 2-gram: 0.024682 3-gram: 0.002488 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.261352 2-gram: 0.050396 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.225358 2-gram: 0.017487 3-gram: 0.005881 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.254438 2-gram: 0.053571 3-gram: 0.011976 4-gram: 0.006024\n",
      "----------\n",
      "Individual 1-gram: 0.252438 2-gram: 0.024223 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.153382 2-gram: 0.011025 3-gram: 0.002774 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.223313 2-gram: 0.011844 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.297297 2-gram: 0.047619 3-gram: 0.006849 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.220264 2-gram: 0.048673 3-gram: 0.013333 4-gram: 0.004464\n",
      "----------\n",
      "Individual 1-gram: 0.201439"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2-gram: 0.036232 3-gram: 0.007299 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.308725 2-gram: 0.081081 3-gram: 0.034014 4-gram: 0.027397\n",
      "----------\n",
      "Individual 1-gram: 0.365000 2-gram: 0.060302 3-gram: 0.015152 4-gram: 0.005076\n",
      "----------\n",
      "Individual 1-gram: 0.165300 2-gram: 0.019324 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.325148 2-gram: 0.048471 3-gram: 0.018291 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.276042 2-gram: 0.052356 3-gram: 0.005263 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.138655 2-gram: 0.033755 3-gram: 0.008475 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.283535 2-gram: 0.040016 3-gram: 0.005030 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.257669 2-gram: 0.024691 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.353846 2-gram: 0.046512 3-gram: 0.007813 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.275424 2-gram: 0.051064 3-gram: 0.017094 4-gram: 0.012876\n",
      "----------\n",
      "Individual 1-gram: 0.244238 2-gram: 0.036431 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.289798 2-gram: 0.050191 3-gram: 0.006723 4-gram: 0.003377\n",
      "----------\n",
      "Individual 1-gram: 0.337778 2-gram: 0.071429 3-gram: 0.013453 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.127119 2-gram: 0.008547 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.308772 2-gram: 0.070423 3-gram: 0.014134 4-gram: 0.007092\n",
      "----------\n",
      "Individual 1-gram: 0.289186 2-gram: 0.063316 3-gram: 0.031886 4-gram: 0.025693\n",
      "----------\n",
      "Individual 1-gram: 0.202204 2-gram: 0.031953 3-gram: 0.004024 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.387755 2-gram: 0.075342 3-gram: 0.020690 4-gram: 0.013889\n",
      "----------\n",
      "Individual 1-gram: 0.137242 2-gram: 0.019303 3-gram: 0.003910 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.205714 2-gram: 0.022989 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.339041 2-gram: 0.031909 3-gram: 0.007119 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.321429 2-gram: 0.064748 3-gram: 0.036232 4-gram: 0.014599\n",
      "----------\n",
      "Individual 1-gram: 0.313432 2-gram: 0.031582 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.258883 2-gram: 0.020408 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.270285 2-gram: 0.042755 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285714 2-gram: 0.023923 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.280612 2-gram: 0.061538 3-gram: 0.036082 4-gram: 0.031088\n",
      "----------\n",
      "Individual 1-gram: 0.265197 2-gram: 0.070784 3-gram: 0.023793 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.348387 2-gram: 0.071429 3-gram: 0.013072 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.293930 2-gram: 0.048077 3-gram: 0.003215 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.129742 2-gram: 0.029655 3-gram: 0.012475 4-gram: 0.005039\n",
      "----------\n",
      "Individual 1-gram: 0.316583 2-gram: 0.080808 3-gram: 0.015228 4-gram: 0.005102\n",
      "----------\n",
      "Individual 1-gram: 0.250072 2-gram: 0.041941 3-gram: 0.003517 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287474 2-gram: 0.056914 3-gram: 0.022003 4-gram: 0.013270\n",
      "----------\n",
      "Individual 1-gram: 0.185225 2-gram: 0.007307 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.298913 2-gram: 0.021858 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.308411 2-gram: 0.042254 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.252675 2-gram: 0.030512 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.227723 2-gram: 0.044776 3-gram: 0.015000 4-gram: 0.005025\n",
      "----------\n",
      "Individual 1-gram: 0.264003 2-gram: 0.018119 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.274725 2-gram: 0.038674 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.174419 2-gram: 0.017544 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.355043 2-gram: 0.056118 3-gram: 0.004028 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.047290 2-gram: 0.009109 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.304348 2-gram: 0.052402 3-gram: 0.017544 4-gram: 0.008811\n",
      "----------\n",
      "Individual 1-gram: 0.224297 2-gram: 0.009408 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.186275 2-gram: 0.024631 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.349315 2-gram: 0.096552 3-gram: 0.055556 4-gram: 0.027972\n",
      "----------\n",
      "Individual 1-gram: 0.333370 2-gram: 0.042922 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.114625 2-gram: 0.015873 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.413333 2-gram: 0.080357 3-gram: 0.017937 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.268623 2-gram: 0.025175 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.276786 2-gram: 0.062780 3-gram: 0.009009 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.163866 2-gram: 0.033755 3-gram: 0.004237 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.348837 2-gram: 0.056075 3-gram: 0.014085 4-gram: 0.009434\n",
      "----------\n",
      "Individual 1-gram: 0.221757 2-gram: 0.063025 3-gram: 0.021097 4-gram: 0.016949\n",
      "----------\n",
      "Individual 1-gram: 0.328791 2-gram: 0.033781 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.012404 2-gram: 0.004998 3-gram: 0.002099 4-gram: 0.001105\n",
      "----------\n",
      "Individual 1-gram: 0.004408 2-gram: 0.000385 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.259459 2-gram: 0.021739 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.309524 2-gram: 0.029940 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.265522 2-gram: 0.032624 3-gram: 0.013149 4-gram: 0.006624\n",
      "----------\n",
      "Individual 1-gram: 0.401961 2-gram: 0.064039 3-gram: 0.009901 4-gram: 0.004975\n",
      "----------\n",
      "Individual 1-gram: 0.271277 2-gram: 0.058824 3-gram: 0.016129 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.205818 2-gram: 0.014885 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.305383 2-gram: 0.045526 3-gram: 0.005727 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.232558 2-gram: 0.073930 3-gram: 0.027344 4-gram: 0.019608\n",
      "----------\n",
      "Individual 1-gram: 0.339901 2-gram: 0.044554 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.225564 2-gram: 0.015152 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.182222 2-gram: 0.033373 3-gram: 0.006724 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.229323 2-gram: 0.026415 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.222798 2-gram: 0.015625 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.328285 2-gram: 0.064817 3-gram: 0.006527 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.214876 2-gram: 0.037344 3-gram: 0.012500 4-gram: 0.008368\n",
      "----------\n",
      "Individual 1-gram: 0.302058 2-gram: 0.031463 3-gram: 0.005280 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.191011 2-gram: 0.033898 3-gram: 0.017045 4-gram: 0.011429\n",
      "----------\n",
      "Individual 1-gram: 0.114625 2-gram: 0.003968 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.074890 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.233578 2-gram: 0.030158 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.264706 2-gram: 0.040590 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.243902 2-gram: 0.024510 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.252874 2-gram: 0.046154 3-gram: 0.003861 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.248927 2-gram: 0.038793 3-gram: 0.004329 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.228155 2-gram: 0.024390 3-gram: 0.004902 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.240387 2-gram: 0.039769 3-gram: 0.003076 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.242424 2-gram: 0.065990 3-gram: 0.030612 4-gram: 0.020513\n",
      "----------\n",
      "Individual 1-gram: 0.113208 2-gram: 0.014218 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.000000 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.211180 2-gram: 0.037383 3-gram: 0.003125 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.248521 2-gram: 0.029762 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.400774 2-gram: 0.069856 3-gram: 0.005404 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.097872 2-gram: 0.021368 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.310485 2-gram: 0.049794 3-gram: 0.004554 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.114500 2-gram: 0.026007 3-gram: 0.009570 4-gram: 0.004842\n",
      "----------\n",
      "Individual 1-gram: 0.186869 2-gram: 0.040609 3-gram: 0.010204 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.236264 2-gram: 0.038674 3-gram: 0.005556 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.253319 2-gram: 0.046310 3-gram: 0.003880 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.252577 2-gram: 0.041451 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.251029 2-gram: 0.061983 3-gram: 0.012448 4-gram: 0.004167\n",
      "----------\n",
      "Individual 1-gram: 0.216048 2-gram: 0.042510 3-gram: 0.021435 4-gram: 0.016214\n",
      "----------\n",
      "Individual 1-gram: 0.193396 2-gram: 0.023697 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285714 2-gram: 0.039604 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.178295 2-gram: 0.031128 3-gram: 0.007813 4-gram: 0.003922\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.267081 2-gram: 0.012500 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.247664 2-gram: 0.032864 3-gram: 0.009434 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.251392 2-gram: 0.044433 3-gram: 0.004477 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.194269 2-gram: 0.031421 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.260638 2-gram: 0.064171 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.197183 2-gram: 0.018868 3-gram: 0.004739 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.289976 2-gram: 0.059471 3-gram: 0.027217 4-gram: 0.016443\n",
      "----------\n",
      "Individual 1-gram: 0.275000 2-gram: 0.040201 3-gram: 0.015152 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.274333 2-gram: 0.037390 3-gram: 0.004698 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.161702 2-gram: 0.016998 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.047244 2-gram: 0.016575 3-gram: 0.004930 4-gram: 0.000997\n",
      "----------\n",
      "Individual 1-gram: 0.058246 2-gram: 0.015249 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.338648 2-gram: 0.020011 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.297674 2-gram: 0.056075 3-gram: 0.023474 4-gram: 0.014151\n",
      "----------\n",
      "Individual 1-gram: 0.323869 2-gram: 0.056733 3-gram: 0.014287 4-gram: 0.007196\n",
      "----------\n",
      "Individual 1-gram: 0.386541 2-gram: 0.067799 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.000227 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.288235 2-gram: 0.047337 3-gram: 0.011905 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.331731 2-gram: 0.057971 3-gram: 0.004854 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.302632 2-gram: 0.059603 3-gram: 0.013333 4-gram: 0.006711\n",
      "----------\n",
      "Individual 1-gram: 0.290614 2-gram: 0.045411 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.267327 2-gram: 0.024876 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.323529 2-gram: 0.074074 3-gram: 0.014925 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.178344 2-gram: 0.025559 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.173913 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.252082 2-gram: 0.070090 3-gram: 0.016793 4-gram: 0.010140\n",
      "----------\n",
      "Individual 1-gram: 0.319672 2-gram: 0.090909 3-gram: 0.016667 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.392157 2-gram: 0.105263 3-gram: 0.026490 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.347826 2-gram: 0.029197 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.289180 2-gram: 0.090166 3-gram: 0.013974 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.347222 2-gram: 0.037209 3-gram: 0.009346 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.245690 2-gram: 0.030303 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.184564 2-gram: 0.021042 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.282051 2-gram: 0.012903 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.079620 2-gram: 0.023467 3-gram: 0.003737 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.280000 2-gram: 0.044177 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.163462 2-gram: 0.033816 3-gram: 0.009709 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.268472 2-gram: 0.043294 3-gram: 0.005455 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.300699 2-gram: 0.049296 3-gram: 0.007092 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.314607 2-gram: 0.052632 3-gram: 0.018868 4-gram: 0.011364\n",
      "----------\n",
      "Individual 1-gram: 0.309100 2-gram: 0.045243 3-gram: 0.005691 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.301653 2-gram: 0.058091 3-gram: 0.004167 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.295918 2-gram: 0.051282 3-gram: 0.005155 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.137874 2-gram: 0.023530 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.186441 2-gram: 0.039773 3-gram: 0.011429 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.304878 2-gram: 0.057143 3-gram: 0.016393 4-gram: 0.012346\n",
      "----------\n",
      "Individual 1-gram: 0.274194 2-gram: 0.048780 3-gram: 0.008197 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.231111 2-gram: 0.049107 3-gram: 0.008969 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.155027 2-gram: 0.016261 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.230120 2-gram: 0.054955 3-gram: 0.015803 4-gram: 0.011929\n",
      "----------\n",
      "Individual 1-gram: 0.177019 2-gram: 0.040498 3-gram: 0.009375 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.104478 2-gram: 0.007519 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.350649 2-gram: 0.104348 3-gram: 0.034934 4-gram: 0.017544\n",
      "----------\n",
      "Individual 1-gram: 0.163361 2-gram: 0.019005 3-gram: 0.003194 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.292683 2-gram: 0.049180 3-gram: 0.008264 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.350365 2-gram: 0.029412 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.327665 2-gram: 0.074800 3-gram: 0.022540 4-gram: 0.011321\n",
      "----------\n",
      "Individual 1-gram: 0.171548 2-gram: 0.016807 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.294444 2-gram: 0.061453 3-gram: 0.005618 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.197605 2-gram: 0.006024 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.295082 2-gram: 0.061728 3-gram: 0.020661 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.271084 2-gram: 0.090909 3-gram: 0.024390 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.272727 2-gram: 0.012195 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.181818 2-gram: 0.025381 3-gram: 0.005102 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.388298 2-gram: 0.064171 3-gram: 0.010753 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287362 2-gram: 0.006580 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.149595 2-gram: 0.032462 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.210210 2-gram: 0.045181 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.361111 2-gram: 0.069930 3-gram: 0.014085 4-gram: 0.007092\n",
      "----------\n",
      "Individual 1-gram: 0.350751 2-gram: 0.055695 3-gram: 0.014003 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.258929 2-gram: 0.035874 3-gram: 0.004505 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.259259 2-gram: 0.074534 3-gram: 0.018750 4-gram: 0.012579\n",
      "----------\n",
      "Individual 1-gram: 0.271429 2-gram: 0.028777 3-gram: 0.007246 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.337636 2-gram: 0.054402 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.297448 2-gram: 0.076193 3-gram: 0.010954 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.185315 2-gram: 0.031579 3-gram: 0.003521 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.211409 2-gram: 0.047834 3-gram: 0.002672 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.318548 2-gram: 0.048583 3-gram: 0.004065 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285714 2-gram: 0.056000 3-gram: 0.008065 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.354300 2-gram: 0.043898 3-gram: 0.004902 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.206452 2-gram: 0.019481 3-gram: 0.006536 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.147059 2-gram: 0.022222 3-gram: 0.007463 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.320225 2-gram: 0.079096 3-gram: 0.022727 4-gram: 0.005714\n",
      "----------\n",
      "Individual 1-gram: 0.255172 2-gram: 0.048611 3-gram: 0.006993 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.096970 2-gram: 0.015198 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.280435 2-gram: 0.054073 3-gram: 0.006050 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.250000 2-gram: 0.034783 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.305689 2-gram: 0.060295 3-gram: 0.033772 4-gram: 0.027241\n",
      "----------\n",
      "Individual 1-gram: 0.255682 2-gram: 0.068571 3-gram: 0.011494 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.000000 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.259574 2-gram: 0.038462 3-gram: 0.008584 4-gram: 0.004310\n",
      "----------\n",
      "Individual 1-gram: 0.193069 2-gram: 0.014925 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.296117 2-gram: 0.048780 3-gram: 0.009804 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.329341 2-gram: 0.078313 3-gram: 0.042424 4-gram: 0.030488\n",
      "----------\n",
      "Individual 1-gram: 0.123932 2-gram: 0.004292 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.200658 2-gram: 0.033003 3-gram: 0.003311 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.267770 2-gram: 0.032899 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.262500 2-gram: 0.016736 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.225000 2-gram: 0.029289 3-gram: 0.016807 4-gram: 0.008439\n",
      "----------\n",
      "Individual 1-gram: 0.329290 2-gram: 0.093805 3-gram: 0.043644 4-gram: 0.036665\n",
      "----------\n",
      "Individual 1-gram: 0.231250 2-gram: 0.018868 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.166667 2-gram: 0.023256 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.121589 2-gram: 0.022389 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.195046 2-gram: 0.018634 3-gram: 0.006231 4-gram: 0.003125\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.241028 2-gram: 0.042747 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.302564 2-gram: 0.072165 3-gram: 0.010363 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287332 2-gram: 0.035373 3-gram: 0.008194 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.297709 2-gram: 0.019403 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.231076 2-gram: 0.052000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.351524 2-gram: 0.057043 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.243824 2-gram: 0.028094 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.260000 2-gram: 0.030151 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.188802 2-gram: 0.023785 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.325631 2-gram: 0.058528 3-gram: 0.023565 4-gram: 0.017791\n",
      "----------\n",
      "Individual 1-gram: 0.269231 2-gram: 0.045161 3-gram: 0.025974 4-gram: 0.019608\n",
      "----------\n",
      "Individual 1-gram: 0.261224 2-gram: 0.028689 3-gram: 0.008230 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285000 2-gram: 0.035176 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.203125 2-gram: 0.054902 3-gram: 0.027559 4-gram: 0.015810\n",
      "----------\n",
      "Individual 1-gram: 0.381715 2-gram: 0.054055 3-gram: 0.010870 4-gram: 0.005465\n",
      "----------\n",
      "Individual 1-gram: 0.115031 2-gram: 0.028340 3-gram: 0.007790 4-gram: 0.002618\n",
      "----------\n",
      "Individual 1-gram: 0.253659 2-gram: 0.068627 3-gram: 0.019704 4-gram: 0.009901\n",
      "----------\n",
      "Individual 1-gram: 0.322222 2-gram: 0.072626 3-gram: 0.022472 4-gram: 0.005650\n",
      "----------\n",
      "Individual 1-gram: 0.253456 2-gram: 0.032407 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.173482 2-gram: 0.043056 3-gram: 0.018235 4-gram: 0.013757\n",
      "----------\n",
      "Individual 1-gram: 0.159574 2-gram: 0.010753 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.095559 2-gram: 0.018053 3-gram: 0.007580 4-gram: 0.004583\n",
      "----------\n",
      "Individual 1-gram: 0.212871 2-gram: 0.019900 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.247016 2-gram: 0.021626 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.281046 2-gram: 0.013158 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.312102 2-gram: 0.012821 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.260870 2-gram: 0.016393 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.097341 2-gram: 0.028591 3-gram: 0.009598 4-gram: 0.005524\n",
      "----------\n",
      "Individual 1-gram: 0.207386 2-gram: 0.037037 3-gram: 0.008571 4-gram: 0.002865\n",
      "----------\n",
      "Individual 1-gram: 0.323815 2-gram: 0.054357 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.269504 2-gram: 0.021429 3-gram: 0.007194 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.057177 2-gram: 0.010848 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.276596 2-gram: 0.035714 3-gram: 0.007194 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.196545 2-gram: 0.023288 3-gram: 0.007819 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.201613 2-gram: 0.036437 3-gram: 0.004065 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.330582 2-gram: 0.095819 3-gram: 0.045701 4-gram: 0.030679\n",
      "----------\n",
      "Individual 1-gram: 0.276204 2-gram: 0.030941 3-gram: 0.005199 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285714 2-gram: 0.053191 3-gram: 0.005348 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.185714 2-gram: 0.033493 3-gram: 0.004808 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.269608 2-gram: 0.039409 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287293 2-gram: 0.066667 3-gram: 0.011173 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287582 2-gram: 0.085526 3-gram: 0.019868 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.153056 2-gram: 0.015409 3-gram: 0.001939 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.226316 2-gram: 0.021164 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.104008 2-gram: 0.019809 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.141762 2-gram: 0.023077 3-gram: 0.003861 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.331302 2-gram: 0.051469 3-gram: 0.003447 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.171618 2-gram: 0.028296 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.018959 2-gram: 0.003222 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.269542 2-gram: 0.032432 3-gram: 0.002710 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.286324 2-gram: 0.051920 3-gram: 0.011624 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.291332 2-gram: 0.057632 3-gram: 0.015830 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.277972 2-gram: 0.016694 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.191249 2-gram: 0.017509 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.179487 2-gram: 0.028939 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.299228 2-gram: 0.040829 3-gram: 0.008246 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.222222 2-gram: 0.029851 3-gram: 0.007519 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.317880 2-gram: 0.038203 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.276334 2-gram: 0.040734 3-gram: 0.006839 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.331967 2-gram: 0.094650 3-gram: 0.004132 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.310185 2-gram: 0.055814 3-gram: 0.014019 4-gram: 0.009390\n",
      "----------\n",
      "Individual 1-gram: 0.197672 2-gram: 0.027770 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.151495 2-gram: 0.008485 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.005880 2-gram: 0.001554 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.165957 2-gram: 0.017094 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.261484 2-gram: 0.024823 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.238523 2-gram: 0.034267 3-gram: 0.003446 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.416640 2-gram: 0.103053 3-gram: 0.047966 4-gram: 0.032251\n",
      "----------\n",
      "Individual 1-gram: 0.242857 2-gram: 0.043165 3-gram: 0.007246 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.266291 2-gram: 0.038821 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.187075 2-gram: 0.030717 3-gram: 0.003425 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.219298 2-gram: 0.039648 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.290123 2-gram: 0.043344 3-gram: 0.003106 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.337607 2-gram: 0.047210 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.000000 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.281270 2-gram: 0.041720 3-gram: 0.018641 4-gram: 0.009371\n",
      "----------\n",
      "Individual 1-gram: 0.333333 2-gram: 0.041885 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.315972 2-gram: 0.034843 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.176955 2-gram: 0.016529 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.213536 2-gram: 0.037396 3-gram: 0.004707 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.258242 2-gram: 0.055249 3-gram: 0.005556 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.200000 2-gram: 0.025890 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.146853 2-gram: 0.021127 3-gram: 0.007092 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.360916 2-gram: 0.036464 3-gram: 0.002613 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.341137 2-gram: 0.043624 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.312500 2-gram: 0.036649 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.234848 2-gram: 0.022901 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.292135 2-gram: 0.028249 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287049 2-gram: 0.063742 3-gram: 0.008556 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.122388 2-gram: 0.008982 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.375326 2-gram: 0.071027 3-gram: 0.008926 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.267606 2-gram: 0.063830 3-gram: 0.035714 4-gram: 0.021583\n",
      "----------\n",
      "Individual 1-gram: 0.176923 2-gram: 0.027027 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.215470 2-gram: 0.022222 3-gram: 0.005587 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.382421 2-gram: 0.078641 3-gram: 0.008785 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.186571 2-gram: 0.041489 3-gram: 0.006673 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.208488 2-gram: 0.022963 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.145000 2-gram: 0.010050 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.338383 2-gram: 0.084999 3-gram: 0.008541 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.246073 2-gram: 0.042105 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.379630 2-gram: 0.046729 3-gram: 0.018868 4-gram: 0.009524\n",
      "----------\n",
      "Individual 1-gram: 0.271739 2-gram: 0.043716 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.278146 2-gram: 0.020000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.356080 2-gram: 0.060506 3-gram: 0.005069 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.196314 2-gram: 0.017574 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.191224 2-gram: 0.029177 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.127559 2-gram: 0.020934 3-gram: 0.002630 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.313788 2-gram: 0.048866 3-gram: 0.013396 4-gram: 0.004489\n",
      "----------\n",
      "Individual 1-gram: 0.280899 2-gram: 0.067669 3-gram: 0.003774 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.145170 2-gram: 0.013814 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.330435 2-gram: 0.069869 3-gram: 0.004386 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.182626 2-gram: 0.029409 3-gram: 0.002467 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.234043 2-gram: 0.021390 3-gram: 0.005376 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.287736 2-gram: 0.023697 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.259036 2-gram: 0.060606 3-gram: 0.006098 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.208394 2-gram: 0.016171 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.264078 2-gram: 0.075012 3-gram: 0.006868 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.294416 2-gram: 0.051020 3-gram: 0.010256 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.342466 2-gram: 0.055172 3-gram: 0.013889 4-gram: 0.006993\n",
      "----------\n",
      "Individual 1-gram: 0.231177 2-gram: 0.042382 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.313008 2-gram: 0.028571 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.279570 2-gram: 0.050360 3-gram: 0.007220 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.362360 2-gram: 0.064736 3-gram: 0.023705 4-gram: 0.017904\n",
      "----------\n",
      "Individual 1-gram: 0.199676 2-gram: 0.053174 3-gram: 0.020804 4-gram: 0.008970\n",
      "----------\n",
      "Individual 1-gram: 0.305032 2-gram: 0.031387 3-gram: 0.006331 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.185633 2-gram: 0.012639 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.240385 2-gram: 0.033816 3-gram: 0.004854 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.330612 2-gram: 0.053279 3-gram: 0.004115 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.362836 2-gram: 0.095450 3-gram: 0.022591 4-gram: 0.011362\n",
      "----------\n",
      "Individual 1-gram: 0.323671 2-gram: 0.063107 3-gram: 0.009756 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.272187 2-gram: 0.045586 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.288000 2-gram: 0.072289 3-gram: 0.008065 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.234146 2-gram: 0.029412 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.177005 2-gram: 0.038987 3-gram: 0.006560 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.308271 2-gram: 0.015152 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.275703 2-gram: 0.019208 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.283453 2-gram: 0.022370 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.247227 2-gram: 0.008604 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.251163 2-gram: 0.035236 3-gram: 0.007119 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.320426 2-gram: 0.052576 3-gram: 0.022713 4-gram: 0.015264\n",
      "----------\n",
      "Individual 1-gram: 0.343727 2-gram: 0.110820 3-gram: 0.034894 4-gram: 0.028128\n",
      "----------\n",
      "Individual 1-gram: 0.188679 2-gram: 0.042654 3-gram: 0.014286 4-gram: 0.009569\n",
      "----------\n",
      "Individual 1-gram: 0.271795 2-gram: 0.051546 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.000000 2-gram: 0.000000 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.279251 2-gram: 0.034053 3-gram: 0.004282 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.319218 2-gram: 0.052288 3-gram: 0.003279 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.335443 2-gram: 0.050955 3-gram: 0.019231 4-gram: 0.012903\n",
      "----------\n",
      "Individual 1-gram: 0.261261 2-gram: 0.036199 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.239796 2-gram: 0.030769 3-gram: 0.010309 4-gram: 0.005181\n",
      "----------\n",
      "Individual 1-gram: 0.274725 2-gram: 0.022099 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.321281 2-gram: 0.056320 3-gram: 0.003774 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.292890 2-gram: 0.045452 3-gram: 0.007642 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.303448 2-gram: 0.048611 3-gram: 0.006993 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.270588 2-gram: 0.043307 3-gram: 0.011858 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.322009 2-gram: 0.068719 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.162437 2-gram: 0.020408 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.316940 2-gram: 0.038462 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.260357 2-gram: 0.035750 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.353791 2-gram: 0.072464 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.259286 2-gram: 0.010659 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.344086 2-gram: 0.054054 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.067704 2-gram: 0.008006 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.300806 2-gram: 0.039967 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.251685 2-gram: 0.036036 3-gram: 0.002257 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.276568 2-gram: 0.041048 3-gram: 0.004588 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.193939 2-gram: 0.027356 3-gram: 0.006098 4-gram: 0.003058\n",
      "----------\n",
      "Individual 1-gram: 0.186957 2-gram: 0.043668 3-gram: 0.008772 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.255609 2-gram: 0.010508 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.161107 2-gram: 0.019306 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.253846 2-gram: 0.034749 3-gram: 0.003876 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.286029 2-gram: 0.059901 3-gram: 0.016058 4-gram: 0.008071\n",
      "----------\n",
      "Individual 1-gram: 0.243355 2-gram: 0.038984 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.230769 2-gram: 0.049724 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.275825 2-gram: 0.049015 3-gram: 0.010969 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.248555 2-gram: 0.034884 3-gram: 0.005848 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.255686 2-gram: 0.044189 3-gram: 0.008079 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.336440 2-gram: 0.053727 3-gram: 0.016216 4-gram: 0.010877\n",
      "----------\n",
      "Individual 1-gram: 0.270469 2-gram: 0.038925 3-gram: 0.005602 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.147436 2-gram: 0.032258 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.261698 2-gram: 0.037640 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.294737 2-gram: 0.021164 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.352227 2-gram: 0.056911 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.188355 2-gram: 0.021775 3-gram: 0.003134 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.268156 2-gram: 0.033708 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.261111 2-gram: 0.044693 3-gram: 0.011236 4-gram: 0.005650\n",
      "----------\n",
      "Individual 1-gram: 0.257143 2-gram: 0.057416 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.325021 2-gram: 0.044546 3-gram: 0.004975 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.346491 2-gram: 0.057269 3-gram: 0.004425 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.302080 2-gram: 0.086869 3-gram: 0.009715 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.269077 2-gram: 0.060723 3-gram: 0.003049 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.147208 2-gram: 0.015306 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.266643 2-gram: 0.055861 3-gram: 0.022470 4-gram: 0.015065\n",
      "----------\n",
      "Individual 1-gram: 0.200000 2-gram: 0.020101 3-gram: 0.005051 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.285068 2-gram: 0.027273 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.011907 2-gram: 0.002494 3-gram: 0.000725 4-gram: 0.000369\n",
      "----------\n",
      "Individual 1-gram: 0.245968 2-gram: 0.044534 3-gram: 0.004065 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.344343 2-gram: 0.029271 3-gram: 0.000000 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.028029 2-gram: 0.002426 3-gram: 0.000817 4-gram: 0.000000\n",
      "----------\n",
      "Individual 1-gram: 0.241865 2-gram: 0.055346 3-gram: 0.005377 4-gram: 0.000000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "scores = {'1-gram':0.0, '2-gram':0.0, '3-gram':0.0, '4-gram':0.0}\n",
    "candidate_list = []\n",
    "reference_list = []\n",
    "with open('./candidate.txt', 'r', encoding='utf-8') as c_fp, open('./reference.txt', 'r', encoding='utf-8') as r_fp:  \n",
    "    candidate_list = c_fp.readlines()\n",
    "    reference_list = r_fp.readlines()\n",
    "for i in range(len(candidate_list)):\n",
    "    candidate = tokenizer(candidate_list[i])\n",
    "    reference = [tokenizer(reference_list[i])]\n",
    "    gram1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "    gram2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
    "    gram3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
    "    gram4 = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))\n",
    "    scores['1-gram'] += gram1\n",
    "    scores['2-gram'] += gram2\n",
    "    scores['3-gram'] += gram3\n",
    "    scores['4-gram'] += gram4\n",
    "    print('Individual 1-gram: %f' % gram1, '2-gram: %f' % gram2, '3-gram: %f' % gram3, '4-gram: %f' % gram4)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average 1-gram: 0.248624\n",
      "The average 2-gram: 0.039748\n",
      "The average 3-gram: 0.006665\n",
      "The average 4-gram: 0.002312\n"
     ]
    }
   ],
   "source": [
    "print('The average 1-gram: %f' % (scores['1-gram']/test_size))\n",
    "print('The average 2-gram: %f' % (scores['2-gram']/test_size))\n",
    "print('The average 3-gram: %f' % (scores['3-gram']/test_size))\n",
    "print('The average 4-gram: %f' % (scores['4-gram']/test_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGH Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0.22464875523430824, 'p': 0.2513598973097868, 'r': 0.2171605855501095}\n",
      "{'f': 0.04563144989389988, 'p': 0.05067971767843119, 'r': 0.04551025212640094}\n",
      "{'f': 0.18845394001903285, 'p': 0.22351587664464667, 'r': 0.1931606002848331}\n"
     ]
    }
   ],
   "source": [
    "files_rouge = FilesRouge('./candidate.txt', './reference.txt')\n",
    "scores = files_rouge.get_scores(avg=True)\n",
    "print(scores['rouge-1'])\n",
    "print(scores['rouge-2'])\n",
    "print(scores['rouge-l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
